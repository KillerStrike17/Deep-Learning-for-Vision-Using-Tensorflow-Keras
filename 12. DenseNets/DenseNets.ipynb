{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KillerStrike17/Deep-Learning-for-Vision-Using-Tensorflow-Keras/blob/master/12.%20DenseNets/DenseNets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wp7vQuVKugu",
        "colab_type": "text"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMcZu8jParf3",
        "colab_type": "code",
        "outputId": "93fb9500-a3f2-4786-f78f-f83d11ce0901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Dec 11 12:24:40 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.36       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVIx_KIigxPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "% matplotlib inline \n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D,AveragePooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import Input, Activation, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Add, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.python.keras.utils import data_utils\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNHw6luQg3gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "K.set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR-MZfPjKyh4",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsO_yGxcg5D8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "l = 8\n",
        "num_filter = 16\n",
        "compression = 0.9\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkgaSC2mK8F1",
        "colab_type": "text"
      },
      "source": [
        "### Clone API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfJxl0IjK-jg",
        "colab_type": "code",
        "outputId": "5848daab-db4e-499e-dba2-375d9c585442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://KillerStrike17:Ki!!erStr1ke@github.com/prateekgulati/rgApi.git\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rgApi'...\n",
            "remote: Enumerating objects: 154, done.\u001b[K\n",
            "remote: Counting objects: 100% (154/154), done.\u001b[K\n",
            "remote: Compressing objects: 100% (152/152), done.\u001b[K\n",
            "remote: Total 460 (delta 106), reused 0 (delta 0), pack-reused 306\u001b[K\n",
            "Receiving objects: 100% (460/460), 4.53 MiB | 10.18 MiB/s, done.\n",
            "Resolving deltas: 100% (285/285), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zURTWGwx63Ft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rgApi.Data import convert_to_tfrecord_data,create_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0Q4g-qXLh6Y",
        "colab_type": "text"
      },
      "source": [
        "### Read and Write TFRecord"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odMyYwTKiOTp",
        "colab_type": "code",
        "outputId": "0f585229-e528-4667-9d11-b3266e93d054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "STEPS_PER_EPOCH = len(x_train) // batch_size\n",
        "VALIDATION_STEPS = len(x_test) // batch_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB7o3zu1g6eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_mean = np.mean(x_train, axis=(0,1,2))\n",
        "# train_std = np.std(x_train, axis=(0,1,2))\n",
        "\n",
        "# test_mean = np.mean(x_train, axis=(0,1,2))\n",
        "# test_std = np.std(x_train, axis=(0,1,2))\n",
        "\n",
        "# normalize = lambda x: ((x - train_mean) / train_std).astype('float32')\n",
        "# normalize_test = lambda x: ((x - test_mean) / test_std).astype('float32')\n",
        "\n",
        "# x_train = normalize(x_train)\n",
        "# x_test = normalize(x_test)\n",
        "\n",
        "# # convert to one hot encoing \n",
        "# y_train = utils.to_categorical(y_train, num_classes)\n",
        "# y_test = utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFzIAFwchI72",
        "colab_type": "code",
        "outputId": "54461102-1e0c-4839-c46a-e79bde98b626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "convert_to_tfrecord_data(x_train,y_train, './train.tfrecord')\n",
        "convert_to_tfrecord_data(x_test,y_test, './test.tfrecord')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating ./train.tfrecord\n",
            "WARNING:tensorflow:From /content/rgApi/Data.py:129: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "Generating ./test.tfrecord\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXuOB8hbG6bI",
        "colab_type": "code",
        "outputId": "255e6cb7-16a9-4125-9dd8-40519337a182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "x_trainTF, y_trainTF = create_dataset('./train.tfrecord')\n",
        "x_testTF, y_testTF = create_dataset('./test.tfrecord')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/rgApi/Data.py:118: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l7KVPBLZB50",
        "colab_type": "text"
      },
      "source": [
        "### Functions for Post Training Analysis "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmfsk76-fadV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJMT4rjgfdZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(test_x, test_y, model):\n",
        "    result = model.predict(test_x)\n",
        "    predicted_class = np.argmax(result, axis=1)\n",
        "    true_class = np.argmax(test_y, axis=1)\n",
        "    num_correct = np.sum(predicted_class == true_class) \n",
        "    accuracy = float(num_correct)/result.shape[0]\n",
        "    return (accuracy * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBwfxGXCL4QA",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee-sge5Kg7vr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dense Block\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOP6IPsGhBwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RaKFpubhDIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anPCpQWhhGb7",
        "colab_type": "code",
        "outputId": "c75e1a66-570f-4c8c-fef0-b11c2d5d3631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw5QRZFnozQd",
        "colab_type": "text"
      },
      "source": [
        "### Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kFh7pdxhNtT",
        "colab_type": "code",
        "outputId": "718b4969-2fa0-430b-be92-47193b7ddf34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 16)   432         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 14)   2016        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 32, 32, 14)   0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 32, 32, 30)   0           conv2d[0][0]                     \n",
            "                                                                 dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 30)   120         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 30)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 14)   3780        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 32, 14)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 44)   0           concatenate[0][0]                \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 44)   176         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 44)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 14)   5544        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 14)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 58)   0           concatenate_1[0][0]              \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 58)   232         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 58)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 14)   7308        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 14)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 72)   0           concatenate_2[0][0]              \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 72)   288         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 72)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 14)   9072        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 14)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 86)   0           concatenate_3[0][0]              \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 86)   344         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 86)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 14)   10836       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32, 32, 14)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 100)  0           concatenate_4[0][0]              \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 100)  400         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 100)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 14)   12600       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 32, 14)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 114)  0           concatenate_5[0][0]              \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 114)  456         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 114)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 14)   14364       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32, 32, 14)   0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 128)  0           concatenate_6[0][0]              \n",
            "                                                                 dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 128)  512         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 128)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 14)   1792        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 32, 32, 14)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 16, 16, 14)   0           dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 14)   56          average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 14)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 14)   1764        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 16, 16, 14)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 16, 16, 28)   0           average_pooling2d[0][0]          \n",
            "                                                                 dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 28)   112         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 28)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 14)   3528        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 16, 16, 14)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 16, 16, 42)   0           concatenate_8[0][0]              \n",
            "                                                                 dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 42)   168         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 42)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 14)   5292        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 16, 16, 14)   0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 16, 16, 56)   0           concatenate_9[0][0]              \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 56)   224         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 56)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 14)   7056        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 16, 16, 14)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 16, 16, 70)   0           concatenate_10[0][0]             \n",
            "                                                                 dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 70)   280         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 70)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 14)   8820        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 16, 16, 14)   0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 16, 16, 84)   0           concatenate_11[0][0]             \n",
            "                                                                 dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 84)   336         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 84)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 14)   10584       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 16, 16, 14)   0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 98)   0           concatenate_12[0][0]             \n",
            "                                                                 dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 98)   392         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 98)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 14)   12348       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 16, 16, 14)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 112)  0           concatenate_13[0][0]             \n",
            "                                                                 dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 112)  448         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 112)  0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 14)   14112       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 16, 16, 14)   0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 126)  0           concatenate_14[0][0]             \n",
            "                                                                 dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 126)  504         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 126)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 14)   1764        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 16, 16, 14)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 8, 8, 14)     0           dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 14)     56          average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 14)     0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 14)     1764        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 8, 8, 14)     0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 8, 8, 28)     0           average_pooling2d_1[0][0]        \n",
            "                                                                 dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 28)     112         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 28)     0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 14)     3528        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 8, 8, 14)     0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 8, 8, 42)     0           concatenate_16[0][0]             \n",
            "                                                                 dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 8, 42)     168         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 8, 8, 42)     0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 14)     5292        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 8, 8, 14)     0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 8, 8, 56)     0           concatenate_17[0][0]             \n",
            "                                                                 dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 8, 56)     224         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 8, 8, 56)     0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 14)     7056        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 8, 8, 14)     0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 8, 8, 70)     0           concatenate_18[0][0]             \n",
            "                                                                 dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 8, 8, 70)     280         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 8, 8, 70)     0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 8, 8, 14)     8820        activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 8, 8, 14)     0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 8, 8, 84)     0           concatenate_19[0][0]             \n",
            "                                                                 dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 8, 8, 84)     336         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 8, 8, 84)     0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 8, 8, 14)     10584       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 8, 8, 14)     0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 8, 8, 98)     0           concatenate_20[0][0]             \n",
            "                                                                 dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 8, 8, 98)     392         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 8, 8, 98)     0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 8, 8, 14)     12348       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 8, 8, 14)     0           conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 8, 8, 112)    0           concatenate_21[0][0]             \n",
            "                                                                 dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 8, 8, 112)    448         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 8, 8, 112)    0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 8, 8, 14)     14112       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 8, 8, 14)     0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 8, 8, 126)    0           concatenate_22[0][0]             \n",
            "                                                                 dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 126)    504         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 8, 126)    0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 8, 8, 14)     1764        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 8, 8, 14)     0           conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 4, 4, 14)     0           dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 4, 4, 14)     56          average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 4, 4, 14)     0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 4, 4, 14)     1764        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 4, 4, 14)     0           conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 4, 4, 28)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 4, 4, 28)     112         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 4, 4, 28)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 4, 4, 14)     3528        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 4, 4, 14)     0           conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 4, 4, 42)     0           concatenate_24[0][0]             \n",
            "                                                                 dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 4, 4, 42)     168         concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 4, 4, 42)     0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 4, 4, 14)     5292        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 4, 4, 14)     0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 4, 4, 56)     0           concatenate_25[0][0]             \n",
            "                                                                 dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 4, 4, 56)     224         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 4, 4, 56)     0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 4, 4, 14)     7056        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 4, 4, 14)     0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 4, 4, 70)     0           concatenate_26[0][0]             \n",
            "                                                                 dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 4, 4, 70)     280         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 4, 4, 70)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 4, 4, 14)     8820        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 4, 4, 14)     0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 4, 4, 84)     0           concatenate_27[0][0]             \n",
            "                                                                 dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 4, 4, 84)     336         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 4, 4, 84)     0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 4, 4, 14)     10584       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 4, 4, 14)     0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 4, 4, 98)     0           concatenate_28[0][0]             \n",
            "                                                                 dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 4, 4, 98)     392         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 4, 4, 98)     0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 4, 4, 14)     12348       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 4, 4, 14)     0           conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 4, 4, 112)    0           concatenate_29[0][0]             \n",
            "                                                                 dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 4, 4, 112)    448         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 4, 4, 112)    0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 4, 4, 14)     14112       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 4, 4, 14)     0           conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 4, 4, 126)    0           concatenate_30[0][0]             \n",
            "                                                                 dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 4, 4, 126)    504         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 4, 4, 126)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 2, 2, 126)    0           activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 504)          0           average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           5050        flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 276,986\n",
            "Trainable params: 271,910\n",
            "Non-trainable params: 5,076\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LOFMqeu_7aA",
        "colab_type": "text"
      },
      "source": [
        "### Callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4XOsW3ahSkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# determine Loss function and Optimizer\n",
        "LEARNING_RATE=0.4\n",
        "WEIGHT_DECAY=5e-4\n",
        "lr_schedule = lambda t: np.interp([t+1], [0, (epochs+1)//5, int(0.8*epochs), epochs], [0, LEARNING_RATE, 0.1*LEARNING_RATE, 0.005])[0]\n",
        "\n",
        "model.compile(optimizer=SGD(momentum=0.9, decay=WEIGHT_DECAY, nesterov=True), \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "!mkdir \"Assignment16/\"\n",
        "filepath=\"Assignment16/maxAccuracy.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [LearningRateScheduler(lr_schedule),checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbf09k-fC3Qt",
        "colab_type": "text"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzrHE-02OEgZ",
        "colab_type": "code",
        "outputId": "de11b46f-4ae9-434d-f2cc-a4f008a1e60a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# STEPS_PER_EPOCH = len(x_train) / batch_size\n",
        "start = time.time()\n",
        "model_info = model.fit(x_trainTF, y_trainTF,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_testTF, y_testTF),\n",
        "                    validation_steps = VALIDATION_STEPS, callbacks=callbacks_list)\n",
        "\n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))\n",
        "# compute test accuracy\n",
        "print (\"Accuracy on test data is: %0.2f\"%accuracy(x_test, y_test, model1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 390 samples, validate on ? samples\n",
            "Epoch 1/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.4348 - acc: 0.4722\n",
            "Epoch 00001: val_acc improved from -inf to 0.40785, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 47s 121ms/step - loss: 1.4337 - acc: 0.4726 - val_loss: 1.9950 - val_acc: 0.4079\n",
            "Epoch 2/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0245 - acc: 0.6343\n",
            "Epoch 00002: val_acc improved from 0.40785 to 0.60286, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 77ms/step - loss: 1.0236 - acc: 0.6347 - val_loss: 1.3026 - val_acc: 0.6029\n",
            "Epoch 3/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8255 - acc: 0.7085\n",
            "Epoch 00003: val_acc did not improve from 0.60286\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.8252 - acc: 0.7086 - val_loss: 1.4395 - val_acc: 0.5898\n",
            "Epoch 4/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7201 - acc: 0.7475\n",
            "Epoch 00004: val_acc improved from 0.60286 to 0.61118, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 77ms/step - loss: 0.7198 - acc: 0.7475 - val_loss: 1.3546 - val_acc: 0.6112\n",
            "Epoch 5/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6438 - acc: 0.7748\n",
            "Epoch 00005: val_acc improved from 0.61118 to 0.70142, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 77ms/step - loss: 0.6440 - acc: 0.7747 - val_loss: 0.9877 - val_acc: 0.7014\n",
            "Epoch 6/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5937 - acc: 0.7930\n",
            "Epoch 00006: val_acc improved from 0.70142 to 0.73087, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 77ms/step - loss: 0.5932 - acc: 0.7932 - val_loss: 0.8185 - val_acc: 0.7309\n",
            "Epoch 7/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5468 - acc: 0.8084\n",
            "Epoch 00007: val_acc improved from 0.73087 to 0.75240, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.5468 - acc: 0.8085 - val_loss: 0.7731 - val_acc: 0.7524\n",
            "Epoch 8/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.8192\n",
            "Epoch 00008: val_acc did not improve from 0.75240\n",
            "390/390 [==============================] - 29s 75ms/step - loss: 0.5145 - acc: 0.8192 - val_loss: 0.8285 - val_acc: 0.7421\n",
            "Epoch 9/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4824 - acc: 0.8327\n",
            "Epoch 00009: val_acc did not improve from 0.75240\n",
            "390/390 [==============================] - 29s 76ms/step - loss: 0.4822 - acc: 0.8329 - val_loss: 0.9429 - val_acc: 0.7296\n",
            "Epoch 10/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4537 - acc: 0.8417\n",
            "Epoch 00010: val_acc did not improve from 0.75240\n",
            "390/390 [==============================] - 29s 76ms/step - loss: 0.4535 - acc: 0.8418 - val_loss: 0.8772 - val_acc: 0.7425\n",
            "Epoch 11/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4155 - acc: 0.8556\n",
            "Epoch 00011: val_acc improved from 0.75240 to 0.78075, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.4155 - acc: 0.8556 - val_loss: 0.7450 - val_acc: 0.7807\n",
            "Epoch 12/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.3859 - acc: 0.8638\n",
            "Epoch 00012: val_acc did not improve from 0.78075\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.3860 - acc: 0.8637 - val_loss: 0.9160 - val_acc: 0.7509\n",
            "Epoch 13/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8760\n",
            "Epoch 00013: val_acc improved from 0.78075 to 0.80759, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 77ms/step - loss: 0.3520 - acc: 0.8760 - val_loss: 0.6226 - val_acc: 0.8076\n",
            "Epoch 14/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.8830\n",
            "Epoch 00014: val_acc improved from 0.80759 to 0.81280, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.3348 - acc: 0.8830 - val_loss: 0.6011 - val_acc: 0.8128\n",
            "Epoch 15/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.3095 - acc: 0.8917\n",
            "Epoch 00015: val_acc did not improve from 0.81280\n",
            "390/390 [==============================] - 29s 75ms/step - loss: 0.3099 - acc: 0.8916 - val_loss: 0.6103 - val_acc: 0.8110\n",
            "Epoch 16/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.2882 - acc: 0.8976\n",
            "Epoch 00016: val_acc improved from 0.81280 to 0.81901, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.2884 - acc: 0.8975 - val_loss: 0.6106 - val_acc: 0.8190\n",
            "Epoch 17/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.2758 - acc: 0.9045\n",
            "Epoch 00017: val_acc improved from 0.81901 to 0.83413, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.2757 - acc: 0.9045 - val_loss: 0.5373 - val_acc: 0.8341\n",
            "Epoch 18/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.2568 - acc: 0.9095\n",
            "Epoch 00018: val_acc did not improve from 0.83413\n",
            "390/390 [==============================] - 29s 75ms/step - loss: 0.2570 - acc: 0.9095 - val_loss: 0.6055 - val_acc: 0.8224\n",
            "Epoch 19/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9149\n",
            "Epoch 00019: val_acc improved from 0.83413 to 0.84746, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.2404 - acc: 0.9149 - val_loss: 0.5354 - val_acc: 0.8475\n",
            "Epoch 20/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9204\n",
            "Epoch 00020: val_acc did not improve from 0.84746\n",
            "390/390 [==============================] - 29s 76ms/step - loss: 0.2260 - acc: 0.9205 - val_loss: 0.5448 - val_acc: 0.8417\n",
            "Epoch 21/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.9243\n",
            "Epoch 00021: val_acc improved from 0.84746 to 0.84786, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.2135 - acc: 0.9242 - val_loss: 0.5408 - val_acc: 0.8479\n",
            "Epoch 22/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9286\n",
            "Epoch 00022: val_acc did not improve from 0.84786\n",
            "390/390 [==============================] - 29s 75ms/step - loss: 0.1991 - acc: 0.9286 - val_loss: 0.5494 - val_acc: 0.8432\n",
            "Epoch 23/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9323\n",
            "Epoch 00023: val_acc did not improve from 0.84786\n",
            "390/390 [==============================] - 29s 76ms/step - loss: 0.1916 - acc: 0.9321 - val_loss: 0.5599 - val_acc: 0.8475\n",
            "Epoch 24/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9361\n",
            "Epoch 00024: val_acc improved from 0.84786 to 0.85066, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.1822 - acc: 0.9361 - val_loss: 0.5323 - val_acc: 0.8507\n",
            "Epoch 25/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9390\n",
            "Epoch 00025: val_acc improved from 0.85066 to 0.85136, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 77ms/step - loss: 0.1706 - acc: 0.9390 - val_loss: 0.5546 - val_acc: 0.8514\n",
            "Epoch 26/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.1619 - acc: 0.9429\n",
            "Epoch 00026: val_acc improved from 0.85136 to 0.85357, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.1620 - acc: 0.9427 - val_loss: 0.5425 - val_acc: 0.8536\n",
            "Epoch 27/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.1558 - acc: 0.9440\n",
            "Epoch 00027: val_acc did not improve from 0.85357\n",
            "390/390 [==============================] - 29s 76ms/step - loss: 0.1557 - acc: 0.9441 - val_loss: 0.5728 - val_acc: 0.8456\n",
            "Epoch 28/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9473\n",
            "Epoch 00028: val_acc improved from 0.85357 to 0.85767, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.1471 - acc: 0.9472 - val_loss: 0.5460 - val_acc: 0.8577\n",
            "Epoch 29/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9510\n",
            "Epoch 00029: val_acc improved from 0.85767 to 0.86128, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.1371 - acc: 0.9510 - val_loss: 0.5292 - val_acc: 0.8613\n",
            "Epoch 30/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9540\n",
            "Epoch 00030: val_acc did not improve from 0.86128\n",
            "390/390 [==============================] - 29s 76ms/step - loss: 0.1287 - acc: 0.9539 - val_loss: 0.5610 - val_acc: 0.8567\n",
            "Epoch 31/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9560\n",
            "Epoch 00031: val_acc improved from 0.86128 to 0.86198, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 77ms/step - loss: 0.1235 - acc: 0.9561 - val_loss: 0.5585 - val_acc: 0.8620\n",
            "Epoch 32/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.9582\n",
            "Epoch 00032: val_acc did not improve from 0.86198\n",
            "390/390 [==============================] - 29s 76ms/step - loss: 0.1166 - acc: 0.9582 - val_loss: 0.5688 - val_acc: 0.8581\n",
            "Epoch 33/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9583\n",
            "Epoch 00033: val_acc improved from 0.86198 to 0.86619, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.1156 - acc: 0.9584 - val_loss: 0.5576 - val_acc: 0.8662\n",
            "Epoch 34/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9590\n",
            "Epoch 00034: val_acc did not improve from 0.86619\n",
            "390/390 [==============================] - 29s 75ms/step - loss: 0.1130 - acc: 0.9591 - val_loss: 0.5498 - val_acc: 0.8640\n",
            "Epoch 35/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9622\n",
            "Epoch 00035: val_acc improved from 0.86619 to 0.86729, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 77ms/step - loss: 0.1061 - acc: 0.9622 - val_loss: 0.5535 - val_acc: 0.8673\n",
            "Epoch 36/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9638\n",
            "Epoch 00036: val_acc did not improve from 0.86729\n",
            "390/390 [==============================] - 29s 76ms/step - loss: 0.1019 - acc: 0.9638 - val_loss: 0.5506 - val_acc: 0.8657\n",
            "Epoch 37/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9640\n",
            "Epoch 00037: val_acc improved from 0.86729 to 0.86739, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.0998 - acc: 0.9640 - val_loss: 0.5499 - val_acc: 0.8674\n",
            "Epoch 38/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9662\n",
            "Epoch 00038: val_acc did not improve from 0.86739\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.0948 - acc: 0.9661 - val_loss: 0.5592 - val_acc: 0.8645\n",
            "Epoch 39/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9674\n",
            "Epoch 00039: val_acc improved from 0.86739 to 0.86919, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.0927 - acc: 0.9674 - val_loss: 0.5541 - val_acc: 0.8692\n",
            "Epoch 40/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9691\n",
            "Epoch 00040: val_acc did not improve from 0.86919\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.0890 - acc: 0.9692 - val_loss: 0.5558 - val_acc: 0.8675\n",
            "Epoch 41/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9695\n",
            "Epoch 00041: val_acc did not improve from 0.86919\n",
            "390/390 [==============================] - 29s 75ms/step - loss: 0.0862 - acc: 0.9696 - val_loss: 0.5493 - val_acc: 0.8692\n",
            "Epoch 42/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9694\n",
            "Epoch 00042: val_acc improved from 0.86919 to 0.86969, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.0868 - acc: 0.9694 - val_loss: 0.5668 - val_acc: 0.8697\n",
            "Epoch 43/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9696\n",
            "Epoch 00043: val_acc improved from 0.86969 to 0.87059, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.0848 - acc: 0.9696 - val_loss: 0.5502 - val_acc: 0.8706\n",
            "Epoch 44/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9716\n",
            "Epoch 00044: val_acc did not improve from 0.87059\n",
            "390/390 [==============================] - 29s 75ms/step - loss: 0.0831 - acc: 0.9717 - val_loss: 0.5673 - val_acc: 0.8673\n",
            "Epoch 45/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9702\n",
            "Epoch 00045: val_acc did not improve from 0.87059\n",
            "390/390 [==============================] - 29s 75ms/step - loss: 0.0838 - acc: 0.9702 - val_loss: 0.5729 - val_acc: 0.8693\n",
            "Epoch 46/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9716\n",
            "Epoch 00046: val_acc improved from 0.87059 to 0.87109, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.0805 - acc: 0.9716 - val_loss: 0.5474 - val_acc: 0.8711\n",
            "Epoch 47/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9708\n",
            "Epoch 00047: val_acc did not improve from 0.87109\n",
            "390/390 [==============================] - 29s 75ms/step - loss: 0.0831 - acc: 0.9708 - val_loss: 0.5602 - val_acc: 0.8694\n",
            "Epoch 48/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9717\n",
            "Epoch 00048: val_acc improved from 0.87109 to 0.87159, saving model to Assignment16/maxAccuracy.hdf5\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.0805 - acc: 0.9715 - val_loss: 0.5567 - val_acc: 0.8716\n",
            "Epoch 49/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9716\n",
            "Epoch 00049: val_acc did not improve from 0.87159\n",
            "390/390 [==============================] - 29s 75ms/step - loss: 0.0804 - acc: 0.9716 - val_loss: 0.5592 - val_acc: 0.8673\n",
            "Epoch 50/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9722\n",
            "Epoch 00050: val_acc did not improve from 0.87159\n",
            "390/390 [==============================] - 29s 75ms/step - loss: 0.0798 - acc: 0.9722 - val_loss: 0.5735 - val_acc: 0.8668\n",
            "Model took 1506.60 seconds to train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5fa2b4c9d9e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Model took %0.2f seconds to train\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# plot model history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mplot_model_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# compute test accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy on test data is: %0.2f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-e05d9b01600e>\u001b[0m in \u001b[0;36mplot_model_history\u001b[0;34m(model_history)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_model_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJRFQ15V9emo",
        "colab_type": "code",
        "outputId": "788611e6-71f2-4116-b914-cd08cdbd286b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plot_model_history(model_info)\n",
        "print (\"Accuracy on test data is: %0.2f\"%accuracy(x_test, y_test, model))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFNCAYAAABSRs15AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test data is: 10.16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHBzYFg2RmqV",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcWydmIVhZGr",
        "colab_type": "code",
        "outputId": "3b400342-745d-4411-bf22-fb703c53c7b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Test the model\n",
        "model.load_weights('./Assignment16/maxAccuracy.hdf5')\n",
        "score = model.evaluate(x_testTF, y_testTF, verbose=1, steps = VALIDATION_STEPS)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78/78 [==============================] - 1s 18ms/step - loss: 0.5466 - acc: 0.8733\n",
            "Test loss: 0.5465542777226522\n",
            "Test accuracy: 0.8732973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8n2RrbN7GdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}