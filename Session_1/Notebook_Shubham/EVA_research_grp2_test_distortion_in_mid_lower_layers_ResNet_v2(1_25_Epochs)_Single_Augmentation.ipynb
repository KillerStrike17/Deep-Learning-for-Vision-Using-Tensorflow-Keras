{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA_research_grp2_test_distortion_in_mid_lower_layers_ResNet_v2(1-25 Epochs)-Single Augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KillerStrike17/EVA---Session-1-Basics-Of-DNN/blob/master/Notebook_Shubham/EVA_research_grp2_test_distortion_in_mid_lower_layers_ResNet_v2(1_25_Epochs)_Single_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynqEXKR8Quk9",
        "colab_type": "text"
      },
      "source": [
        "### Choose tf2 backend on colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzpm6z7xyQ2m",
        "colab_type": "code",
        "outputId": "308cfee4-c965-4f58-fcdf-556e974f6c06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /tensorflow-2.0.0-rc2/python3.6 (from tensorflow_addons) (1.12.0)\n",
            "Requirement already satisfied: tensorflow-gpu==2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_addons) (2.0.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /tensorflow-2.0.0-rc2/python3.6 (from tensorflow-gpu==2.0.0->tensorflow_addons) (3.9.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /tensorflow-2.0.0-rc2/python3.6 (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /tensorflow-2.0.0-rc2/python3.6 (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.17.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /tensorflow-2.0.0-rc2/python3.6 (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.33.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /tensorflow-2.0.0-rc2/python3.6 (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /tensorflow-2.0.0-rc2/python3.6 (from tensorflow-gpu==2.0.0->tensorflow_addons) (3.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /tensorflow-2.0.0-rc2/python3.6 (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.11.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /tensorflow-2.0.0-rc2/python3.6 (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-2.0.0-rc2/python3.6 (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /tensorflow-2.0.0-rc2/python3.6 (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.8.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /tensorflow-2.0.0-rc2/python3.6 (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.2.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /tensorflow-2.0.0-rc2/python3.6 (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.1.7)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /tensorflow-2.0.0-rc2/python3.6 (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.24.0)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /tensorflow-2.0.0-rc2/python3.6 (from tensorflow-gpu==2.0.0->tensorflow_addons) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /tensorflow-2.0.0-rc2/python3.6 (from protobuf>=3.6.1->tensorflow-gpu==2.0.0->tensorflow_addons) (41.2.0)\n",
            "Requirement already satisfied: h5py in /tensorflow-2.0.0-rc2/python3.6 (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0->tensorflow_addons) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /tensorflow-2.0.0-rc2/python3.6 (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow_addons) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /tensorflow-2.0.0-rc2/python3.6 (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow_addons) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFPFRpPDQ2e6",
        "colab_type": "text"
      },
      "source": [
        "### Install tf_utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0yHfmeMv-rI",
        "colab_type": "code",
        "outputId": "8dfb01da-00ba-4ae8-ebdf-4de1d55f5901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "!pip install --upgrade git+https://github.com/ravindrabharathi/tf_utils "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ravindrabharathi/tf_utils\n",
            "  Cloning https://github.com/ravindrabharathi/tf_utils to /tmp/pip-req-build-wfcke796\n",
            "  Running command git clone -q https://github.com/ravindrabharathi/tf_utils /tmp/pip-req-build-wfcke796\n",
            "Building wheels for collected packages: tf-utils\n",
            "  Building wheel for tf-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tf-utils: filename=tf_utils-0.1-cp36-none-any.whl size=8427 sha256=659debaed37916194883cc1cf40bf9eb98cc813335ff89b330f7228a31e6477e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c563cgai/wheels/95/af/bb/690b94c65a5aad47a5c39e75f158a2b043448e908c5c121791\n",
            "Successfully built tf-utils\n",
            "Installing collected packages: tf-utils\n",
            "Successfully installed tf-utils-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH-5ffTMQ8ms",
        "colab_type": "text"
      },
      "source": [
        "### import the data module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBcelJMpwYoX",
        "colab_type": "code",
        "outputId": "e5c2727a-496e-4d36-d71f-a8c206e36077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tf_utils.data as ds"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished 'get_cpu_num' in 0.0000 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb0kbJalRCPJ",
        "colab_type": "text"
      },
      "source": [
        "### set batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJTA3CO8xOtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=512\n",
        "ds.batch_size=batch_size\n",
        "EPOCHS=24"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTroxaquRE2Z",
        "colab_type": "text"
      },
      "source": [
        "### downlaod data and create tf records"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF3qoDnbykb6",
        "colab_type": "code",
        "outputId": "67ad47aa-e3c5-4e7d-c519-a278b3a800f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "ds.get_cifar10_and_create_tfrecords()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4583fed919648e4846c483677c81358",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='cifar-10-python.tar.gz', max=170498071, style=ProgressStyle(dâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Finished 'download_file' in 6.5977 secs\n",
            "Finished 'download_cifar10_files' in 6.5979 secs\n",
            "Done\n",
            "Finished 'extract_cifar10_files' in 2.0207 secs\n",
            "Finished '_get_file_names' in 0.0000 secs\n",
            "Generating ./train.tfrecords\n",
            "Finished 'read_pickle_from_file' in 0.1640 secs\n",
            "Finished 'read_pickle_from_file' in 0.1685 secs\n",
            "Finished 'read_pickle_from_file' in 0.1501 secs\n",
            "Finished 'read_pickle_from_file' in 0.1534 secs\n",
            "Finished 'read_pickle_from_file' in 0.1802 secs\n",
            "Finished 'convert_to_tfrecord' in 3.3129 secs\n",
            "Done!\n",
            "Generating ./eval.tfrecords\n",
            "Finished 'read_pickle_from_file' in 0.1500 secs\n",
            "Finished 'convert_to_tfrecord' in 0.7154 secs\n",
            "Done!\n",
            "Finished 'create_tf_records' in 4.0304 secs\n",
            "Finished 'get_cifar10_and_create_tfrecords' in 12.6495 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U-_CmC6RKDF",
        "colab_type": "text"
      },
      "source": [
        "### create train and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lYOp7UKyoG5",
        "colab_type": "code",
        "outputId": "d7f07c91-d984-4faa-c9d2-d5b2825dad36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "train_ds1=ds.get_train_ds(batch_size=batch_size,shuffle=True,distort=True)\n",
        "train_ds2=ds.get_train_ds(batch_size=batch_size,shuffle=True,distort=False)\n",
        "\n",
        "test_ds=ds.get_eval_ds(batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distorting image\n",
            "Finished 'get_tf_dataset' in 2.8128 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 2.8132 secs\n",
            "Finished 'get_train_ds' in 2.8134 secs\n",
            "Finished 'get_tf_dataset' in 0.1904 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 0.1908 secs\n",
            "Finished 'get_train_ds' in 0.1911 secs\n",
            "Finished 'get_tf_dataset' in 0.0337 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 0.0341 secs\n",
            "Finished 'get_eval_ds' in 0.0342 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7z9t4e5RRbH",
        "colab_type": "text"
      },
      "source": [
        "### import visualization module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHq2g5VjzmDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tf_utils.visualize as vz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utItaz0RRXft",
        "colab_type": "text"
      },
      "source": [
        "### plot images from train dataset1 , train dataset by default uses image augmenttation of cutout,flip-left-right,random-pad-crop "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvonf0IP0uOX",
        "colab_type": "code",
        "outputId": "db40fa7d-4144-4963-b9f4-b9014c60b633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vz.plot_cifar10_files(train_ds1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHZlJREFUeJztnXuQnGd15p/Tl+m5azSakTTWxSPJ\nii7YSBbjC+AErxOCTUgZ1wYKUxgv5bWo3ZAKu+xWOd6q4M2masluDMXuH6QEdsVkCdjBEBzizWK8\ngDDBskdClmQkW6ObLWk0o+vcp6cvZ//oViLNvudMay49Eu/zq1Kp5z39ft/pd77T3/T79DlHVBWE\nkPhIzLcDhJD5gcFPSKQw+AmJFAY/IZHC4CckUhj8hEQKg5+QSGHwExIpDH5CIiU1k8kicjeALwNI\nAviaqn7Be35T0wJtb18ctB050jMTV65aOpcvM23ZfNG0pVI1pq2mJmnampqbg+MJEduP7LhpO3fy\nmGlrWbrctA0MDgXHR0fD4yXse9HERM60jY6OmbZ0Oh0cr8lkbDcKedMkCXsdC0X795nL2cc0X7Z9\nKsD4Ym4hX0SxUPRm/vPhp/v1XhFJAngTwPsBHAfwKoD7VfWX1pzVq9fqn/yX/xG0PfCJD07Lj6ud\nrz/+X01bzzn7om1rswNrZUeLabvzA+8PjmfS9sV+uMf8leHpP/m0abv3Pz5u2r7/wg+D47t3/dSc\nI0nbx2PHTpu2nTt/Ydo6lofXsbOz05xTHDpj2tJ1taZteGzEtJ08ZfsvNeHol7Qdw5oLx+3ZUwPI\nZfMVBf9M/uy/FUCPqh5W1QkA3wJw7wyORwipIjMJ/mUA3r7k5+PlMULINcCcb/iJyFYR6RaR7sHB\nwbk+HSGkQmYS/CcArLjk5+XlsctQ1W2q2qWqXc3GZhQhpPrMJPhfBbBWRFaJSA2AjwF4bnbcIoTM\nNdOW+lQ1LyKfAfB/UJL6nlTV1705tZkMblx7/XRPeU1yenjCtK3f8A7Tdsftt5m2xR1huRQAVA25\nyRF1Fi+xlYXl628xbWvXrjdtt5wL73wnxZYpd7z6smkbG8+ati1bbjZtt9/23uD4DWvWmHP27ftH\n01Ys2CEzNl4wbYNDtiKRNV7bmX5bIWhb1B4cFx0250xmRjq/qj4P4PmZHIMQMj/wG36ERAqDn5BI\nYfATEikMfkIihcFPSKRMO7FnOqxbt1G3/cVfBW133tVVNT+qyd//3f81be++bYtpq28IZ6MBgBav\n/HfmZfV5hxu8cN60pdJ25qF1XSWTtsCUd67FM2fPmrbXd+0ybaNj4YzFH73ykjnn8JE3TdvgkP0t\n1aFh23bLpnebtn/3B58Ljh84YPuxfv2vBcc/+alPYP/+X855Yg8h5BqGwU9IpDD4CYkUBj8hkcLg\nJyRSZvTd/itGFYWCXefsV5H3/bqtYiTV2YF36vt5deTE2NUvOgXhErDP1dLaatry7u8yvHPvqQ4e\nTY128tGC+jrTdtZQK37r7nC5MwAYcZKxevtOmraeQ/bu/MiIXSdxUcui4PiWzfa1U1cbVoNSjpoy\nGd75CYkUBj8hkcLgJyRSGPyERAqDn5BIYfATEinVlfpEkEzG9X7zk592m7a5SOyxEmocddCVAQfP\nnTNtTOy5nOkm9tx2S7he43QSe/JOq7HJxBWJhJB/gsFPSKQw+AmJFAY/IZHC4CckUhj8hETKjKQ+\nETkKYAhAAUBeVd1CfMlEAgsa7AysX0UO7PqpaRsYsNsxzXa7rqKTQTjsSFTf+8qfmraP/fvHTNtL\nO3YGx3ftesWc47XrOnT4bdNWm8mYNqtd103rbzTnSN5ej2LBzi702nXt2PGqabvvJ/cFx8+cvfJ2\nXf39p8w5k5kNnf9fqOqZWTgOIaSK8M9+QiJlpsGvAH4gIjtFZOtsOEQIqQ4z/bP/DlU9ISKLAbwg\nIgdUdfulTyi/KWwFgI6lHTM8HSFktpjRnV9VT5T/7wfwXQC3Bp6zTVW7VLWrpcUuCUUIqS7TDn4R\naRCRpouPAfw2gH2z5RghZG6ZyZ/9SwB8t1wwMgXgr1X1H7wJ49ks9h08NoNTXnu0N9qZbwf2v27a\nzpweMG0rO1pM250fCBemzKRtOay/77hpO37AlqgOHjxg2l7d+fPg+G5H+kw7PtbV2radO39h2nr7\nw3JZZ2enOac4ZItX6bpa0zY8NmLbRmzZTmrC9+CFy+rNOfnccHBcxZYbJzPt4FfVwwA2TXc+IWR+\nodRHSKQw+AmJFAY/IZHC4CckUhj8hESKWIUW54JkulbrWjuDtpH+N6rmRzX5o0f/2LQ1NzSZtsHR\nrGkb7n/LtP3ahg3B8fs//glzzj98+2umbfuPXzBthwZtGfP65eHst+0/swtnesUx87kx0zY8YEts\njQ1huWzMKWg6NmzLrKtWLTVtXs/ATK0trDUvNL78JnYRV2guOPzyS3sxODBcUUNE3vkJiRQGPyGR\nwuAnJFIY/IRECoOfkEiparuuYiGP7AWjxliDnevffF14h3X92hvMOfd0vcO0vf92u03WyPCoaTs3\nOBQcf/3QUXPO4ZO9pm24YNuWNNrJO+vWdJq2fb8I18F7ZXVYBQCAv/3BP5q2Bet+07Sdf3m7aXvx\nybCCIPX2RnRbe6Npa2+112N0PNySCwAWXxe+rnIpeyf9zKk+07a0zb5Ob7n5naatvX2ZaduwLjxv\n+XXXm3OOnwwnyB068EfmnMnwzk9IpDD4CYkUBj8hkcLgJyRSGPyERAqDn5BIqarUtzSdwEPLm4O2\nQ80LzXkta1YHx5csv86cU4Bdy+x/vWAnl5zos2WeG29YFRzvXGYne0yM2wkph07YtfN2HbOTd1BY\naZpamsMS1pEjPeaco2fD9eAAYGPtItN2y50fMm37X/tZcLyh0a6B9+u3vce0DZ61k23e2Ge38jrU\nczg4/vDDD5tzbr3Jluyuu85u1/XOd202bfW1toyZNZK4hgftRKc1q9cEx5sa7WSxyfDOT0ikMPgJ\niRQGPyGRwuAnJFIY/IRECoOfkEiZUuoTkScBfAhAv6reWB5rBfA0gE4ARwF8VFXPT3WsjlWdeHTb\n/wza6gfsbLriiRPB8cO9R805J06EJR4AeLPvpGkbP27bXusJtyLsWWdnzI3m8qZtadti03bXSltS\nyg7b0lyxED7f17751+act3vD6wsA+b17TNvv/NYHTNtH7vtIcPz6DrtZ6/o160zb3gN2a7N8wa53\neORouDbk/tftFl933GxnfbY22dmFAjtjcWDQDo/iRPh3VihMmHNGR8MyYLFYebuuSu78fwng7klj\njwB4UVXXAnix/DMh5BpiyuBX1e0Azk0avhfAU+XHTwH48Cz7RQiZY6b7mX+Jql6sRHEKpY69hJBr\niBlv+Gmp8L9Z/F9EtopIt4h0n7lgf0WTEFJdphv8fSLSAQDl//utJ6rqNlXtUtWutpYF0zwdIWS2\nmW7wPwfgwfLjBwF8b3bcIYRUi0qkvm8CuBNAm4gcB/B5AF8A8IyIPATgGICPVnKygYkcvn8i/EdC\n0SmouGzjxuD4olvfZc5Zl7bf194zbksoD/fbWX19+3YGx/fu3G3OefmNN03broItDZ3ssIs3Ni6x\n5bI1ixqC46119q9675mzpi15fL9p+9/PT94H/mc2Lgh/Ely55CZzTkN92HcAyOaLpu0Ptv5b0/ba\nnleD4994+m/NOd/+u++Ytoc++SnTthLhrE8ASHgNtAxjIpG0p7gHrIwpg19V7zdMdllXQshVD7/h\nR0ikMPgJiRQGPyGRwuAnJFIY/IRESlULeGazWRw9eDBoSyRt6WLk+JHgeHOzXRSxvq7eti2w59U6\nttZ7fjc4vuX3HjDnvOf8GdOW3W9nzPXssuXDl07ZPf72nB4JjmccGW1xm52pVhg2eisCSGTsHnmd\nG24Ojl+/ypYws1lbgk1nbCm4rdX+dvm9H/54cPzkmfA6AcDLr9q9C8VW39w7acL8DixQNGwiziTH\nVCm88xMSKQx+QiKFwU9IpDD4CYkUBj8hkcLgJyRSqir1DQ2dw09eeiZoW750mTnPknIW1Nv1AWoz\nGdOWqbFfdsrJLmyoCx8z5Zyrptn2saXdlqhWPvSQaWv98YumLfHznwfH71thZwLusBVHZBbYPRSl\nMdx3EQD+/sWwj40t9vFWr7BlwPWORLjCKXZaNDLjNm+2+/HV1dv9BG/ceKN9LqN4KgBAncKaRtHN\nQsGek0oa9+0rkAB55yckUhj8hEQKg5+QSGHwExIpDH5CIqWqu/2SSCGRCSeR/Lg7XB8PAJobw8k2\nHYvs3eb2lib7eHVOQlCtnRBU21AXHE8476H5nF17LuftzCZt1eH4Kbt23vIl1wXHLyxaZM6pdZKg\nzvXbCTAbWmwFoX1BOJHolR9+35xztPMG07Zgkd3abP0N9rxlnWuC41qwF//Tn/rXpq2h0b6uck5r\nNnUTe8LXiDXu2yrf7uedn5BIYfATEikMfkIihcFPSKQw+AmJFAY/IZFSSbuuJwF8CEC/qt5YHnsM\nwMMATpef9qiqPj/VsRoamnHbLfcEbd27DpvzNq0L14NrbbVll97T4VqBADAwavYVRV2N/X4op8Py\nSsFpuzWazZm2kQm7Bt54zvZj/coNpm1pazjJaO/BcB1EAGhptJOPNm6wW6KtL75l2u7ZdGtwfHjc\nXquBCdu2t8+WNxOOLLrQkCP7Tpw05/T12nULW1ttybToJOJo3rZZCTxeolChEL4+rqS0XyV3/r8E\ncHdg/Euqurn8b8rAJ4RcXUwZ/Kq6HYD9tksIuSaZyWf+z4jIHhF5UkTsJG1CyFXJdIP/KwDWANgM\noBfA49YTRWSriHSLSPfIyOg0T0cImW2mFfyq2qeqBVUtAvgqgPDuTum521S1S1W7Ghrs75ATQqrL\ntIJfRDou+fE+APtmxx1CSLWoROr7JoA7AbSJyHEAnwdwp4hsRklZOArg05Wc7Pz583j22aeDtoUL\n7ZZRR94Ky4DJRDhjCwCaG1eYtoa0/Z5Xk7HFknQqLEVls/bHmcGTffa5xH7N6Ywte13X0WHa3jh4\nPDje3GBnMqo665G2+1Mde+u8aXutJ5wNmF5m1+JrXmzX4lvdFs5WBIDm9qWm7dSZ8F513zm7jVrS\nqclYyNvSrSv1Fe0MPbWy+opelqARul764CSmDH5VvT8w/ETFZyCEXJXwG36ERAqDn5BIYfATEikM\nfkIihcFPSKRUtYBnLpdHb39Yerl5yyZzXvfOXcHxmzbdZM750faXHD8cOcSRa9bdsCo4vqKjzZwz\ncMGW7JyuUNi0fqVpW7nUlr062sPzeg7vNeeMD9lZjpnxs6at/aZbTFuyJXxfWbK43ZxT02zbun/2\nsmnbs++/m7aJ7ET4eK/tNuec7O01bZ/8yL80bRs3vMO01TfZxWYnctngeM6RFSURvq70CqQ+3vkJ\niRQGPyGRwuAnJFIY/IRECoOfkEhh8BMSKVWV+tLpFDqWhuWcw4d7zHnFibAUUjNhZ0q1L7D7yG24\ncb1pGx0ZM20XBgeC428cOWrO6eu9YNrGhu0+eHe/7w7TNjphv2cvaAv/Ss+N2tl5RbWzx2oT4bUH\ngMb6cO9CAMimw5lxu3uOmnNWLbdlqsY6+1zpjP3aGjPh4p7ffettc06yYF9Xe/bZkqk4/f8WttrF\nrsbGw4Vcm5vswqqDQ+FrMWscKwTv/IRECoOfkEhh8BMSKQx+QiKFwU9IpFR1t39J2yL84b/6eNB2\nYXjQnFdbE3azo95u1/U7d9m75cuvt5NmIE47qdFwrb6+c3byy3jO3klP2qfC6uV2nb5Uys4IytSG\nz7dooX28d23aaNo6li4xbTVJe5d92RIj2cmZk3daWm15p63QNDXZVaHHT4aTdJo77WsgI86ufbut\nIjWkHbWixrYlDZVg5IKdcNVYWxMcTzi+/3/PrfiZhJBfKRj8hEQKg5+QSGHwExIpDH5CIoXBT0ik\nVNKuawWArwNYglJ7rm2q+mURaQXwNIBOlFp2fVRV7f5NADI1aaxdEa4/19gQro8HAClD6qupCSdt\nAEA6bdvgSGyZTFhCAYAVmbDMs6XObhuWMhJcACCRsn3UvC0RJsROPBGE/d94gy2jpd5n9llFzvHD\nKxenxfD5is6kgtPSyqutmHcScfJt4dp5azfZ9fZyWTuZyUrCAYCxMTspbNyZl9LwBZlylqOuNjwn\nadT2C1HJnT8P4HOquhHA7QB+X0Q2AngEwIuquhbAi+WfCSHXCFMGv6r2ququ8uMhAPsBLANwL4Cn\nyk97CsCH58pJQsjsc0Wf+UWkE8DNAHYAWKKqF78+dQqljwWEkGuEioNfRBoBPAvgs6p62XdxtVQs\nPPhhTkS2iki3iHRfGBiakbOEkNmjouAXkTRKgf8NVf1OebhPRDrK9g4AwS8iq+o2Ve1S1a6WBfZ3\n8Qkh1WXK4BcRAfAEgP2q+sVLTM8BeLD8+EEA35t99wghc0UlWX3vBfAAgL0icrHH0aMAvgDgGRF5\nCMAxAB+d6kBaLGJ83KhbV7TllXRNOBOskLHruhUdGTDj2HJOVlRRw3JTPhduCQUA6ZQt/0jCfu+V\nlJ39BrHnJcSQxMSW7IqOVOb56N05NBG+tNRYQwDQgu1jwcn4KzrzYLy0kZxdP3HCycQs5mw/Co4s\nipzdeithyKLirJU1x/j0HWTK4FfVl2Ar479Z8ZkIIVcV/IYfIZHC4CckUhj8hEQKg5+QSGHwExIp\nVS3gqcUCcobUl07YRSnTybA0p3n7vSufdOQrR5JJOa2rEjB8VPtcRdhyTTJl+68FR2Jz1gpWey0j\ncwwAxJGHEs5rSxRt+apYMGzO8eBk9VkKJgAUvfU3pLmcI70VPFvWec0Tti3nyMETE2EfJxw/rOw9\nLbKAJyFkChj8hEQKg5+QSGHwExIpDH5CIoXBT0ikVFfqU7sgZM6RScxMME82stK5ACTgFPd0pZKw\nTTPO8dRbYjtzL+nM02S4ZyBgS4Redp5X0dRbjbyTdWYes2ifq+hIsEUziw1Qp7hn0fp9er57Nifr\ns1Ccnv95QxadmPCyRQ2p7wqy+njnJyRSGPyERAqDn5BIYfATEikMfkIipaq7/YBCjB16r8lQQsJW\nL6FDnbZQRSeBJJ32EmCMcWeDtejsRBsvq4x90KRXwy9p/EqdtfLc8Fpy+UZrt9+eUfQShTwlIO/U\nxzN8LDi1+Ip557py1CCrxiPg78IXzCQop6ahoep41/1keOcnJFIY/IRECoOfkEhh8BMSKQx+QiKF\nwU9IpEwp9YnICgBfR6kFtwLYpqpfFpHHADwM4HT5qY+q6vP+0RRqJD8UnJZLaaP+nDjui6OjqSP1\nTSdZyJPsLJmyZHPeez0XC85rM44pzgG9pJOEkxDktcnKG+21vLZb3jXgyoqOzarVVyw4kp3jY95p\nbVZ0E3uuXKqcyNoJXDVGYo8vv15OJTp/HsDnVHWXiDQB2CkiL5RtX1LVP6/4bISQq4ZKevX1Augt\nPx4Skf0Als21Y4SQueWKPvOLSCeAmwHsKA99RkT2iMiTIrJwln0jhMwhFQe/iDQCeBbAZ1V1EMBX\nAKwBsBmlvwweN+ZtFZFuEekeGLY/wxBCqktFwS8iaZQC/xuq+h0AUNU+VS2oahHAVwHcGpqrqttU\ntUtVuxY01s+W34SQGTJl8Etp2/wJAPtV9YuXjHdc8rT7AOybffcIIXNFJbv97wXwAIC9IrK7PPYo\ngPtFZDNK8t9RAJ+e6kCqatYrKxTsOnimAlTjue9kxSXt2nl5p0WSGC2SvNZPRUcq8+rqJVO2jwkn\njbCYD7fr8rK90il7HYuOtOXJdpZ8mEh6EqyjmTqyaM6pdVcwJD1P7nVrAjqv2TumWUsQ9nXlKcHW\nHDdFcxKV7Pa/ZBxyCk2fEHI1w2/4ERIpDH5CIoXBT0ikMPgJiRQGPyGRUtUCnkVVZCfGg7a62ow5\nz8p+04Ij8eScQpzpWtOWTNgSW8HIvjJlFwAJsZfYa8eUcDL33EzBZHitrHEAKDhZfbkJOxvNlcuM\nYpbJ1PSkTxiZnQCgnr5l/D4LnjzrZn3apmLOXqvx8fB1DwDZbFienXBa2I2nwnNc3yfBOz8hkcLg\nJyRSGPyERAqDn5BIYfATEikMfkIipapSX0IEmVQ4e0/dvnvhca+Yop0KCCSdvm8Jp99awsgGFEdG\n83q7JZK2/0lHvhInK9HMBHOWKu/IQwVnrbzCpZYt60heCbXXMe/4UXDW2LLkHXnTK2ha8DL+nHme\nrGu9tvGxMXNOTU1N2FB5/U7e+QmJFQY/IZHC4CckUhj8hEQKg5+QSGHwExIpVZX6ALuH3vi4LWuk\nDSktXWMX/XRqXCKXtWUXr7in5Yd6RS4djS3lFOlMOX54Pf4sX7wMPK8fn5OE50p9VsFQT85zC2d6\nGZx5+5dt9Qz0iq5665vLOZKd479XQNWS+nKOdGjN8c4zGd75CYkUBj8hkcLgJyRSGPyERAqDn5BI\nmXK3X0RqAWwHkCk//9uq+nkRWQXgWwAWAdgJ4AFVtbdCUUqKGBgaDNrq6+26ehPGzmyNU18ukbG3\nooteKy8j8chDnB39gqMEFJ3dYSePBQnnPVuN15Zyko/MzCkABaO92sWzWVjiwrQThbxz5Z1aiAir\nJl6txuxEuD4eAOScOn2WsgD4CUHW9e0dz5oz27v9WQB3qeomlNpx3y0itwP4MwBfUtUbAJwH8FDF\nZyWEzDtTBr+WGC7/mC7/UwB3Afh2efwpAB+eEw8JIXNCRZ/5RSRZ7tDbD+AFAIcAXFDVi38DHQew\nbG5cJITMBRUFv6oWVHUzgOUAbgWwvtITiMhWEekWke6hEftbfISQ6nJFu/2qegHAjwC8G0CLyD91\npFgO4IQxZ5uqdqlqV1ND3YycJYTMHlMGv4i0i0hL+XEdgPcD2I/Sm8DvlZ/2IIDvzZWThJDZp5LE\nng4AT4lIEqU3i2dU9fsi8ksA3xKRPwXwCwBPTHUgBTBRCEsRaSc5Y2TMkHIStizX6CXoOK9as/ZH\nE9Vw3bSU12bKwUsw0oLtpJUcBQDpWqtGopN0Ylp8icqX7cJrkhsfMec4pfiQSHq/NEd+M47p1f3L\nO22yvBZaXqespJMsVDTqTU44EqY1x5J6Q0wZ/Kq6B8DNgfHDKH3+J4Rcg/AbfoRECoOfkEhh8BMS\nKQx+QiKFwU9IpMiVZAHN+GQipwEcK//YBuBM1U5uQz8uh35czrXmx/Wq2l7JAasa/JedWKRbVbvm\n5eT0g37QD/7ZT0isMPgJiZT5DP5t83juS6Efl0M/LudX1o95+8xPCJlf+Gc/IZEyL8EvIneLyBsi\n0iMij8yHD2U/jorIXhHZLSLdVTzvkyLSLyL7LhlrFZEXRORg+f+F8+THYyJyorwmu0Xkg1XwY4WI\n/EhEfikir4vIH5bHq7omjh9VXRMRqRWRV0TktbIf/7k8vkpEdpTj5mkRCaeZVoqqVvUfgCRKZcBW\nA6gB8BqAjdX2o+zLUQBt83De3wCwBcC+S8b+G4BHyo8fAfBn8+THYwD+Q5XXowPAlvLjJgBvAthY\n7TVx/KjqmgAQAI3lx2kAOwDcDuAZAB8rj/8FgH8zk/PMx53/VgA9qnpYS6W+vwXg3nnwY95Q1e0A\nzk0avhelQqhAlQqiGn5UHVXtVdVd5cdDKBWLWYYqr4njR1XREnNeNHc+gn8ZgLcv+Xk+i38qgB+I\nyE4R2TpPPlxkiar2lh+fArBkHn35jIjsKX8smPOPH5ciIp0o1Y/YgXlck0l+AFVek2oUzY19w+8O\nVd0C4B4Avy8ivzHfDgGld374BXbmkq8AWINSj4ZeAI9X68Qi0gjgWQCfVdXLurtUc00CflR9TXQG\nRXMrZT6C/wSAFZf8bBb/nGtU9UT5/34A38X8VibqE5EOACj/3z8fTqhqX/nCKwL4Kqq0JiKSRing\nvqGq3ykPV31NQn7M15qUz33FRXMrZT6C/1UAa8s7lzUAPgbguWo7ISINItJ08TGA3wawz581pzyH\nUiFUYB4Lol4MtjL3oQprIqWihE8A2K+qX7zEVNU1sfyo9ppUrWhutXYwJ+1mfhClndRDAP7TPPmw\nGiWl4TUAr1fTDwDfROnPxxxKn90eQqnn4YsADgL4IYDWefLjrwDsBbAHpeDrqIIfd6D0J/0eALvL\n/z5Y7TVx/KjqmgB4J0pFcfeg9Ebzx5dcs68A6AHwNwAyMzkPv+FHSKTEvuFHSLQw+AmJFAY/IZHC\n4CckUhj8hEQKg5+QSGHwExIpDH5CIuX/AY/aiG8Ne8cWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncn4hEkXOL0SiyPmFSBQ5vxCJ8n8BU0WEAMEVxGUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKnsdKUUovds",
        "colab_type": "code",
        "outputId": "4c91824d-c4f3-4369-a00c-abc5c110a70e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vz.plot_cifar10_files(train_ds2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhcgUOb8QmSLnFyJT5PxCZMr/AUUwOJCXJqZWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC5Eo/wuOY7M520H3nwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOb8QiSLnFyJR5PxCJMr/B3pCSCUGj/CMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/EJkyv8HFjJ+e20dPfsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRc4vRKLI+YVIFDm/EIki5xciUf4v5xkqmkcpycYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qN5z9vjRsvL",
        "colab_type": "text"
      },
      "source": [
        "### Training - Build model , compile and train "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m26XDkJi1YyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D,  Activation, GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization , Dense, Lambda\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "from tensorflow.keras.layers import AveragePooling2D,Flatten\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "n = 3\n",
        "WEIGHT_DECAY=1.25e-4\n",
        "depth = n * 9 + 2\n",
        "\n",
        "\n",
        "\n",
        "def apply_weight(x):\n",
        "  return x*0.125  \n",
        "\n",
        "def random_pad_crop(image,padding=2):\n",
        "  shp=tf.shape(image)\n",
        "  \n",
        "  image=tf.pad(image,[(0, 0), (padding, padding), (padding, padding), (0, 0)], mode='reflect')\n",
        "  \n",
        "  image=tf.image.random_crop(image,size=shp)\n",
        "  return image  \n",
        "\n",
        "def flip_left_right(image):\n",
        "  return tf.image.random_flip_left_right(image)    \n",
        "\n",
        "def aug1(image):\n",
        "  print('is_training',is_training)\n",
        "  if is_training:\n",
        "    return ds.cutout(ds.random_pad_crop(image,padding=2),100,size=4)\n",
        "    #return flip_left_right(random_pad_crop(image))\n",
        "  else:\n",
        "    print('inside validation cycle\\n===============\\n')\n",
        "    return image  \n",
        "\n",
        "def aug2(image):\n",
        "  print('is_training',is_training)\n",
        "  if is_training:\n",
        "    return ds.cutout(ds.random_pad_crop(image,padding=1),100,size=2)\n",
        "    #return flip_left_right(random_pad_crop(image))\n",
        "  else:\n",
        "    print('inside validation cycle\\n===============\\n')\n",
        "    return image        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3eP7dQTsXCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpM77A6d7EaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "def random_pad_crop(image,padding=4):\n",
        "    padding = random.randint(1,5)\n",
        "    shape=tf.shape(image)  \n",
        "    image=tf.pad(image,[(padding, padding), (padding, padding), (0, 0)], mode='reflect')\n",
        "    image=tf.image.random_crop(image,size=shape)\n",
        "    return image  \n",
        "\n",
        "def cutout_channel(batch, prob=50, size=8, min_size=5, use_fixed_size=True):\n",
        "  #tf.print(img.shape)\n",
        "  #print(batch.shape)\n",
        "  #return tf.cond(tf.random.uniform([], 0, 100) > prob, true_fn = tf.map_fn(lambda channel: get_cutout_channel(channel,size,min_size,use_fixed_size),batch),false_fn = batch)\n",
        "  return tf.map_fn(lambda channel: get_cutout_channel(channel,size,min_size,use_fixed_size,prob),batch)\n",
        "\n",
        "def get_cutout_channel(img, size=8,min_size=2,use_fixed_size=True,prob = 50):\n",
        "    img2 = img\n",
        "    height = tf.shape(img)[0]\n",
        "    width = tf.shape(img)[1]\n",
        "    channel = tf.shape(img)[2]\n",
        "    area = tf.cast(width*height, tf.float32)\n",
        "    if (use_fixed_size==True):\n",
        "      s=size\n",
        "    else:  \n",
        "      s=tf.random.uniform([], min_size, size, tf.int32)\n",
        "    x1 = tf.random.uniform([], 0, height+1-s , tf.int32) # get the x offset from top left\n",
        "    y1 = tf.random.uniform([], 0, width+1-s , tf.int32)\n",
        "    img1 = tf.ones_like(img)  \n",
        "    #print(tf.shape(img1))\n",
        "    cut_slice = tf.slice(\n",
        "    img1,\n",
        "    [x1, y1, 0],\n",
        "    [s, s, channel])\n",
        "    #create mask similar in shape to input image with cutout area having ones and rest of the area padded with zeros \n",
        "    mask = tf.image.pad_to_bounding_box(\n",
        "      cut_slice,\n",
        "      x1,\n",
        "      y1,\n",
        "      height,\n",
        "      width\n",
        "    )\n",
        "    mask = tf.ones_like(mask) - mask\n",
        "    tmp_img = tf.multiply(img,mask)\n",
        "    cut_img =tmp_img\n",
        "    #print(cut_img.shape)\n",
        "    return tf.cond(tf.random.uniform([], 0, 100) > prob,lambda: cut_img,lambda: img2)\n",
        "    \n",
        "\n",
        "\n",
        "def random_crop_aug(batch,pad = 0):\n",
        "  if is_training:\n",
        "    #tf.print(batch.shape)\n",
        "    return tf.map_fn(lambda channel: random_pad_crop(channel,padding=pad),batch)\n",
        "  else:\n",
        "    return batch\n",
        "\n",
        "\n",
        "def rotate_aug(batch):\n",
        "  if is_training:\n",
        "    #print(\"in rotate\")\n",
        "    #tf.print(batch.shape)\n",
        "    degree = random.randint(-11,11)\n",
        "    return tf.map_fn(lambda channel: tfa.image.transform_ops.rotate(channel, math.radians(degree)), batch)\n",
        "  else:\n",
        "    #print('inside validation cycle\\n===============\\n')\n",
        "    return batch\n",
        "\n",
        "\n",
        "def flip_lr_aug(batch):\n",
        "    #tf.print(\"Aug\")\n",
        "    if is_training:\n",
        "      #tf.print(batch.shape)\n",
        "      return tf.map_fn(lambda channel: flip_left_right(channel), batch)\n",
        "    else:    \n",
        "      return batch  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh2ULUl3sf8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_v2(input_shape, depth, num_classes=10,distortion = 0):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "    if distortion == 1:\n",
        "      x = Lambda(random_crop_aug)(x)\n",
        "    if distortion == 2:\n",
        "      x = Lambda(rotate_aug)(x)\n",
        "    if distortion == 3:\n",
        "      x = Lambda(cutout_channel)(x)\n",
        "    if distortion == 4:\n",
        "      x = Lambda(flip_lr_aug)(x)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            if distortion == 5 and stage == 0:\n",
        "              y = Lambda(random_crop_aug)(y)\n",
        "            if distortion == 6 and stage == 0:\n",
        "              y = Lambda(rotate_aug)(y)\n",
        "            if distortion == 7 and stage == 0:\n",
        "              y = Lambda(cutout_channel)(y)\n",
        "            if distortion == 8 and stage == 0:\n",
        "              y = Lambda(flip_lr_aug)(y)\n",
        "            if distortion == 9 and stage == 1:\n",
        "              y = Lambda(random_crop_aug)(y)\n",
        "            if distortion == 10 and stage == 1:\n",
        "              y = Lambda(rotate_aug)(y)\n",
        "            if distortion == 11 and stage == 1:\n",
        "              y = Lambda(cutout_channel)(y)\n",
        "            if distortion == 12 and stage == 1:\n",
        "              y = Lambda(flip_lr_aug)(y)\n",
        "            if distortion == 13 and stage == 2:\n",
        "              y = Lambda(random_crop_aug)(y)\n",
        "            if distortion == 14 and stage == 2:\n",
        "              y = Lambda(rotate_aug)(y)\n",
        "            if distortion == 15 and stage == 2:\n",
        "              y = Lambda(cutout_channel)(y)\n",
        "            if distortion == 16 and stage == 2:\n",
        "              y = Lambda(flip_lr_aug)(y)\n",
        "              \n",
        "            \"\"\"\n",
        "            if distortion == 2 and stage == 0:\n",
        "              y = Lambda(aug2)(y)\n",
        "            if distortion == 5 and stage == 1:\n",
        "              y = Lambda(aug2)(y)\n",
        "            if distortion == 8 and stage == 2:\n",
        "              y = Lambda(aug2)(y)\n",
        "            \"\"\"\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            \n",
        "            if distortion == 17 and stage == 0:\n",
        "              y = Lambda(random_crop_aug)(y)\n",
        "            if distortion == 18 and stage == 0:\n",
        "              y = Lambda(rotate_aug)(y)\n",
        "            if distortion == 19 and stage == 0:\n",
        "              y = Lambda(cutout_channel)(y)\n",
        "            if distortion == 20 and stage == 0:\n",
        "              y = Lambda(flip_lr_aug)(y)\n",
        "            if distortion == 21 and stage == 1:\n",
        "              y = Lambda(random_crop_aug)(y)\n",
        "            if distortion == 22 and stage == 1:\n",
        "              y = Lambda(rotate_aug)(y)\n",
        "            if distortion == 23 and stage == 1:\n",
        "              y = Lambda(cutout_channel)(y)\n",
        "            if distortion == 24 and stage == 1:\n",
        "              y = Lambda(flip_lr_aug)(y)\n",
        "            if distortion == 25 and stage == 2:\n",
        "              y = Lambda(random_crop_aug)(y)\n",
        "            if distortion == 26 and stage == 2:\n",
        "              y = Lambda(rotate_aug)(y)\n",
        "            if distortion == 27 and stage == 2:\n",
        "              y = Lambda(cutout_channel)(y)\n",
        "            if distortion == 28 and stage == 2:\n",
        "              y = Lambda(flip_lr_aug)(y)\n",
        "            \"\"\"\n",
        "            if distortion == 3 and stage == 0:\n",
        "              y = Lambda(aug2)(y)\n",
        "            if distortion == 6 and stage == 1:\n",
        "              y = Lambda(aug2)(y)\n",
        "            if distortion == 9 and stage == 2:\n",
        "              y = Lambda(aug2)(y)\n",
        "            \"\"\"\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if distortion == 29 and stage == 0:\n",
        "              y = Lambda(random_crop_aug)(y)\n",
        "            if distortion == 30 and stage == 0:\n",
        "              y = Lambda(rotate_aug)(y)\n",
        "            if distortion == 31 and stage == 0:\n",
        "              y = Lambda(cutout_channel)(y)\n",
        "            if distortion == 32 and stage == 0:\n",
        "              y = Lambda(flip_lr_aug)(y)\n",
        "            if distortion == 33 and stage == 1:\n",
        "              y = Lambda(random_crop_aug)(y)\n",
        "            if distortion == 34 and stage == 1:\n",
        "              y = Lambda(rotate_aug)(y)\n",
        "            if distortion == 35 and stage == 1:\n",
        "              y = Lambda(cutout_channel)(y)\n",
        "            if distortion == 36 and stage == 1:\n",
        "              y = Lambda(flip_lr_aug)(y)\n",
        "            if distortion == 37 and stage == 2:\n",
        "              y = Lambda(random_crop_aug)(y)\n",
        "            if distortion == 38 and stage == 2:\n",
        "              y = Lambda(rotate_aug)(y)\n",
        "            if distortion == 39 and stage == 2:\n",
        "              y = Lambda(cutout_channel)(y)\n",
        "            if distortion == 40 and stage == 2:\n",
        "              y = Lambda(flip_lr_aug)(y)\n",
        "              \n",
        "            \"\"\"\n",
        "            if distortion == 4 and stage == 0:\n",
        "              y = Lambda(aug2)(y)\n",
        "            if distortion == 7 and stage == 1:\n",
        "              y = Lambda(aug2)(y)\n",
        "            if distortion == 10 and stage == 2:\n",
        "              y = Lambda(aug2)(y)\n",
        "            \"\"\"\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = tf.keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvLCygOJsiQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = resnet_v2(input_shape=(32,32,3), depth=depth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5Rg1-wasi7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nng9bFm1rQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def lr_schedule():\n",
        "    \n",
        "    def schedule(epoch):\n",
        "\n",
        "      lr=lr1=np.interp([epoch],[0, 5,24], [0.025, 0.4, 0])[0]\n",
        "      print('epoch ', epoch+1, ': setting learning rate to ',lr1)\n",
        "      return lr\n",
        "    \n",
        "    return LearningRateScheduler(schedule)\n",
        "\n",
        "lr_sched = lr_schedule()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaDK2E2h81vV",
        "colab_type": "code",
        "outputId": "12ec1099-1d04-4375-c95b-58c173949e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "for model_params in [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]:\n",
        "  is_training=True\n",
        "  print(\"Model Param:- \",model_params)\n",
        "  model = resnet_v2(input_shape=(32,32,3), depth=depth, distortion = model_params)\n",
        "  model.summary()\n",
        "  #global_step = tf.train.get_or_create_global_step()\n",
        "  #model=model=build_model(model_params)\n",
        "  opt=SGD(lr=0.025,momentum=0.9,nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,metrics=['accuracy']\n",
        "              )\n",
        "  \n",
        "  \n",
        "  #if model_params in [0,4,5]:  \n",
        "  train_ds=train_ds2  \n",
        "  model.fit(train_ds,epochs=EPOCHS, steps_per_epoch=np.ceil(50000/batch_size), \n",
        "          callbacks=[lr_sched],\n",
        "          verbose=1)\n",
        "  is_training=False\n",
        "  score=model.evaluate(test_ds, steps =np.ceil(10000/batch_size), verbose=1)\n",
        "\n",
        "  del(model)\n",
        "  del(train_ds)\n",
        "  \n",
        "  print('val accuracy score at the end of training model type ',model_params, score)\n",
        "  print(\"=========================================\\n\")\n",
        "\n",
        "#validation_data=test_ds, validation_steps=np.ceil(10000/batch_size),"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Param:-  0\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 32, 32, 16)   448         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 32, 32, 16)   64          conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 32, 32, 16)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 32, 32, 16)   272         activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 32, 32, 16)   64          conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 32, 32, 16)   0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 32, 32, 16)   2320        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 32, 32, 16)   64          conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 32, 32, 16)   0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 32, 32, 64)   1088        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 32, 32, 64)   1088        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 32, 32, 64)   0           conv2d_36[0][0]                  \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 32, 32, 64)   256         add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 32, 32, 64)   0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 32, 32, 16)   1040        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 32, 32, 16)   64          conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 32, 32, 16)   0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 32, 32, 16)   2320        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 32, 32, 16)   64          conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 32, 32, 16)   0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 32, 32, 64)   1088        activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 32, 32, 64)   0           add_9[0][0]                      \n",
            "                                                                 conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 32, 32, 64)   256         add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 32, 32, 64)   0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 32, 32, 16)   1040        activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 32, 32, 16)   64          conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 32, 32, 16)   0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 32, 32, 16)   2320        activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 32, 32, 16)   64          conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 32, 32, 16)   0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 32, 32, 64)   1088        activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 32, 32, 64)   0           add_10[0][0]                     \n",
            "                                                                 conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 32, 32, 64)   256         add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 32, 32, 64)   0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 16, 16, 64)   4160        activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 16, 16, 64)   256         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 16, 16, 64)   0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 16, 16, 64)   36928       activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 16, 16, 64)   256         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 16, 16, 64)   0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 16, 16, 128)  8320        add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 16, 16, 128)  8320        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 16, 16, 128)  0           conv2d_46[0][0]                  \n",
            "                                                                 conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 16, 16, 128)  512         add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 16, 16, 128)  0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 16, 16, 64)   8256        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 16, 16, 64)   256         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 16, 16, 64)   0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 16, 16, 64)   36928       activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 16, 16, 64)   256         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 16, 16, 64)   0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 16, 16, 128)  8320        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 16, 16, 128)  0           add_12[0][0]                     \n",
            "                                                                 conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 16, 16, 128)  512         add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 16, 16, 128)  0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 16, 16, 64)   8256        activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 16, 16, 64)   256         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 16, 16, 64)   0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 16, 16, 64)   36928       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 16, 16, 64)   256         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 16, 16, 64)   0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 16, 16, 128)  8320        activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 16, 16, 128)  0           add_13[0][0]                     \n",
            "                                                                 conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 16, 16, 128)  512         add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 16, 16, 128)  0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 8, 8, 128)    16512       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 8, 8, 128)    512         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 8, 8, 128)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 8, 8, 128)    147584      activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 8, 8, 128)    512         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 8, 8, 128)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 8, 8, 256)    33024       add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 8, 8, 256)    33024       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 8, 8, 256)    0           conv2d_56[0][0]                  \n",
            "                                                                 conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 8, 8, 256)    1024        add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 8, 8, 256)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 8, 8, 128)    32896       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 8, 8, 128)    512         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 8, 8, 128)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 8, 8, 128)    147584      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 8, 8, 128)    512         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 8, 8, 128)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 8, 8, 256)    33024       activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 8, 8, 256)    0           add_15[0][0]                     \n",
            "                                                                 conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 8, 8, 256)    1024        add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 8, 8, 256)    0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 8, 8, 128)    32896       activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 8, 8, 128)    512         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 8, 8, 128)    0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 8, 8, 128)    147584      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 8, 8, 128)    512         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 8, 8, 128)    0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 8, 8, 256)    33024       activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 8, 8, 256)    0           add_16[0][0]                     \n",
            "                                                                 conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 8, 8, 256)    1024        add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 8, 8, 256)    0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 256)    0           activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 256)          0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           2570        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 27s 279ms/step - loss: 2.4932 - accuracy: 0.3163\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 24s 247ms/step - loss: 2.0652 - accuracy: 0.4674\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 24s 244ms/step - loss: 1.7518 - accuracy: 0.5738\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 23s 240ms/step - loss: 1.4899 - accuracy: 0.6587\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 1.3230 - accuracy: 0.7060\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 1.1577 - accuracy: 0.7522\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 0.9940 - accuracy: 0.8003\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 24s 242ms/step - loss: 0.8811 - accuracy: 0.8326\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 0.7969 - accuracy: 0.8523\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 0.7227 - accuracy: 0.8738\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 0.6668 - accuracy: 0.8869\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 0.6169 - accuracy: 0.9024\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 0.5653 - accuracy: 0.9175\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 0.5210 - accuracy: 0.9305\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 0.4870 - accuracy: 0.9416\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 0.4484 - accuracy: 0.9536\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 0.4077 - accuracy: 0.9659\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 24s 242ms/step - loss: 0.3875 - accuracy: 0.9704\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 0.3423 - accuracy: 0.9840\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 0.3080 - accuracy: 0.9935\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 0.2873 - accuracy: 0.9975\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 0.2721 - accuracy: 0.9998\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 0.2645 - accuracy: 1.0000\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 24s 243ms/step - loss: 0.2607 - accuracy: 1.0000\n",
            "20/20 [==============================] - 2s 120ms/step - loss: 0.9777 - accuracy: 0.8479\n",
            "val accuracy score at the end of training model type  0 [0.9777205377817154, 0.8478516]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  1\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 32, 32, 16)   448         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 32, 32, 16)   64          conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 32, 32, 16)   0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 32, 32, 16)   0           activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 32, 32, 16)   272         lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 32, 32, 16)   64          conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 32, 32, 16)   0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 32, 32, 16)   2320        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 32, 32, 16)   64          conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 32, 32, 16)   0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 32, 32, 64)   1088        lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 32, 32, 64)   1088        activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 32, 32, 64)   0           conv2d_67[0][0]                  \n",
            "                                                                 conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 32, 32, 64)   256         add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 32, 32, 64)   0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 32, 32, 16)   1040        activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 32, 32, 16)   64          conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 32, 32, 16)   0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 32, 32, 16)   2320        activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 32, 32, 16)   64          conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 32, 32, 16)   0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 32, 32, 64)   1088        activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 32, 32, 64)   0           add_18[0][0]                     \n",
            "                                                                 conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 32, 32, 64)   256         add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 32, 32, 64)   0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 32, 32, 16)   1040        activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 32, 32, 16)   64          conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 32, 32, 16)   0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 32, 32, 16)   2320        activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 32, 32, 16)   64          conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 32, 32, 16)   0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 32, 32, 64)   1088        activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 32, 32, 64)   0           add_19[0][0]                     \n",
            "                                                                 conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 32, 32, 64)   256         add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 32, 32, 64)   0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 16, 16, 64)   4160        activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 16, 16, 64)   256         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 16, 16, 64)   0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 16, 16, 64)   36928       activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 16, 16, 64)   256         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 16, 16, 64)   0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 16, 16, 128)  8320        add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 16, 16, 128)  8320        activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 16, 16, 128)  0           conv2d_77[0][0]                  \n",
            "                                                                 conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 16, 16, 128)  512         add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 16, 16, 128)  0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 16, 16, 64)   8256        activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 16, 16, 64)   256         conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 16, 16, 64)   0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 16, 16, 64)   36928       activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 16, 16, 64)   256         conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 16, 16, 64)   0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 16, 16, 128)  8320        activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 16, 16, 128)  0           add_21[0][0]                     \n",
            "                                                                 conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 16, 16, 128)  512         add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 16, 16, 128)  0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 16, 16, 64)   8256        activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 16, 16, 64)   256         conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 16, 16, 64)   0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 16, 16, 64)   36928       activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 16, 16, 64)   256         conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 16, 16, 64)   0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 16, 16, 128)  8320        activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 16, 16, 128)  0           add_22[0][0]                     \n",
            "                                                                 conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 16, 16, 128)  512         add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 16, 16, 128)  0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 8, 8, 128)    16512       activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 8, 8, 128)    512         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 8, 8, 128)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 8, 8, 128)    147584      activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 8, 8, 128)    512         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 8, 8, 128)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 8, 8, 256)    33024       add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 8, 8, 256)    33024       activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 8, 8, 256)    0           conv2d_87[0][0]                  \n",
            "                                                                 conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 8, 8, 256)    1024        add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 8, 8, 256)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 8, 8, 128)    32896       activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 8, 8, 128)    512         conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 8, 8, 128)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 8, 8, 128)    147584      activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 8, 8, 128)    512         conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 8, 8, 128)    0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 8, 8, 256)    33024       activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 8, 8, 256)    0           add_24[0][0]                     \n",
            "                                                                 conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 8, 8, 256)    1024        add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 8, 8, 256)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 8, 8, 128)    32896       activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 8, 8, 128)    512         conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 8, 8, 128)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 8, 8, 128)    147584      activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 8, 8, 128)    512         conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 8, 8, 128)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 8, 8, 256)    33024       activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 8, 8, 256)    0           add_25[0][0]                     \n",
            "                                                                 conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 8, 8, 256)    1024        add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 8, 8, 256)    0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 1, 1, 256)    0           activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 256)          0           average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           2570        flatten_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 63s 639ms/step - loss: 2.5124 - accuracy: 0.2993\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 59s 597ms/step - loss: 2.1077 - accuracy: 0.4421\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 58s 591ms/step - loss: 1.8075 - accuracy: 0.5519\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 58s 591ms/step - loss: 1.5484 - accuracy: 0.6380\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 57s 587ms/step - loss: 1.3477 - accuracy: 0.6955\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 57s 584ms/step - loss: 1.1811 - accuracy: 0.7444\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 58s 588ms/step - loss: 1.0193 - accuracy: 0.7893\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 58s 593ms/step - loss: 0.9088 - accuracy: 0.8196\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 58s 589ms/step - loss: 0.8304 - accuracy: 0.8366\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 58s 588ms/step - loss: 0.7673 - accuracy: 0.8508\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 57s 584ms/step - loss: 0.7150 - accuracy: 0.8620\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 57s 584ms/step - loss: 0.6625 - accuracy: 0.8758\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 58s 591ms/step - loss: 0.6250 - accuracy: 0.8840\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 57s 584ms/step - loss: 0.5866 - accuracy: 0.8930\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 58s 588ms/step - loss: 0.5458 - accuracy: 0.9044\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 57s 583ms/step - loss: 0.5105 - accuracy: 0.9135\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 57s 586ms/step - loss: 0.4800 - accuracy: 0.9205\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 58s 591ms/step - loss: 0.4453 - accuracy: 0.9316\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 57s 583ms/step - loss: 0.4114 - accuracy: 0.9411\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 57s 585ms/step - loss: 0.3815 - accuracy: 0.9512\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 58s 589ms/step - loss: 0.3467 - accuracy: 0.9614\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 58s 589ms/step - loss: 0.3157 - accuracy: 0.9702\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 57s 586ms/step - loss: 0.2846 - accuracy: 0.9816\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 58s 591ms/step - loss: 0.2646 - accuracy: 0.9882\n",
            "20/20 [==============================] - 2s 104ms/step - loss: 0.6274 - accuracy: 0.8877\n",
            "val accuracy score at the end of training model type  1 [0.627430671453476, 0.8876953]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-b8991836617b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model Param:- \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistortion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m#global_step = tf.train.get_or_create_global_step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-4637a9a499d9>\u001b[0m in \u001b[0;36mresnet_v2\u001b[0;34m(input_shape, depth, num_classes, distortion)\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_crop_aug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdistortion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotate_aug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdistortion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcutout_channel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    841\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training)\u001b[0m\n\u001b[1;32m    793\u001b[0m       \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-2ea8efb0a640>\u001b[0m in \u001b[0;36mrotate_aug\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m#tf.print(batch.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mdegree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mradians\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m#print('inside validation cycle\\n===============\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/map_fn.py\u001b[0m in \u001b[0;36mmap_fn\u001b[0;34m(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mback_prop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         maximum_iterations=n)\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0mresults_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2673\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0mreturn_same_structure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_same_structure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m         back_prop=back_prop)\n\u001b[0m\u001b[1;32m   2676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"while\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/while_v2.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, maximum_iterations, name, return_same_structure, back_prop)\u001b[0m\n\u001b[1;32m    196\u001b[0m         func_graph=util.WhileBodyFuncGraph(\n\u001b[1;32m    197\u001b[0m             body_name, collections=ops.get_default_graph()._collections),  # pylint: disable=protected-access\n\u001b[0;32m--> 198\u001b[0;31m         add_control_dependencies=add_control_dependencies)\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0;31m# Add external captures of body to the list of loop vars.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;31m# Note that external tensors will be treated as loop invariants, i.e.,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    913\u001b[0m                                           converted_func)\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/while_v2.py\u001b[0m in \u001b[0;36mwrapped_body\u001b[0;34m(loop_counter, maximum_iterations_arg, *args)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0;31m# `orig_loop_vars` and `args`, converts flows in `args` to TensorArrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;31m# and packs it into the structure of `orig_loop_vars`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_pack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence_or_composite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/map_fn.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(i, tas)\u001b[0m\n\u001b[1;32m    255\u001b[0m       \"\"\"\n\u001b[1;32m    256\u001b[0m       \u001b[0mpacked_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_ta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem_ta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melems_ta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m       \u001b[0mpacked_fn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m       \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacked_fn_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m       \u001b[0mflat_fn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_fn_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-2ea8efb0a640>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(channel)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m#tf.print(batch.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mdegree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mradians\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m#print('inside validation cycle\\n===============\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tfa' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdpsGxqZUUdZ",
        "colab_type": "code",
        "outputId": "daac0aaa-1e53-4fee-bae2-d0c33d9f24bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for model_params in [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]:\n",
        "  is_training=True\n",
        "  print(\"Model Param:- \",model_params)\n",
        "  model = resnet_v2(input_shape=(32,32,3), depth=depth, distortion = model_params)\n",
        "  model.summary()\n",
        "  #global_step = tf.train.get_or_create_global_step()\n",
        "  #model=model=build_model(model_params)\n",
        "  opt=SGD(lr=0.025,momentum=0.9,nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,metrics=['accuracy']\n",
        "              )\n",
        "  \n",
        "  \n",
        "  #if model_params in [0,4,5]:  \n",
        "  train_ds=train_ds2  \n",
        "  model.fit(train_ds,epochs=EPOCHS, steps_per_epoch=np.ceil(50000/batch_size), \n",
        "          callbacks=[lr_sched],\n",
        "          verbose=1)\n",
        "  is_training=False\n",
        "  score=model.evaluate(test_ds, steps =np.ceil(10000/batch_size), verbose=1)\n",
        "\n",
        "  del(model)\n",
        "  del(train_ds)\n",
        "  \n",
        "  print('val accuracy score at the end of training model type ',model_params, score)\n",
        "  print(\"=========================================\\n\")\n",
        "\n",
        "#validation_data=test_ds, validation_steps=np.ceil(10000/batch_size),"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Param:-  2\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 32, 32, 16)   0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   272         lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 64)   1088        lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 64)   1088        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 64)   0           conv2d_4[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 64)   256         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   1040        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 64)   1088        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 64)   0           add[0][0]                        \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 16)   1040        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 16)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2320        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 16)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 64)   1088        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 64)   0           add_1[0][0]                      \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 64)   4160        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 64)   36928       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 128)  8320        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 128)  8320        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 128)  0           conv2d_14[0][0]                  \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 128)  512         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 64)   8256        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 64)   256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 64)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 64)   36928       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 128)  8320        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 128)  0           add_3[0][0]                      \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 128)  512         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 128)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 64)   8256        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 64)   256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 64)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 64)   36928       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 64)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 128)  8320        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 128)  0           add_4[0][0]                      \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 128)  512         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 128)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 128)    16512       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 128)    512         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 128)    0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 128)    147584      activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 8, 128)    512         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 8, 8, 128)    0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 8, 8, 256)    33024       add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 8, 8, 256)    33024       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 8, 8, 256)    0           conv2d_24[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 8, 256)    1024        add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 8, 8, 256)    0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 8, 8, 128)    32896       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 8, 8, 128)    512         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 8, 8, 128)    0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 8, 8, 128)    147584      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 8, 8, 128)    512         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 8, 8, 128)    0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 8, 8, 256)    33024       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 256)    0           add_6[0][0]                      \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 8, 8, 256)    1024        add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 8, 8, 256)    0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 128)    32896       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 8, 8, 128)    512         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 8, 8, 128)    0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 128)    147584      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 128)    512         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 8, 128)    0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 256)    33024       activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 256)    0           add_7[0][0]                      \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 256)    1024        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 256)    0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 1, 1, 256)    0           activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 256)          0           average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           2570        flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 78s 800ms/step - loss: 2.4838 - accuracy: 0.3165\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 69s 708ms/step - loss: 2.0839 - accuracy: 0.4549\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 69s 706ms/step - loss: 1.7933 - accuracy: 0.5578\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 70s 710ms/step - loss: 1.5366 - accuracy: 0.6399\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 70s 710ms/step - loss: 1.3617 - accuracy: 0.6937\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 70s 710ms/step - loss: 1.2043 - accuracy: 0.7359\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 70s 715ms/step - loss: 1.0324 - accuracy: 0.7858\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 70s 717ms/step - loss: 0.9127 - accuracy: 0.8203\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 70s 709ms/step - loss: 0.8230 - accuracy: 0.8430\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 70s 713ms/step - loss: 0.7558 - accuracy: 0.8626\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 71s 721ms/step - loss: 0.6910 - accuracy: 0.8812\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 70s 712ms/step - loss: 0.6394 - accuracy: 0.8958\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 70s 715ms/step - loss: 0.5871 - accuracy: 0.9121\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 70s 716ms/step - loss: 0.5468 - accuracy: 0.9245\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 71s 721ms/step - loss: 0.5094 - accuracy: 0.9356\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 71s 720ms/step - loss: 0.4597 - accuracy: 0.9524\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 70s 719ms/step - loss: 0.4245 - accuracy: 0.9623\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 70s 718ms/step - loss: 0.3884 - accuracy: 0.9738\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 70s 714ms/step - loss: 0.3508 - accuracy: 0.9832\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 70s 719ms/step - loss: 0.3210 - accuracy: 0.9919\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 70s 717ms/step - loss: 0.2955 - accuracy: 0.9967\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 70s 715ms/step - loss: 0.2792 - accuracy: 0.9997\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 70s 714ms/step - loss: 0.2710 - accuracy: 1.0000\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 70s 717ms/step - loss: 0.2672 - accuracy: 1.0000\n",
            "20/20 [==============================] - 2s 110ms/step - loss: 1.1309 - accuracy: 0.8192\n",
            "val accuracy score at the end of training model type  2 [1.1308519959449768, 0.8192383]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  3\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 32, 32, 16)   448         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 32, 32, 16)   64          conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 32, 32, 16)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 32, 32, 16)   0           activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 32, 32, 16)   272         lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 32, 32, 16)   64          conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 32, 32, 16)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 32, 32, 16)   2320        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 32, 32, 16)   64          conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 32, 32, 16)   0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 32, 32, 64)   1088        lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 32, 32, 64)   1088        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 32, 32, 64)   0           conv2d_35[0][0]                  \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 32, 32, 64)   256         add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 32, 32, 64)   0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 32, 32, 16)   1040        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 32, 32, 16)   64          conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 32, 32, 16)   0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 32, 32, 16)   2320        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 32, 32, 16)   64          conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 32, 32, 16)   0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 32, 32, 64)   1088        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 32, 32, 64)   0           add_9[0][0]                      \n",
            "                                                                 conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 32, 32, 64)   256         add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 32, 32, 64)   0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 32, 32, 16)   1040        activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 32, 32, 16)   64          conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 32, 32, 16)   0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 32, 32, 16)   2320        activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 32, 32, 16)   64          conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 32, 32, 16)   0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 32, 32, 64)   1088        activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 32, 32, 64)   0           add_10[0][0]                     \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 32, 32, 64)   256         add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 32, 32, 64)   0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 16, 16, 64)   4160        activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 16, 16, 64)   256         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 16, 16, 64)   0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 16, 16, 64)   36928       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 16, 16, 64)   256         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 16, 16, 64)   0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 16, 16, 128)  8320        add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 16, 16, 128)  8320        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 16, 16, 128)  0           conv2d_45[0][0]                  \n",
            "                                                                 conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 16, 16, 128)  512         add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 16, 16, 128)  0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 16, 16, 64)   8256        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 16, 16, 64)   256         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 16, 16, 64)   0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 16, 16, 64)   36928       activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 16, 16, 64)   256         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 16, 16, 64)   0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 16, 16, 128)  8320        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 16, 16, 128)  0           add_12[0][0]                     \n",
            "                                                                 conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 16, 16, 128)  512         add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 16, 16, 128)  0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 16, 16, 64)   8256        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 16, 16, 64)   256         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 16, 16, 64)   0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 16, 16, 64)   36928       activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 16, 16, 64)   256         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 16, 16, 64)   0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 16, 16, 128)  8320        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 16, 16, 128)  0           add_13[0][0]                     \n",
            "                                                                 conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 16, 16, 128)  512         add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 16, 16, 128)  0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 8, 8, 128)    16512       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 8, 8, 128)    512         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 8, 8, 128)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 8, 8, 128)    147584      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 8, 8, 128)    512         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 8, 8, 128)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 8, 8, 256)    33024       add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 8, 8, 256)    33024       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 8, 8, 256)    0           conv2d_55[0][0]                  \n",
            "                                                                 conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 8, 8, 256)    1024        add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 8, 8, 256)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 8, 8, 128)    32896       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 8, 8, 128)    512         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 8, 8, 128)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 8, 8, 128)    147584      activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 8, 8, 128)    512         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 8, 8, 128)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 8, 8, 256)    33024       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 8, 8, 256)    0           add_15[0][0]                     \n",
            "                                                                 conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 8, 8, 256)    1024        add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 8, 8, 256)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 8, 8, 128)    32896       activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 8, 8, 128)    512         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 8, 8, 128)    0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 8, 8, 128)    147584      activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 8, 8, 128)    512         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 8, 8, 128)    0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 8, 8, 256)    33024       activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 8, 8, 256)    0           add_16[0][0]                     \n",
            "                                                                 conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 8, 8, 256)    1024        add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 8, 8, 256)    0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 256)    0           activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 256)          0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           2570        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 84s 858ms/step - loss: 2.4814 - accuracy: 0.3212\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 78s 795ms/step - loss: 2.0471 - accuracy: 0.4707\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 78s 799ms/step - loss: 1.7755 - accuracy: 0.5654\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 78s 800ms/step - loss: 1.5195 - accuracy: 0.6490\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 78s 793ms/step - loss: 1.3284 - accuracy: 0.7037\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 79s 802ms/step - loss: 1.1855 - accuracy: 0.7420\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 78s 799ms/step - loss: 1.0274 - accuracy: 0.7893\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 79s 809ms/step - loss: 0.9133 - accuracy: 0.8186\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 79s 802ms/step - loss: 0.8254 - accuracy: 0.8410\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 79s 809ms/step - loss: 0.7571 - accuracy: 0.8614\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 79s 805ms/step - loss: 0.6976 - accuracy: 0.8762\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 79s 807ms/step - loss: 0.6521 - accuracy: 0.8881\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 78s 797ms/step - loss: 0.6045 - accuracy: 0.9023\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 78s 800ms/step - loss: 0.5589 - accuracy: 0.9141\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 78s 801ms/step - loss: 0.5233 - accuracy: 0.9268\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 78s 800ms/step - loss: 0.4931 - accuracy: 0.9336\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 78s 796ms/step - loss: 0.4490 - accuracy: 0.9480\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 78s 799ms/step - loss: 0.4163 - accuracy: 0.9578\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 78s 796ms/step - loss: 0.3824 - accuracy: 0.9682\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 79s 803ms/step - loss: 0.3523 - accuracy: 0.9767\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 78s 798ms/step - loss: 0.3216 - accuracy: 0.9851\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 78s 797ms/step - loss: 0.2955 - accuracy: 0.9922\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 77s 783ms/step - loss: 0.2809 - accuracy: 0.9955\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 78s 791ms/step - loss: 0.2735 - accuracy: 0.9967\n",
            "20/20 [==============================] - 10s 492ms/step - loss: 0.9078 - accuracy: 0.8501\n",
            "val accuracy score at the end of training model type  3 [0.9078414559364318, 0.85009766]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  4\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 32, 32, 16)   448         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 32, 32, 16)   64          conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 32, 32, 16)   0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 32, 32, 16)   0           activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 32, 32, 16)   272         lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 32, 32, 16)   64          conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 32, 32, 16)   0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 32, 32, 16)   2320        activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 32, 32, 16)   64          conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 32, 32, 16)   0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 32, 32, 64)   1088        lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 32, 32, 64)   1088        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 32, 32, 64)   0           conv2d_66[0][0]                  \n",
            "                                                                 conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 32, 32, 64)   256         add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 32, 32, 64)   0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 32, 32, 16)   1040        activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 32, 32, 16)   64          conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 32, 32, 16)   0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 32, 32, 16)   2320        activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 32, 32, 16)   64          conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 32, 32, 16)   0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 32, 32, 64)   1088        activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 32, 32, 64)   0           add_18[0][0]                     \n",
            "                                                                 conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 32, 32, 64)   256         add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 32, 32, 64)   0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 32, 32, 16)   1040        activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 32, 32, 16)   64          conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 32, 32, 16)   0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 32, 32, 16)   2320        activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 32, 32, 16)   64          conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 32, 32, 16)   0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 32, 32, 64)   1088        activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 32, 32, 64)   0           add_19[0][0]                     \n",
            "                                                                 conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 32, 32, 64)   256         add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 32, 32, 64)   0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 16, 16, 64)   4160        activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 16, 16, 64)   256         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 16, 16, 64)   0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 16, 16, 64)   36928       activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 16, 16, 64)   256         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 16, 16, 64)   0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 16, 16, 128)  8320        add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 16, 16, 128)  8320        activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 16, 16, 128)  0           conv2d_76[0][0]                  \n",
            "                                                                 conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 16, 16, 128)  512         add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 16, 16, 128)  0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 16, 16, 64)   8256        activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 16, 16, 64)   256         conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 16, 16, 64)   0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 16, 16, 64)   36928       activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 16, 16, 64)   256         conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 16, 16, 64)   0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 16, 16, 128)  8320        activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 16, 16, 128)  0           add_21[0][0]                     \n",
            "                                                                 conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 16, 16, 128)  512         add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 16, 16, 128)  0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 16, 16, 64)   8256        activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 16, 16, 64)   256         conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 16, 16, 64)   0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 16, 16, 64)   36928       activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 16, 16, 64)   256         conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 16, 16, 64)   0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 16, 16, 128)  8320        activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 16, 16, 128)  0           add_22[0][0]                     \n",
            "                                                                 conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 16, 16, 128)  512         add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 16, 16, 128)  0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 8, 8, 128)    16512       activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 8, 8, 128)    512         conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 8, 8, 128)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 8, 8, 128)    147584      activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 8, 8, 128)    512         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 8, 8, 128)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 8, 8, 256)    33024       add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 8, 8, 256)    33024       activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 8, 8, 256)    0           conv2d_86[0][0]                  \n",
            "                                                                 conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 8, 8, 256)    1024        add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 8, 8, 256)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 8, 8, 128)    32896       activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 8, 8, 128)    512         conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 8, 8, 128)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 8, 8, 128)    147584      activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 8, 8, 128)    512         conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 8, 8, 128)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 8, 8, 256)    33024       activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 8, 8, 256)    0           add_24[0][0]                     \n",
            "                                                                 conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 8, 8, 256)    1024        add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 8, 8, 256)    0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 8, 8, 128)    32896       activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 8, 8, 128)    512         conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 8, 8, 128)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 8, 8, 128)    147584      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 8, 8, 128)    512         conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 8, 8, 128)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 8, 8, 256)    33024       activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 8, 8, 256)    0           add_25[0][0]                     \n",
            "                                                                 conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 8, 8, 256)    1024        add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 8, 8, 256)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 1, 1, 256)    0           activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 256)          0           average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           2570        flatten_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 63s 642ms/step - loss: 2.4759 - accuracy: 0.3190\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 56s 575ms/step - loss: 2.0721 - accuracy: 0.4590\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 56s 573ms/step - loss: 1.7903 - accuracy: 0.5593\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 56s 570ms/step - loss: 1.5671 - accuracy: 0.6295\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 55s 565ms/step - loss: 1.3711 - accuracy: 0.6867\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 55s 558ms/step - loss: 1.2262 - accuracy: 0.7254\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 54s 552ms/step - loss: 1.0766 - accuracy: 0.7645\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 53s 545ms/step - loss: 0.9604 - accuracy: 0.7988\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 54s 550ms/step - loss: 0.8675 - accuracy: 0.8221\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 53s 546ms/step - loss: 0.7939 - accuracy: 0.8410\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 53s 546ms/step - loss: 0.7343 - accuracy: 0.8579\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 53s 543ms/step - loss: 0.6823 - accuracy: 0.8698\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 54s 552ms/step - loss: 0.6377 - accuracy: 0.8817\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 55s 557ms/step - loss: 0.5957 - accuracy: 0.8940\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 55s 562ms/step - loss: 0.5564 - accuracy: 0.9048\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 55s 560ms/step - loss: 0.5164 - accuracy: 0.9169\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 55s 563ms/step - loss: 0.4764 - accuracy: 0.9287\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 55s 560ms/step - loss: 0.4430 - accuracy: 0.9377\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 55s 560ms/step - loss: 0.4077 - accuracy: 0.9485\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 55s 562ms/step - loss: 0.3709 - accuracy: 0.9604\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 56s 568ms/step - loss: 0.3317 - accuracy: 0.9726\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 56s 576ms/step - loss: 0.2977 - accuracy: 0.9837\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 56s 574ms/step - loss: 0.2750 - accuracy: 0.9900\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 56s 575ms/step - loss: 0.2558 - accuracy: 0.9961\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.7448 - accuracy: 0.8694\n",
            "val accuracy score at the end of training model type  4 [0.7448440998792648, 0.8694336]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  5\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 32, 32, 16)   448         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 32, 32, 16)   64          conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 32, 32, 16)   0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 32, 32, 16)   272         activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 32, 32, 16)   0           conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 32, 32, 16)   64          lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 32, 32, 16)   0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 32, 32, 16)   2320        activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 32, 32, 16)   64          conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 32, 32, 16)   0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 32, 32, 64)   1088        activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 32, 32, 64)   1088        activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 32, 32, 64)   0           conv2d_97[0][0]                  \n",
            "                                                                 conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 32, 32, 64)   256         add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 32, 32, 64)   0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 32, 32, 16)   1040        activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 32, 32, 16)   0           conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 32, 32, 16)   64          lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 32, 32, 16)   0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 32, 32, 16)   2320        activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 32, 32, 16)   64          conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 32, 32, 16)   0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 32, 32, 64)   1088        activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_28 (Add)                    (None, 32, 32, 64)   0           add_27[0][0]                     \n",
            "                                                                 conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 32, 32, 64)   256         add_28[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 32, 32, 64)   0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 32, 32, 16)   1040        activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 32, 32, 16)   0           conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 32, 32, 16)   64          lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 32, 32, 16)   0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 32, 32, 16)   2320        activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 32, 32, 16)   64          conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 32, 32, 16)   0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 32, 32, 64)   1088        activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_29 (Add)                    (None, 32, 32, 64)   0           add_28[0][0]                     \n",
            "                                                                 conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 32, 32, 64)   256         add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 32, 32, 64)   0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 16, 16, 64)   4160        activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 16, 16, 64)   256         conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 16, 16, 64)   0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 16, 16, 64)   36928       activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 16, 16, 64)   256         conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 16, 16, 64)   0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 16, 16, 128)  8320        add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 16, 16, 128)  8320        activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_30 (Add)                    (None, 16, 16, 128)  0           conv2d_107[0][0]                 \n",
            "                                                                 conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 16, 16, 128)  512         add_30[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 16, 16, 128)  0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 16, 16, 64)   8256        activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 16, 16, 64)   256         conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 16, 16, 64)   0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 16, 16, 64)   36928       activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 16, 16, 64)   256         conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 16, 16, 64)   0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 16, 16, 128)  8320        activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_31 (Add)                    (None, 16, 16, 128)  0           add_30[0][0]                     \n",
            "                                                                 conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 16, 16, 128)  512         add_31[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 16, 16, 128)  0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 16, 16, 64)   8256        activation_99[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 16, 16, 64)   256         conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 16, 16, 64)   0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 16, 16, 64)   36928       activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 16, 16, 64)   256         conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 16, 16, 64)   0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 16, 16, 128)  8320        activation_101[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_32 (Add)                    (None, 16, 16, 128)  0           add_31[0][0]                     \n",
            "                                                                 conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 16, 16, 128)  512         add_32[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 16, 16, 128)  0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 8, 8, 128)    16512       activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 8, 8, 128)    512         conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 8, 8, 128)    0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 8, 8, 128)    147584      activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 8, 8, 128)    512         conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 8, 8, 128)    0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 8, 8, 256)    33024       add_32[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 8, 8, 256)    33024       activation_104[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_33 (Add)                    (None, 8, 8, 256)    0           conv2d_117[0][0]                 \n",
            "                                                                 conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 8, 8, 256)    1024        add_33[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 8, 8, 256)    0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 8, 8, 128)    32896       activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 8, 8, 128)    512         conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 8, 8, 128)    0           batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 8, 8, 128)    147584      activation_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 8, 8, 128)    512         conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 8, 8, 128)    0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 8, 8, 256)    33024       activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_34 (Add)                    (None, 8, 8, 256)    0           add_33[0][0]                     \n",
            "                                                                 conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 8, 8, 256)    1024        add_34[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 8, 8, 256)    0           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 8, 8, 128)    32896       activation_108[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 8, 8, 128)    512         conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 8, 8, 128)    0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 8, 8, 128)    147584      activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 8, 8, 128)    512         conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 8, 8, 128)    0           batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 8, 8, 256)    33024       activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_35 (Add)                    (None, 8, 8, 256)    0           add_34[0][0]                     \n",
            "                                                                 conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 8, 8, 256)    1024        add_35[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 8, 8, 256)    0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 1, 1, 256)    0           activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 256)          0           average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           2570        flatten_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 140s 1s/step - loss: 2.5250 - accuracy: 0.2922\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 135s 1s/step - loss: 2.1491 - accuracy: 0.4234\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 136s 1s/step - loss: 1.8823 - accuracy: 0.5237\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 136s 1s/step - loss: 1.6300 - accuracy: 0.6076\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 135s 1s/step - loss: 1.4266 - accuracy: 0.6636\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 134s 1s/step - loss: 1.3008 - accuracy: 0.6950\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 132s 1s/step - loss: 1.1403 - accuracy: 0.7394\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 130s 1s/step - loss: 1.0221 - accuracy: 0.7727\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 131s 1s/step - loss: 0.9297 - accuracy: 0.7977\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 131s 1s/step - loss: 0.8583 - accuracy: 0.8134\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 132s 1s/step - loss: 0.8013 - accuracy: 0.8279\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 132s 1s/step - loss: 0.7469 - accuracy: 0.8438\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 133s 1s/step - loss: 0.7006 - accuracy: 0.8534\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 134s 1s/step - loss: 0.6566 - accuracy: 0.8651\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 133s 1s/step - loss: 0.6214 - accuracy: 0.8760\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 133s 1s/step - loss: 0.5811 - accuracy: 0.8872\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 134s 1s/step - loss: 0.5558 - accuracy: 0.8917\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 135s 1s/step - loss: 0.5168 - accuracy: 0.9051\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 134s 1s/step - loss: 0.4784 - accuracy: 0.9148\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 133s 1s/step - loss: 0.4476 - accuracy: 0.9243\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 133s 1s/step - loss: 0.4111 - accuracy: 0.9364\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 133s 1s/step - loss: 0.3812 - accuracy: 0.9450\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 134s 1s/step - loss: 0.3423 - accuracy: 0.9583\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 135s 1s/step - loss: 0.3119 - accuracy: 0.9696\n",
            "20/20 [==============================] - 2s 112ms/step - loss: 0.6046 - accuracy: 0.8792\n",
            "val accuracy score at the end of training model type  5 [0.6046453267335892, 0.8791992]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  6\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function angles_to_projective_transforms at 0x7f7594936048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function rotate at 0x7f7594936268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 32, 32, 16)   448         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 32, 32, 16)   64          conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 32, 32, 16)   0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 32, 32, 16)   272         activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 32, 32, 16)   0           conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 32, 32, 16)   64          lambda_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 32, 32, 16)   0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 32, 32, 16)   2320        activation_113[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 32, 32, 16)   64          conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 32, 32, 16)   0           batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 32, 32, 64)   1088        activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 32, 32, 64)   1088        activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_36 (Add)                    (None, 32, 32, 64)   0           conv2d_128[0][0]                 \n",
            "                                                                 conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 32, 32, 64)   256         add_36[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 32, 32, 64)   0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 32, 32, 16)   1040        activation_115[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 32, 32, 16)   0           conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 32, 32, 16)   64          lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 32, 32, 16)   0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 32, 32, 16)   2320        activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 32, 32, 16)   64          conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 32, 32, 16)   0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 32, 32, 64)   1088        activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_37 (Add)                    (None, 32, 32, 64)   0           add_36[0][0]                     \n",
            "                                                                 conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 32, 32, 64)   256         add_37[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 32, 32, 64)   0           batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 32, 32, 16)   1040        activation_118[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 32, 32, 16)   0           conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 32, 32, 16)   64          lambda_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 32, 32, 16)   0           batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 32, 32, 16)   2320        activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 32, 32, 16)   64          conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 32, 32, 16)   0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 32, 32, 64)   1088        activation_120[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_38 (Add)                    (None, 32, 32, 64)   0           add_37[0][0]                     \n",
            "                                                                 conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 32, 32, 64)   256         add_38[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 32, 32, 64)   0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 16, 16, 64)   4160        activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 16, 16, 64)   256         conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 16, 16, 64)   0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 16, 16, 64)   36928       activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 16, 16, 64)   256         conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 16, 16, 64)   0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 16, 16, 128)  8320        add_38[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 16, 16, 128)  8320        activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_39 (Add)                    (None, 16, 16, 128)  0           conv2d_138[0][0]                 \n",
            "                                                                 conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 16, 16, 128)  512         add_39[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 16, 16, 128)  0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 16, 16, 64)   8256        activation_124[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 16, 16, 64)   256         conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 16, 16, 64)   0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 16, 16, 64)   36928       activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 16, 16, 64)   256         conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 16, 16, 64)   0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 16, 16, 128)  8320        activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_40 (Add)                    (None, 16, 16, 128)  0           add_39[0][0]                     \n",
            "                                                                 conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 16, 16, 128)  512         add_40[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 16, 16, 128)  0           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 16, 16, 64)   8256        activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 16, 16, 64)   256         conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 16, 16, 64)   0           batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 16, 16, 64)   36928       activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 16, 16, 64)   256         conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 16, 16, 64)   0           batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 16, 16, 128)  8320        activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_41 (Add)                    (None, 16, 16, 128)  0           add_40[0][0]                     \n",
            "                                                                 conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 16, 16, 128)  512         add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 16, 16, 128)  0           batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, 8, 8, 128)    16512       activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 8, 8, 128)    512         conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 8, 8, 128)    0           batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, 8, 8, 128)    147584      activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 8, 8, 128)    512         conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 8, 8, 128)    0           batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_148 (Conv2D)             (None, 8, 8, 256)    33024       add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, 8, 8, 256)    33024       activation_132[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_42 (Add)                    (None, 8, 8, 256)    0           conv2d_148[0][0]                 \n",
            "                                                                 conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, 8, 8, 256)    1024        add_42[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 8, 8, 256)    0           batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_149 (Conv2D)             (None, 8, 8, 128)    32896       activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 8, 8, 128)    512         conv2d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 8, 8, 128)    0           batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_150 (Conv2D)             (None, 8, 8, 128)    147584      activation_134[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 8, 8, 128)    512         conv2d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 8, 8, 128)    0           batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_151 (Conv2D)             (None, 8, 8, 256)    33024       activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_43 (Add)                    (None, 8, 8, 256)    0           add_42[0][0]                     \n",
            "                                                                 conv2d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 8, 8, 256)    1024        add_43[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 8, 8, 256)    0           batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_152 (Conv2D)             (None, 8, 8, 128)    32896       activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 8, 8, 128)    512         conv2d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 8, 8, 128)    0           batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, 8, 8, 128)    147584      activation_137[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 8, 8, 128)    512         conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 8, 8, 128)    0           batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, 8, 8, 256)    33024       activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_44 (Add)                    (None, 8, 8, 256)    0           add_43[0][0]                     \n",
            "                                                                 conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, 8, 8, 256)    1024        add_44[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 8, 8, 256)    0           batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 1, 1, 256)    0           activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 256)          0           average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 10)           2570        flatten_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function angles_to_projective_transforms at 0x7f7594936048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 8 calls to <function rotate at 0x7f7594936268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function angles_to_projective_transforms at 0x7f7594936048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 9 calls to <function rotate at 0x7f7594936268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 8 calls to <function angles_to_projective_transforms at 0x7f7594936048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function rotate at 0x7f7594936268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "98/98 [==============================] - 169s 2s/step - loss: 2.4874 - accuracy: 0.3168\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 162s 2s/step - loss: 2.1294 - accuracy: 0.4364\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 163s 2s/step - loss: 1.8103 - accuracy: 0.5500\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 162s 2s/step - loss: 1.5455 - accuracy: 0.6363\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 161s 2s/step - loss: 1.3605 - accuracy: 0.6942\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 161s 2s/step - loss: 1.2333 - accuracy: 0.7268\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 161s 2s/step - loss: 1.0747 - accuracy: 0.7730\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.9584 - accuracy: 0.8036\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.8710 - accuracy: 0.8270\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.7948 - accuracy: 0.8483\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.7349 - accuracy: 0.8679\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.6678 - accuracy: 0.8898\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.6208 - accuracy: 0.9045\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.5897 - accuracy: 0.9107\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.5378 - accuracy: 0.9288\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.4931 - accuracy: 0.9442\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.4530 - accuracy: 0.9555\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.4158 - accuracy: 0.9670\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.3848 - accuracy: 0.9755\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.3459 - accuracy: 0.9878\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.3127 - accuracy: 0.9960\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.2938 - accuracy: 0.9995\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.2849 - accuracy: 1.0000\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.2806 - accuracy: 1.0000\n",
            "20/20 [==============================] - 2s 111ms/step - loss: 1.5218 - accuracy: 0.7692\n",
            "val accuracy score at the end of training model type  6 [1.5218131363391876, 0.7692383]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  7\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, 32, 32, 16)   448         input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_140 (BatchN (None, 32, 32, 16)   64          conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 32, 32, 16)   0           batch_normalization_140[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, 32, 32, 16)   272         activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 32, 32, 16)   0           conv2d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_141 (BatchN (None, 32, 32, 16)   64          lambda_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 32, 32, 16)   0           batch_normalization_141[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, 32, 32, 16)   2320        activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_142 (BatchN (None, 32, 32, 16)   64          conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 32, 32, 16)   0           batch_normalization_142[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 32, 32, 64)   1088        activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, 32, 32, 64)   1088        activation_142[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_45 (Add)                    (None, 32, 32, 64)   0           conv2d_159[0][0]                 \n",
            "                                                                 conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_143 (BatchN (None, 32, 32, 64)   256         add_45[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 32, 32, 64)   0           batch_normalization_143[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 32, 32, 16)   1040        activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 32, 32, 16)   0           conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_144 (BatchN (None, 32, 32, 16)   64          lambda_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 32, 32, 16)   0           batch_normalization_144[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 32, 32, 16)   2320        activation_144[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_145 (BatchN (None, 32, 32, 16)   64          conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 32, 32, 16)   0           batch_normalization_145[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 32, 32, 64)   1088        activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_46 (Add)                    (None, 32, 32, 64)   0           add_45[0][0]                     \n",
            "                                                                 conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_146 (BatchN (None, 32, 32, 64)   256         add_46[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 32, 32, 64)   0           batch_normalization_146[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 32, 32, 16)   1040        activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 32, 32, 16)   0           conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_147 (BatchN (None, 32, 32, 16)   64          lambda_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 32, 32, 16)   0           batch_normalization_147[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_164 (Conv2D)             (None, 32, 32, 16)   2320        activation_147[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_148 (BatchN (None, 32, 32, 16)   64          conv2d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 32, 32, 16)   0           batch_normalization_148[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_165 (Conv2D)             (None, 32, 32, 64)   1088        activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_47 (Add)                    (None, 32, 32, 64)   0           add_46[0][0]                     \n",
            "                                                                 conv2d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_149 (BatchN (None, 32, 32, 64)   256         add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 32, 32, 64)   0           batch_normalization_149[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_166 (Conv2D)             (None, 16, 16, 64)   4160        activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_150 (BatchN (None, 16, 16, 64)   256         conv2d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 16, 16, 64)   0           batch_normalization_150[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_167 (Conv2D)             (None, 16, 16, 64)   36928       activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_151 (BatchN (None, 16, 16, 64)   256         conv2d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 16, 16, 64)   0           batch_normalization_151[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_169 (Conv2D)             (None, 16, 16, 128)  8320        add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_168 (Conv2D)             (None, 16, 16, 128)  8320        activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_48 (Add)                    (None, 16, 16, 128)  0           conv2d_169[0][0]                 \n",
            "                                                                 conv2d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_152 (BatchN (None, 16, 16, 128)  512         add_48[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 16, 16, 128)  0           batch_normalization_152[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, 16, 16, 64)   8256        activation_152[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_153 (BatchN (None, 16, 16, 64)   256         conv2d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 16, 16, 64)   0           batch_normalization_153[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, 16, 16, 64)   36928       activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, 16, 16, 64)   256         conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 16, 16, 64)   0           batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_172 (Conv2D)             (None, 16, 16, 128)  8320        activation_154[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_49 (Add)                    (None, 16, 16, 128)  0           add_48[0][0]                     \n",
            "                                                                 conv2d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, 16, 16, 128)  512         add_49[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 16, 16, 128)  0           batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_173 (Conv2D)             (None, 16, 16, 64)   8256        activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 16, 16, 64)   256         conv2d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 16, 16, 64)   0           batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, 16, 16, 64)   36928       activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, 16, 16, 64)   256         conv2d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 16, 16, 64)   0           batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_175 (Conv2D)             (None, 16, 16, 128)  8320        activation_157[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_50 (Add)                    (None, 16, 16, 128)  0           add_49[0][0]                     \n",
            "                                                                 conv2d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, 16, 16, 128)  512         add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 16, 16, 128)  0           batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_176 (Conv2D)             (None, 8, 8, 128)    16512       activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, 8, 8, 128)    512         conv2d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 8, 8, 128)    0           batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_177 (Conv2D)             (None, 8, 8, 128)    147584      activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, 8, 8, 128)    512         conv2d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 8, 8, 128)    0           batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, 8, 8, 256)    33024       add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, 8, 8, 256)    33024       activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_51 (Add)                    (None, 8, 8, 256)    0           conv2d_179[0][0]                 \n",
            "                                                                 conv2d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, 8, 8, 256)    1024        add_51[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 8, 8, 256)    0           batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, 8, 8, 128)    32896       activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, 8, 8, 128)    512         conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 8, 8, 128)    0           batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 8, 8, 128)    147584      activation_162[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, 8, 8, 128)    512         conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 8, 8, 128)    0           batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 8, 8, 256)    33024       activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_52 (Add)                    (None, 8, 8, 256)    0           add_51[0][0]                     \n",
            "                                                                 conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_164 (BatchN (None, 8, 8, 256)    1024        add_52[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, 8, 8, 256)    0           batch_normalization_164[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 8, 8, 128)    32896       activation_164[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, 8, 8, 128)    512         conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 8, 8, 128)    0           batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 8, 8, 128)    147584      activation_165[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, 8, 8, 128)    512         conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, 8, 8, 128)    0           batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 8, 8, 256)    33024       activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_53 (Add)                    (None, 8, 8, 256)    0           add_52[0][0]                     \n",
            "                                                                 conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 8, 8, 256)    1024        add_53[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, 8, 8, 256)    0           batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 1, 1, 256)    0           activation_167[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 256)          0           average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 10)           2570        flatten_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 183s 2s/step - loss: 2.4519 - accuracy: 0.3351\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 177s 2s/step - loss: 2.0393 - accuracy: 0.4744\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 175s 2s/step - loss: 1.7131 - accuracy: 0.5850\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 176s 2s/step - loss: 1.4664 - accuracy: 0.6666\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 176s 2s/step - loss: 1.3083 - accuracy: 0.7099\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 175s 2s/step - loss: 1.1598 - accuracy: 0.7524\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 176s 2s/step - loss: 0.9959 - accuracy: 0.8004\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 175s 2s/step - loss: 0.8882 - accuracy: 0.8278\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 177s 2s/step - loss: 0.8020 - accuracy: 0.8510\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 176s 2s/step - loss: 0.7366 - accuracy: 0.8675\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 176s 2s/step - loss: 0.6737 - accuracy: 0.8861\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 176s 2s/step - loss: 0.6259 - accuracy: 0.8969\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 175s 2s/step - loss: 0.5832 - accuracy: 0.9105\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 178s 2s/step - loss: 0.5454 - accuracy: 0.9211\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 172s 2s/step - loss: 0.4916 - accuracy: 0.9382\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 169s 2s/step - loss: 0.4651 - accuracy: 0.9456\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 168s 2s/step - loss: 0.4287 - accuracy: 0.9572\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 168s 2s/step - loss: 0.3874 - accuracy: 0.9695\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 167s 2s/step - loss: 0.3567 - accuracy: 0.9776\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 167s 2s/step - loss: 0.3186 - accuracy: 0.9886\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.2944 - accuracy: 0.9943\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 168s 2s/step - loss: 0.2770 - accuracy: 0.9983\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 167s 2s/step - loss: 0.2669 - accuracy: 0.9994\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 167s 2s/step - loss: 0.2619 - accuracy: 0.9997\n",
            "20/20 [==============================] - 22s 1s/step - loss: 0.9263 - accuracy: 0.8518\n",
            "val accuracy score at the end of training model type  7 [0.9262648344039917, 0.8517578]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  8\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 32, 32, 16)   448         input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 32, 32, 16)   64          conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_168 (Activation)     (None, 32, 32, 16)   0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 32, 32, 16)   272         activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_12 (Lambda)              (None, 32, 32, 16)   0           conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 32, 32, 16)   64          lambda_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_169 (Activation)     (None, 32, 32, 16)   0           batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 32, 32, 16)   2320        activation_169[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 32, 32, 16)   64          conv2d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_170 (Activation)     (None, 32, 32, 16)   0           batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 32, 32, 64)   1088        activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 32, 32, 64)   1088        activation_170[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_54 (Add)                    (None, 32, 32, 64)   0           conv2d_190[0][0]                 \n",
            "                                                                 conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 32, 32, 64)   256         add_54[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_171 (Activation)     (None, 32, 32, 64)   0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 32, 32, 16)   1040        activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_13 (Lambda)              (None, 32, 32, 16)   0           conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 32, 32, 16)   64          lambda_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_172 (Activation)     (None, 32, 32, 16)   0           batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 32, 32, 16)   2320        activation_172[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 32, 32, 16)   64          conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_173 (Activation)     (None, 32, 32, 16)   0           batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 32, 32, 64)   1088        activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_55 (Add)                    (None, 32, 32, 64)   0           add_54[0][0]                     \n",
            "                                                                 conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 32, 32, 64)   256         add_55[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_174 (Activation)     (None, 32, 32, 64)   0           batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 32, 32, 16)   1040        activation_174[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_14 (Lambda)              (None, 32, 32, 16)   0           conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 32, 32, 16)   64          lambda_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_175 (Activation)     (None, 32, 32, 16)   0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 32, 32, 16)   2320        activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 32, 32, 16)   64          conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_176 (Activation)     (None, 32, 32, 16)   0           batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 32, 32, 64)   1088        activation_176[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_56 (Add)                    (None, 32, 32, 64)   0           add_55[0][0]                     \n",
            "                                                                 conv2d_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, 32, 32, 64)   256         add_56[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_177 (Activation)     (None, 32, 32, 64)   0           batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 16, 16, 64)   4160        activation_177[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, 16, 16, 64)   256         conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_178 (Activation)     (None, 16, 16, 64)   0           batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 16, 16, 64)   36928       activation_178[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, 16, 16, 64)   256         conv2d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_179 (Activation)     (None, 16, 16, 64)   0           batch_normalization_179[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 16, 16, 128)  8320        add_56[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 16, 16, 128)  8320        activation_179[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_57 (Add)                    (None, 16, 16, 128)  0           conv2d_200[0][0]                 \n",
            "                                                                 conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, 16, 16, 128)  512         add_57[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_180 (Activation)     (None, 16, 16, 128)  0           batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 16, 16, 64)   8256        activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_181 (BatchN (None, 16, 16, 64)   256         conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_181 (Activation)     (None, 16, 16, 64)   0           batch_normalization_181[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 16, 16, 64)   36928       activation_181[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_182 (BatchN (None, 16, 16, 64)   256         conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_182 (Activation)     (None, 16, 16, 64)   0           batch_normalization_182[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 16, 16, 128)  8320        activation_182[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_58 (Add)                    (None, 16, 16, 128)  0           add_57[0][0]                     \n",
            "                                                                 conv2d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_183 (BatchN (None, 16, 16, 128)  512         add_58[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_183 (Activation)     (None, 16, 16, 128)  0           batch_normalization_183[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 16, 16, 64)   8256        activation_183[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_184 (BatchN (None, 16, 16, 64)   256         conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_184 (Activation)     (None, 16, 16, 64)   0           batch_normalization_184[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 16, 16, 64)   36928       activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_185 (BatchN (None, 16, 16, 64)   256         conv2d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_185 (Activation)     (None, 16, 16, 64)   0           batch_normalization_185[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 16, 16, 128)  8320        activation_185[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_59 (Add)                    (None, 16, 16, 128)  0           add_58[0][0]                     \n",
            "                                                                 conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_186 (BatchN (None, 16, 16, 128)  512         add_59[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_186 (Activation)     (None, 16, 16, 128)  0           batch_normalization_186[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 8, 8, 128)    16512       activation_186[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_187 (BatchN (None, 8, 8, 128)    512         conv2d_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_187 (Activation)     (None, 8, 8, 128)    0           batch_normalization_187[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 8, 8, 128)    147584      activation_187[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_188 (BatchN (None, 8, 8, 128)    512         conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_188 (Activation)     (None, 8, 8, 128)    0           batch_normalization_188[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 8, 8, 256)    33024       add_59[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 8, 8, 256)    33024       activation_188[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_60 (Add)                    (None, 8, 8, 256)    0           conv2d_210[0][0]                 \n",
            "                                                                 conv2d_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_189 (BatchN (None, 8, 8, 256)    1024        add_60[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_189 (Activation)     (None, 8, 8, 256)    0           batch_normalization_189[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 8, 8, 128)    32896       activation_189[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_190 (BatchN (None, 8, 8, 128)    512         conv2d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_190 (Activation)     (None, 8, 8, 128)    0           batch_normalization_190[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_212 (Conv2D)             (None, 8, 8, 128)    147584      activation_190[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_191 (BatchN (None, 8, 8, 128)    512         conv2d_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_191 (Activation)     (None, 8, 8, 128)    0           batch_normalization_191[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_213 (Conv2D)             (None, 8, 8, 256)    33024       activation_191[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_61 (Add)                    (None, 8, 8, 256)    0           add_60[0][0]                     \n",
            "                                                                 conv2d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_192 (BatchN (None, 8, 8, 256)    1024        add_61[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_192 (Activation)     (None, 8, 8, 256)    0           batch_normalization_192[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_214 (Conv2D)             (None, 8, 8, 128)    32896       activation_192[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_193 (BatchN (None, 8, 8, 128)    512         conv2d_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_193 (Activation)     (None, 8, 8, 128)    0           batch_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_215 (Conv2D)             (None, 8, 8, 128)    147584      activation_193[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_194 (BatchN (None, 8, 8, 128)    512         conv2d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_194 (Activation)     (None, 8, 8, 128)    0           batch_normalization_194[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_216 (Conv2D)             (None, 8, 8, 256)    33024       activation_194[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_62 (Add)                    (None, 8, 8, 256)    0           add_61[0][0]                     \n",
            "                                                                 conv2d_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_195 (BatchN (None, 8, 8, 256)    1024        add_62[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_195 (Activation)     (None, 8, 8, 256)    0           batch_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 1, 1, 256)    0           activation_195[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 256)          0           average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 10)           2570        flatten_6[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 117s 1s/step - loss: 2.4672 - accuracy: 0.3207\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 2.0847 - accuracy: 0.4562\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 1.8072 - accuracy: 0.5527\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 1.5835 - accuracy: 0.6244\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 1.4104 - accuracy: 0.6721\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 1.2780 - accuracy: 0.7057\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 111s 1s/step - loss: 1.1464 - accuracy: 0.7385\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 1.0408 - accuracy: 0.7655\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 0.9584 - accuracy: 0.7856\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 111s 1s/step - loss: 0.8866 - accuracy: 0.8031\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 0.8300 - accuracy: 0.8174\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 0.7821 - accuracy: 0.8310\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 111s 1s/step - loss: 0.7370 - accuracy: 0.8423\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 0.6910 - accuracy: 0.8559\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 0.6504 - accuracy: 0.8685\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 0.6132 - accuracy: 0.8787\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 0.5723 - accuracy: 0.8936\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 0.5258 - accuracy: 0.9069\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 0.4945 - accuracy: 0.9170\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 109s 1s/step - loss: 0.4496 - accuracy: 0.9311\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 0.4100 - accuracy: 0.9441\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 0.3629 - accuracy: 0.9608\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 0.3247 - accuracy: 0.9731\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 110s 1s/step - loss: 0.2949 - accuracy: 0.9841\n",
            "20/20 [==============================] - 3s 134ms/step - loss: 0.7857 - accuracy: 0.8479\n",
            "val accuracy score at the end of training model type  8 [0.7856774806976319, 0.8478516]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  9\n",
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_217 (Conv2D)             (None, 32, 32, 16)   448         input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_196 (BatchN (None, 32, 32, 16)   64          conv2d_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_196 (Activation)     (None, 32, 32, 16)   0           batch_normalization_196[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_218 (Conv2D)             (None, 32, 32, 16)   272         activation_196[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_197 (BatchN (None, 32, 32, 16)   64          conv2d_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_197 (Activation)     (None, 32, 32, 16)   0           batch_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_219 (Conv2D)             (None, 32, 32, 16)   2320        activation_197[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_198 (BatchN (None, 32, 32, 16)   64          conv2d_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_198 (Activation)     (None, 32, 32, 16)   0           batch_normalization_198[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_221 (Conv2D)             (None, 32, 32, 64)   1088        activation_196[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_220 (Conv2D)             (None, 32, 32, 64)   1088        activation_198[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_63 (Add)                    (None, 32, 32, 64)   0           conv2d_221[0][0]                 \n",
            "                                                                 conv2d_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_199 (BatchN (None, 32, 32, 64)   256         add_63[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_199 (Activation)     (None, 32, 32, 64)   0           batch_normalization_199[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_222 (Conv2D)             (None, 32, 32, 16)   1040        activation_199[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_200 (BatchN (None, 32, 32, 16)   64          conv2d_222[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_200 (Activation)     (None, 32, 32, 16)   0           batch_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_223 (Conv2D)             (None, 32, 32, 16)   2320        activation_200[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_201 (BatchN (None, 32, 32, 16)   64          conv2d_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_201 (Activation)     (None, 32, 32, 16)   0           batch_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_224 (Conv2D)             (None, 32, 32, 64)   1088        activation_201[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_64 (Add)                    (None, 32, 32, 64)   0           add_63[0][0]                     \n",
            "                                                                 conv2d_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_202 (BatchN (None, 32, 32, 64)   256         add_64[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_202 (Activation)     (None, 32, 32, 64)   0           batch_normalization_202[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_225 (Conv2D)             (None, 32, 32, 16)   1040        activation_202[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_203 (BatchN (None, 32, 32, 16)   64          conv2d_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_203 (Activation)     (None, 32, 32, 16)   0           batch_normalization_203[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_226 (Conv2D)             (None, 32, 32, 16)   2320        activation_203[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_204 (BatchN (None, 32, 32, 16)   64          conv2d_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_204 (Activation)     (None, 32, 32, 16)   0           batch_normalization_204[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_227 (Conv2D)             (None, 32, 32, 64)   1088        activation_204[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_65 (Add)                    (None, 32, 32, 64)   0           add_64[0][0]                     \n",
            "                                                                 conv2d_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_205 (BatchN (None, 32, 32, 64)   256         add_65[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_205 (Activation)     (None, 32, 32, 64)   0           batch_normalization_205[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_228 (Conv2D)             (None, 16, 16, 64)   4160        activation_205[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_15 (Lambda)              (None, 16, 16, 64)   0           conv2d_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_206 (BatchN (None, 16, 16, 64)   256         lambda_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_206 (Activation)     (None, 16, 16, 64)   0           batch_normalization_206[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_229 (Conv2D)             (None, 16, 16, 64)   36928       activation_206[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_207 (BatchN (None, 16, 16, 64)   256         conv2d_229[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_207 (Activation)     (None, 16, 16, 64)   0           batch_normalization_207[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_231 (Conv2D)             (None, 16, 16, 128)  8320        add_65[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_230 (Conv2D)             (None, 16, 16, 128)  8320        activation_207[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_66 (Add)                    (None, 16, 16, 128)  0           conv2d_231[0][0]                 \n",
            "                                                                 conv2d_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_208 (BatchN (None, 16, 16, 128)  512         add_66[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_208 (Activation)     (None, 16, 16, 128)  0           batch_normalization_208[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_232 (Conv2D)             (None, 16, 16, 64)   8256        activation_208[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_16 (Lambda)              (None, 16, 16, 64)   0           conv2d_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_209 (BatchN (None, 16, 16, 64)   256         lambda_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_209 (Activation)     (None, 16, 16, 64)   0           batch_normalization_209[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_233 (Conv2D)             (None, 16, 16, 64)   36928       activation_209[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_210 (BatchN (None, 16, 16, 64)   256         conv2d_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_210 (Activation)     (None, 16, 16, 64)   0           batch_normalization_210[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_234 (Conv2D)             (None, 16, 16, 128)  8320        activation_210[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_67 (Add)                    (None, 16, 16, 128)  0           add_66[0][0]                     \n",
            "                                                                 conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_211 (BatchN (None, 16, 16, 128)  512         add_67[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_211 (Activation)     (None, 16, 16, 128)  0           batch_normalization_211[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_235 (Conv2D)             (None, 16, 16, 64)   8256        activation_211[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_17 (Lambda)              (None, 16, 16, 64)   0           conv2d_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_212 (BatchN (None, 16, 16, 64)   256         lambda_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_212 (Activation)     (None, 16, 16, 64)   0           batch_normalization_212[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_236 (Conv2D)             (None, 16, 16, 64)   36928       activation_212[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_213 (BatchN (None, 16, 16, 64)   256         conv2d_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_213 (Activation)     (None, 16, 16, 64)   0           batch_normalization_213[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_237 (Conv2D)             (None, 16, 16, 128)  8320        activation_213[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_68 (Add)                    (None, 16, 16, 128)  0           add_67[0][0]                     \n",
            "                                                                 conv2d_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_214 (BatchN (None, 16, 16, 128)  512         add_68[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_214 (Activation)     (None, 16, 16, 128)  0           batch_normalization_214[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_238 (Conv2D)             (None, 8, 8, 128)    16512       activation_214[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_215 (BatchN (None, 8, 8, 128)    512         conv2d_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_215 (Activation)     (None, 8, 8, 128)    0           batch_normalization_215[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_239 (Conv2D)             (None, 8, 8, 128)    147584      activation_215[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_216 (BatchN (None, 8, 8, 128)    512         conv2d_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_216 (Activation)     (None, 8, 8, 128)    0           batch_normalization_216[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_241 (Conv2D)             (None, 8, 8, 256)    33024       add_68[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_240 (Conv2D)             (None, 8, 8, 256)    33024       activation_216[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_69 (Add)                    (None, 8, 8, 256)    0           conv2d_241[0][0]                 \n",
            "                                                                 conv2d_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_217 (BatchN (None, 8, 8, 256)    1024        add_69[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_217 (Activation)     (None, 8, 8, 256)    0           batch_normalization_217[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_242 (Conv2D)             (None, 8, 8, 128)    32896       activation_217[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_218 (BatchN (None, 8, 8, 128)    512         conv2d_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_218 (Activation)     (None, 8, 8, 128)    0           batch_normalization_218[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_243 (Conv2D)             (None, 8, 8, 128)    147584      activation_218[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_219 (BatchN (None, 8, 8, 128)    512         conv2d_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_219 (Activation)     (None, 8, 8, 128)    0           batch_normalization_219[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_244 (Conv2D)             (None, 8, 8, 256)    33024       activation_219[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_70 (Add)                    (None, 8, 8, 256)    0           add_69[0][0]                     \n",
            "                                                                 conv2d_244[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_220 (BatchN (None, 8, 8, 256)    1024        add_70[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_220 (Activation)     (None, 8, 8, 256)    0           batch_normalization_220[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_245 (Conv2D)             (None, 8, 8, 128)    32896       activation_220[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_221 (BatchN (None, 8, 8, 128)    512         conv2d_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_221 (Activation)     (None, 8, 8, 128)    0           batch_normalization_221[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_246 (Conv2D)             (None, 8, 8, 128)    147584      activation_221[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_222 (BatchN (None, 8, 8, 128)    512         conv2d_246[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_222 (Activation)     (None, 8, 8, 128)    0           batch_normalization_222[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_247 (Conv2D)             (None, 8, 8, 256)    33024       activation_222[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_71 (Add)                    (None, 8, 8, 256)    0           add_70[0][0]                     \n",
            "                                                                 conv2d_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_223 (BatchN (None, 8, 8, 256)    1024        add_71[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_223 (Activation)     (None, 8, 8, 256)    0           batch_normalization_223[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 1, 1, 256)    0           activation_223[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 256)          0           average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 10)           2570        flatten_7[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 128s 1s/step - loss: 2.4940 - accuracy: 0.3115\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 122s 1s/step - loss: 2.0754 - accuracy: 0.4553\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 122s 1s/step - loss: 1.8020 - accuracy: 0.5530\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 123s 1s/step - loss: 1.5786 - accuracy: 0.6216\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 123s 1s/step - loss: 1.4134 - accuracy: 0.6698\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 122s 1s/step - loss: 1.2804 - accuracy: 0.7020\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 123s 1s/step - loss: 1.1280 - accuracy: 0.7444\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 123s 1s/step - loss: 1.0082 - accuracy: 0.7779\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 122s 1s/step - loss: 0.9094 - accuracy: 0.8041\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 124s 1s/step - loss: 0.8391 - accuracy: 0.8208\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 123s 1s/step - loss: 0.7665 - accuracy: 0.8407\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 123s 1s/step - loss: 0.7110 - accuracy: 0.8542\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 124s 1s/step - loss: 0.6645 - accuracy: 0.8675\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 127s 1s/step - loss: 0.6189 - accuracy: 0.8789\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 127s 1s/step - loss: 0.5768 - accuracy: 0.8909\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 126s 1s/step - loss: 0.5322 - accuracy: 0.9040\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 127s 1s/step - loss: 0.4927 - accuracy: 0.9152\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 126s 1s/step - loss: 0.4550 - accuracy: 0.9279\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 124s 1s/step - loss: 0.4116 - accuracy: 0.9405\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 125s 1s/step - loss: 0.3673 - accuracy: 0.9562\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 124s 1s/step - loss: 0.3212 - accuracy: 0.9719\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 125s 1s/step - loss: 0.2812 - accuracy: 0.9854\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 125s 1s/step - loss: 0.2541 - accuracy: 0.9938\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 124s 1s/step - loss: 0.2394 - accuracy: 0.9978\n",
            "20/20 [==============================] - 2s 110ms/step - loss: 0.7889 - accuracy: 0.8564\n",
            "val accuracy score at the end of training model type  9 [0.7888979226350784, 0.8564453]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  10\n",
            "WARNING:tensorflow:9 out of the last 9 calls to <function angles_to_projective_transforms at 0x7f7594936048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function rotate at 0x7f7594936268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function rotate at 0x7f7594936268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function rotate at 0x7f7594936268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_248 (Conv2D)             (None, 32, 32, 16)   448         input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_224 (BatchN (None, 32, 32, 16)   64          conv2d_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_224 (Activation)     (None, 32, 32, 16)   0           batch_normalization_224[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_249 (Conv2D)             (None, 32, 32, 16)   272         activation_224[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_225 (BatchN (None, 32, 32, 16)   64          conv2d_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_225 (Activation)     (None, 32, 32, 16)   0           batch_normalization_225[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_250 (Conv2D)             (None, 32, 32, 16)   2320        activation_225[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_226 (BatchN (None, 32, 32, 16)   64          conv2d_250[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_226 (Activation)     (None, 32, 32, 16)   0           batch_normalization_226[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_252 (Conv2D)             (None, 32, 32, 64)   1088        activation_224[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_251 (Conv2D)             (None, 32, 32, 64)   1088        activation_226[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_72 (Add)                    (None, 32, 32, 64)   0           conv2d_252[0][0]                 \n",
            "                                                                 conv2d_251[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_227 (BatchN (None, 32, 32, 64)   256         add_72[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_227 (Activation)     (None, 32, 32, 64)   0           batch_normalization_227[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_253 (Conv2D)             (None, 32, 32, 16)   1040        activation_227[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_228 (BatchN (None, 32, 32, 16)   64          conv2d_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_228 (Activation)     (None, 32, 32, 16)   0           batch_normalization_228[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_254 (Conv2D)             (None, 32, 32, 16)   2320        activation_228[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_229 (BatchN (None, 32, 32, 16)   64          conv2d_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_229 (Activation)     (None, 32, 32, 16)   0           batch_normalization_229[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_255 (Conv2D)             (None, 32, 32, 64)   1088        activation_229[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_73 (Add)                    (None, 32, 32, 64)   0           add_72[0][0]                     \n",
            "                                                                 conv2d_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_230 (BatchN (None, 32, 32, 64)   256         add_73[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_230 (Activation)     (None, 32, 32, 64)   0           batch_normalization_230[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_256 (Conv2D)             (None, 32, 32, 16)   1040        activation_230[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_231 (BatchN (None, 32, 32, 16)   64          conv2d_256[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_231 (Activation)     (None, 32, 32, 16)   0           batch_normalization_231[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_257 (Conv2D)             (None, 32, 32, 16)   2320        activation_231[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_232 (BatchN (None, 32, 32, 16)   64          conv2d_257[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_232 (Activation)     (None, 32, 32, 16)   0           batch_normalization_232[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_258 (Conv2D)             (None, 32, 32, 64)   1088        activation_232[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_74 (Add)                    (None, 32, 32, 64)   0           add_73[0][0]                     \n",
            "                                                                 conv2d_258[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_233 (BatchN (None, 32, 32, 64)   256         add_74[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_233 (Activation)     (None, 32, 32, 64)   0           batch_normalization_233[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_259 (Conv2D)             (None, 16, 16, 64)   4160        activation_233[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_18 (Lambda)              (None, 16, 16, 64)   0           conv2d_259[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_234 (BatchN (None, 16, 16, 64)   256         lambda_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_234 (Activation)     (None, 16, 16, 64)   0           batch_normalization_234[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_260 (Conv2D)             (None, 16, 16, 64)   36928       activation_234[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_235 (BatchN (None, 16, 16, 64)   256         conv2d_260[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_235 (Activation)     (None, 16, 16, 64)   0           batch_normalization_235[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_262 (Conv2D)             (None, 16, 16, 128)  8320        add_74[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_261 (Conv2D)             (None, 16, 16, 128)  8320        activation_235[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_75 (Add)                    (None, 16, 16, 128)  0           conv2d_262[0][0]                 \n",
            "                                                                 conv2d_261[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_236 (BatchN (None, 16, 16, 128)  512         add_75[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_236 (Activation)     (None, 16, 16, 128)  0           batch_normalization_236[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_263 (Conv2D)             (None, 16, 16, 64)   8256        activation_236[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_19 (Lambda)              (None, 16, 16, 64)   0           conv2d_263[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_237 (BatchN (None, 16, 16, 64)   256         lambda_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_237 (Activation)     (None, 16, 16, 64)   0           batch_normalization_237[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_264 (Conv2D)             (None, 16, 16, 64)   36928       activation_237[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_238 (BatchN (None, 16, 16, 64)   256         conv2d_264[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_238 (Activation)     (None, 16, 16, 64)   0           batch_normalization_238[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_265 (Conv2D)             (None, 16, 16, 128)  8320        activation_238[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_76 (Add)                    (None, 16, 16, 128)  0           add_75[0][0]                     \n",
            "                                                                 conv2d_265[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_239 (BatchN (None, 16, 16, 128)  512         add_76[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_239 (Activation)     (None, 16, 16, 128)  0           batch_normalization_239[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_266 (Conv2D)             (None, 16, 16, 64)   8256        activation_239[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_20 (Lambda)              (None, 16, 16, 64)   0           conv2d_266[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_240 (BatchN (None, 16, 16, 64)   256         lambda_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_240 (Activation)     (None, 16, 16, 64)   0           batch_normalization_240[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_267 (Conv2D)             (None, 16, 16, 64)   36928       activation_240[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_241 (BatchN (None, 16, 16, 64)   256         conv2d_267[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_241 (Activation)     (None, 16, 16, 64)   0           batch_normalization_241[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_268 (Conv2D)             (None, 16, 16, 128)  8320        activation_241[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_77 (Add)                    (None, 16, 16, 128)  0           add_76[0][0]                     \n",
            "                                                                 conv2d_268[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_242 (BatchN (None, 16, 16, 128)  512         add_77[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_242 (Activation)     (None, 16, 16, 128)  0           batch_normalization_242[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_269 (Conv2D)             (None, 8, 8, 128)    16512       activation_242[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_243 (BatchN (None, 8, 8, 128)    512         conv2d_269[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_243 (Activation)     (None, 8, 8, 128)    0           batch_normalization_243[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_270 (Conv2D)             (None, 8, 8, 128)    147584      activation_243[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_244 (BatchN (None, 8, 8, 128)    512         conv2d_270[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_244 (Activation)     (None, 8, 8, 128)    0           batch_normalization_244[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_272 (Conv2D)             (None, 8, 8, 256)    33024       add_77[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_271 (Conv2D)             (None, 8, 8, 256)    33024       activation_244[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_78 (Add)                    (None, 8, 8, 256)    0           conv2d_272[0][0]                 \n",
            "                                                                 conv2d_271[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_245 (BatchN (None, 8, 8, 256)    1024        add_78[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_245 (Activation)     (None, 8, 8, 256)    0           batch_normalization_245[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_273 (Conv2D)             (None, 8, 8, 128)    32896       activation_245[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_246 (BatchN (None, 8, 8, 128)    512         conv2d_273[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_246 (Activation)     (None, 8, 8, 128)    0           batch_normalization_246[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_274 (Conv2D)             (None, 8, 8, 128)    147584      activation_246[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_247 (BatchN (None, 8, 8, 128)    512         conv2d_274[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_247 (Activation)     (None, 8, 8, 128)    0           batch_normalization_247[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_275 (Conv2D)             (None, 8, 8, 256)    33024       activation_247[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_79 (Add)                    (None, 8, 8, 256)    0           add_78[0][0]                     \n",
            "                                                                 conv2d_275[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_248 (BatchN (None, 8, 8, 256)    1024        add_79[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_248 (Activation)     (None, 8, 8, 256)    0           batch_normalization_248[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_276 (Conv2D)             (None, 8, 8, 128)    32896       activation_248[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_249 (BatchN (None, 8, 8, 128)    512         conv2d_276[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_249 (Activation)     (None, 8, 8, 128)    0           batch_normalization_249[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_277 (Conv2D)             (None, 8, 8, 128)    147584      activation_249[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_250 (BatchN (None, 8, 8, 128)    512         conv2d_277[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_250 (Activation)     (None, 8, 8, 128)    0           batch_normalization_250[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_278 (Conv2D)             (None, 8, 8, 256)    33024       activation_250[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_80 (Add)                    (None, 8, 8, 256)    0           add_79[0][0]                     \n",
            "                                                                 conv2d_278[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_251 (BatchN (None, 8, 8, 256)    1024        add_80[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_251 (Activation)     (None, 8, 8, 256)    0           batch_normalization_251[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 1, 1, 256)    0           activation_251[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_8 (Flatten)             (None, 256)          0           average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 10)           2570        flatten_8[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "WARNING:tensorflow:9 out of the last 11 calls to <function angles_to_projective_transforms at 0x7f7594936048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 12 calls to <function rotate at 0x7f7594936268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 11 calls to <function angles_to_projective_transforms at 0x7f7594936048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 13 calls to <function rotate at 0x7f7594936268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function rotate at 0x7f7594936268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function angles_to_projective_transforms at 0x7f7594936048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 12 calls to <function rotate at 0x7f7594936268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "98/98 [==============================] - 162s 2s/step - loss: 2.4778 - accuracy: 0.3223\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 155s 2s/step - loss: 2.0457 - accuracy: 0.4680\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 155s 2s/step - loss: 1.7306 - accuracy: 0.5814\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 155s 2s/step - loss: 1.4835 - accuracy: 0.6596\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 156s 2s/step - loss: 1.3450 - accuracy: 0.6989\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 155s 2s/step - loss: 1.1753 - accuracy: 0.7460\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 156s 2s/step - loss: 1.0068 - accuracy: 0.7959\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.8994 - accuracy: 0.8249\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.8069 - accuracy: 0.8498\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.7386 - accuracy: 0.8679\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.6763 - accuracy: 0.8849\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.6295 - accuracy: 0.8999\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.5784 - accuracy: 0.9147\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.5322 - accuracy: 0.9299\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4952 - accuracy: 0.9400\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.4564 - accuracy: 0.9521\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.4240 - accuracy: 0.9619\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.3827 - accuracy: 0.9746\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.3426 - accuracy: 0.9858\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.3181 - accuracy: 0.9915\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.2917 - accuracy: 0.9973\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.2764 - accuracy: 0.9997\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.2687 - accuracy: 1.0000\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.2649 - accuracy: 1.0000\n",
            "20/20 [==============================] - 3s 140ms/step - loss: 1.0829 - accuracy: 0.8349\n",
            "val accuracy score at the end of training model type  10 [1.0829016476869584, 0.8348633]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  11\n",
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_279 (Conv2D)             (None, 32, 32, 16)   448         input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_252 (BatchN (None, 32, 32, 16)   64          conv2d_279[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_252 (Activation)     (None, 32, 32, 16)   0           batch_normalization_252[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_280 (Conv2D)             (None, 32, 32, 16)   272         activation_252[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_253 (BatchN (None, 32, 32, 16)   64          conv2d_280[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_253 (Activation)     (None, 32, 32, 16)   0           batch_normalization_253[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_281 (Conv2D)             (None, 32, 32, 16)   2320        activation_253[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_254 (BatchN (None, 32, 32, 16)   64          conv2d_281[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_254 (Activation)     (None, 32, 32, 16)   0           batch_normalization_254[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_283 (Conv2D)             (None, 32, 32, 64)   1088        activation_252[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_282 (Conv2D)             (None, 32, 32, 64)   1088        activation_254[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_81 (Add)                    (None, 32, 32, 64)   0           conv2d_283[0][0]                 \n",
            "                                                                 conv2d_282[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_255 (BatchN (None, 32, 32, 64)   256         add_81[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_255 (Activation)     (None, 32, 32, 64)   0           batch_normalization_255[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_284 (Conv2D)             (None, 32, 32, 16)   1040        activation_255[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_256 (BatchN (None, 32, 32, 16)   64          conv2d_284[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_256 (Activation)     (None, 32, 32, 16)   0           batch_normalization_256[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_285 (Conv2D)             (None, 32, 32, 16)   2320        activation_256[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_257 (BatchN (None, 32, 32, 16)   64          conv2d_285[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_257 (Activation)     (None, 32, 32, 16)   0           batch_normalization_257[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_286 (Conv2D)             (None, 32, 32, 64)   1088        activation_257[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_82 (Add)                    (None, 32, 32, 64)   0           add_81[0][0]                     \n",
            "                                                                 conv2d_286[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_258 (BatchN (None, 32, 32, 64)   256         add_82[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_258 (Activation)     (None, 32, 32, 64)   0           batch_normalization_258[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_287 (Conv2D)             (None, 32, 32, 16)   1040        activation_258[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_259 (BatchN (None, 32, 32, 16)   64          conv2d_287[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_259 (Activation)     (None, 32, 32, 16)   0           batch_normalization_259[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_288 (Conv2D)             (None, 32, 32, 16)   2320        activation_259[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_260 (BatchN (None, 32, 32, 16)   64          conv2d_288[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_260 (Activation)     (None, 32, 32, 16)   0           batch_normalization_260[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_289 (Conv2D)             (None, 32, 32, 64)   1088        activation_260[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_83 (Add)                    (None, 32, 32, 64)   0           add_82[0][0]                     \n",
            "                                                                 conv2d_289[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_261 (BatchN (None, 32, 32, 64)   256         add_83[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_261 (Activation)     (None, 32, 32, 64)   0           batch_normalization_261[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_290 (Conv2D)             (None, 16, 16, 64)   4160        activation_261[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_21 (Lambda)              (None, 16, 16, 64)   0           conv2d_290[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_262 (BatchN (None, 16, 16, 64)   256         lambda_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_262 (Activation)     (None, 16, 16, 64)   0           batch_normalization_262[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_291 (Conv2D)             (None, 16, 16, 64)   36928       activation_262[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_263 (BatchN (None, 16, 16, 64)   256         conv2d_291[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_263 (Activation)     (None, 16, 16, 64)   0           batch_normalization_263[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_293 (Conv2D)             (None, 16, 16, 128)  8320        add_83[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_292 (Conv2D)             (None, 16, 16, 128)  8320        activation_263[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_84 (Add)                    (None, 16, 16, 128)  0           conv2d_293[0][0]                 \n",
            "                                                                 conv2d_292[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_264 (BatchN (None, 16, 16, 128)  512         add_84[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_264 (Activation)     (None, 16, 16, 128)  0           batch_normalization_264[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_294 (Conv2D)             (None, 16, 16, 64)   8256        activation_264[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_22 (Lambda)              (None, 16, 16, 64)   0           conv2d_294[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_265 (BatchN (None, 16, 16, 64)   256         lambda_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_265 (Activation)     (None, 16, 16, 64)   0           batch_normalization_265[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_295 (Conv2D)             (None, 16, 16, 64)   36928       activation_265[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_266 (BatchN (None, 16, 16, 64)   256         conv2d_295[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_266 (Activation)     (None, 16, 16, 64)   0           batch_normalization_266[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_296 (Conv2D)             (None, 16, 16, 128)  8320        activation_266[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_85 (Add)                    (None, 16, 16, 128)  0           add_84[0][0]                     \n",
            "                                                                 conv2d_296[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_267 (BatchN (None, 16, 16, 128)  512         add_85[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_267 (Activation)     (None, 16, 16, 128)  0           batch_normalization_267[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_297 (Conv2D)             (None, 16, 16, 64)   8256        activation_267[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_23 (Lambda)              (None, 16, 16, 64)   0           conv2d_297[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_268 (BatchN (None, 16, 16, 64)   256         lambda_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_268 (Activation)     (None, 16, 16, 64)   0           batch_normalization_268[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_298 (Conv2D)             (None, 16, 16, 64)   36928       activation_268[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_269 (BatchN (None, 16, 16, 64)   256         conv2d_298[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_269 (Activation)     (None, 16, 16, 64)   0           batch_normalization_269[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_299 (Conv2D)             (None, 16, 16, 128)  8320        activation_269[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_86 (Add)                    (None, 16, 16, 128)  0           add_85[0][0]                     \n",
            "                                                                 conv2d_299[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_270 (BatchN (None, 16, 16, 128)  512         add_86[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_270 (Activation)     (None, 16, 16, 128)  0           batch_normalization_270[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_300 (Conv2D)             (None, 8, 8, 128)    16512       activation_270[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_271 (BatchN (None, 8, 8, 128)    512         conv2d_300[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_271 (Activation)     (None, 8, 8, 128)    0           batch_normalization_271[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_301 (Conv2D)             (None, 8, 8, 128)    147584      activation_271[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_272 (BatchN (None, 8, 8, 128)    512         conv2d_301[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_272 (Activation)     (None, 8, 8, 128)    0           batch_normalization_272[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_303 (Conv2D)             (None, 8, 8, 256)    33024       add_86[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_302 (Conv2D)             (None, 8, 8, 256)    33024       activation_272[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_87 (Add)                    (None, 8, 8, 256)    0           conv2d_303[0][0]                 \n",
            "                                                                 conv2d_302[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_273 (BatchN (None, 8, 8, 256)    1024        add_87[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_273 (Activation)     (None, 8, 8, 256)    0           batch_normalization_273[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_304 (Conv2D)             (None, 8, 8, 128)    32896       activation_273[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_274 (BatchN (None, 8, 8, 128)    512         conv2d_304[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_274 (Activation)     (None, 8, 8, 128)    0           batch_normalization_274[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_305 (Conv2D)             (None, 8, 8, 128)    147584      activation_274[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_275 (BatchN (None, 8, 8, 128)    512         conv2d_305[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_275 (Activation)     (None, 8, 8, 128)    0           batch_normalization_275[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_306 (Conv2D)             (None, 8, 8, 256)    33024       activation_275[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_88 (Add)                    (None, 8, 8, 256)    0           add_87[0][0]                     \n",
            "                                                                 conv2d_306[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_276 (BatchN (None, 8, 8, 256)    1024        add_88[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_276 (Activation)     (None, 8, 8, 256)    0           batch_normalization_276[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_307 (Conv2D)             (None, 8, 8, 128)    32896       activation_276[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_277 (BatchN (None, 8, 8, 128)    512         conv2d_307[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_277 (Activation)     (None, 8, 8, 128)    0           batch_normalization_277[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_308 (Conv2D)             (None, 8, 8, 128)    147584      activation_277[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_278 (BatchN (None, 8, 8, 128)    512         conv2d_308[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_278 (Activation)     (None, 8, 8, 128)    0           batch_normalization_278[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_309 (Conv2D)             (None, 8, 8, 256)    33024       activation_278[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_89 (Add)                    (None, 8, 8, 256)    0           add_88[0][0]                     \n",
            "                                                                 conv2d_309[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_279 (BatchN (None, 8, 8, 256)    1024        add_89[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_279 (Activation)     (None, 8, 8, 256)    0           batch_normalization_279[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_9 (AveragePoo (None, 1, 1, 256)    0           activation_279[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_9 (Flatten)             (None, 256)          0           average_pooling2d_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 10)           2570        flatten_9[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 183s 2s/step - loss: 2.4565 - accuracy: 0.3258\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 176s 2s/step - loss: 2.0532 - accuracy: 0.4675\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 175s 2s/step - loss: 1.7964 - accuracy: 0.5564\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 174s 2s/step - loss: 1.6175 - accuracy: 0.6103\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 172s 2s/step - loss: 1.3889 - accuracy: 0.6812\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 171s 2s/step - loss: 1.2296 - accuracy: 0.7237\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 170s 2s/step - loss: 1.0630 - accuracy: 0.7722\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 170s 2s/step - loss: 0.9381 - accuracy: 0.8075\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 170s 2s/step - loss: 0.8440 - accuracy: 0.8309\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 170s 2s/step - loss: 0.7763 - accuracy: 0.8486\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 170s 2s/step - loss: 0.7101 - accuracy: 0.8678\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 169s 2s/step - loss: 0.6586 - accuracy: 0.8818\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 170s 2s/step - loss: 0.6103 - accuracy: 0.8971\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 170s 2s/step - loss: 0.5642 - accuracy: 0.9088\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 170s 2s/step - loss: 0.5221 - accuracy: 0.9225\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 169s 2s/step - loss: 0.4857 - accuracy: 0.9332\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 172s 2s/step - loss: 0.4452 - accuracy: 0.9470\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 170s 2s/step - loss: 0.4029 - accuracy: 0.9591\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 170s 2s/step - loss: 0.3689 - accuracy: 0.9699\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 169s 2s/step - loss: 0.3374 - accuracy: 0.9794\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 171s 2s/step - loss: 0.3032 - accuracy: 0.9891\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 171s 2s/step - loss: 0.2805 - accuracy: 0.9951\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 171s 2s/step - loss: 0.2669 - accuracy: 0.9979\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 172s 2s/step - loss: 0.2596 - accuracy: 0.9991\n",
            "20/20 [==============================] - 22s 1s/step - loss: 0.9521 - accuracy: 0.8414\n",
            "val accuracy score at the end of training model type  11 [0.9521063387393951, 0.8414062]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  12\n",
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_11 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_310 (Conv2D)             (None, 32, 32, 16)   448         input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_280 (BatchN (None, 32, 32, 16)   64          conv2d_310[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_280 (Activation)     (None, 32, 32, 16)   0           batch_normalization_280[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_311 (Conv2D)             (None, 32, 32, 16)   272         activation_280[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_281 (BatchN (None, 32, 32, 16)   64          conv2d_311[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_281 (Activation)     (None, 32, 32, 16)   0           batch_normalization_281[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_312 (Conv2D)             (None, 32, 32, 16)   2320        activation_281[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_282 (BatchN (None, 32, 32, 16)   64          conv2d_312[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_282 (Activation)     (None, 32, 32, 16)   0           batch_normalization_282[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_314 (Conv2D)             (None, 32, 32, 64)   1088        activation_280[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_313 (Conv2D)             (None, 32, 32, 64)   1088        activation_282[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_90 (Add)                    (None, 32, 32, 64)   0           conv2d_314[0][0]                 \n",
            "                                                                 conv2d_313[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_283 (BatchN (None, 32, 32, 64)   256         add_90[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_283 (Activation)     (None, 32, 32, 64)   0           batch_normalization_283[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_315 (Conv2D)             (None, 32, 32, 16)   1040        activation_283[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_284 (BatchN (None, 32, 32, 16)   64          conv2d_315[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_284 (Activation)     (None, 32, 32, 16)   0           batch_normalization_284[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_316 (Conv2D)             (None, 32, 32, 16)   2320        activation_284[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_285 (BatchN (None, 32, 32, 16)   64          conv2d_316[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_285 (Activation)     (None, 32, 32, 16)   0           batch_normalization_285[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_317 (Conv2D)             (None, 32, 32, 64)   1088        activation_285[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_91 (Add)                    (None, 32, 32, 64)   0           add_90[0][0]                     \n",
            "                                                                 conv2d_317[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_286 (BatchN (None, 32, 32, 64)   256         add_91[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_286 (Activation)     (None, 32, 32, 64)   0           batch_normalization_286[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_318 (Conv2D)             (None, 32, 32, 16)   1040        activation_286[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_287 (BatchN (None, 32, 32, 16)   64          conv2d_318[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_287 (Activation)     (None, 32, 32, 16)   0           batch_normalization_287[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_319 (Conv2D)             (None, 32, 32, 16)   2320        activation_287[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_288 (BatchN (None, 32, 32, 16)   64          conv2d_319[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_288 (Activation)     (None, 32, 32, 16)   0           batch_normalization_288[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_320 (Conv2D)             (None, 32, 32, 64)   1088        activation_288[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_92 (Add)                    (None, 32, 32, 64)   0           add_91[0][0]                     \n",
            "                                                                 conv2d_320[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_289 (BatchN (None, 32, 32, 64)   256         add_92[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_289 (Activation)     (None, 32, 32, 64)   0           batch_normalization_289[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_321 (Conv2D)             (None, 16, 16, 64)   4160        activation_289[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_24 (Lambda)              (None, 16, 16, 64)   0           conv2d_321[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_290 (BatchN (None, 16, 16, 64)   256         lambda_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_290 (Activation)     (None, 16, 16, 64)   0           batch_normalization_290[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_322 (Conv2D)             (None, 16, 16, 64)   36928       activation_290[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_291 (BatchN (None, 16, 16, 64)   256         conv2d_322[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_291 (Activation)     (None, 16, 16, 64)   0           batch_normalization_291[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_324 (Conv2D)             (None, 16, 16, 128)  8320        add_92[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_323 (Conv2D)             (None, 16, 16, 128)  8320        activation_291[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_93 (Add)                    (None, 16, 16, 128)  0           conv2d_324[0][0]                 \n",
            "                                                                 conv2d_323[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_292 (BatchN (None, 16, 16, 128)  512         add_93[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_292 (Activation)     (None, 16, 16, 128)  0           batch_normalization_292[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_325 (Conv2D)             (None, 16, 16, 64)   8256        activation_292[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_25 (Lambda)              (None, 16, 16, 64)   0           conv2d_325[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_293 (BatchN (None, 16, 16, 64)   256         lambda_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_293 (Activation)     (None, 16, 16, 64)   0           batch_normalization_293[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_326 (Conv2D)             (None, 16, 16, 64)   36928       activation_293[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_294 (BatchN (None, 16, 16, 64)   256         conv2d_326[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_294 (Activation)     (None, 16, 16, 64)   0           batch_normalization_294[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_327 (Conv2D)             (None, 16, 16, 128)  8320        activation_294[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_94 (Add)                    (None, 16, 16, 128)  0           add_93[0][0]                     \n",
            "                                                                 conv2d_327[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_295 (BatchN (None, 16, 16, 128)  512         add_94[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_295 (Activation)     (None, 16, 16, 128)  0           batch_normalization_295[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_328 (Conv2D)             (None, 16, 16, 64)   8256        activation_295[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_26 (Lambda)              (None, 16, 16, 64)   0           conv2d_328[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_296 (BatchN (None, 16, 16, 64)   256         lambda_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_296 (Activation)     (None, 16, 16, 64)   0           batch_normalization_296[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_329 (Conv2D)             (None, 16, 16, 64)   36928       activation_296[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_297 (BatchN (None, 16, 16, 64)   256         conv2d_329[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_297 (Activation)     (None, 16, 16, 64)   0           batch_normalization_297[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_330 (Conv2D)             (None, 16, 16, 128)  8320        activation_297[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_95 (Add)                    (None, 16, 16, 128)  0           add_94[0][0]                     \n",
            "                                                                 conv2d_330[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_298 (BatchN (None, 16, 16, 128)  512         add_95[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_298 (Activation)     (None, 16, 16, 128)  0           batch_normalization_298[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_331 (Conv2D)             (None, 8, 8, 128)    16512       activation_298[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_299 (BatchN (None, 8, 8, 128)    512         conv2d_331[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_299 (Activation)     (None, 8, 8, 128)    0           batch_normalization_299[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_332 (Conv2D)             (None, 8, 8, 128)    147584      activation_299[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_300 (BatchN (None, 8, 8, 128)    512         conv2d_332[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_300 (Activation)     (None, 8, 8, 128)    0           batch_normalization_300[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_334 (Conv2D)             (None, 8, 8, 256)    33024       add_95[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_333 (Conv2D)             (None, 8, 8, 256)    33024       activation_300[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_96 (Add)                    (None, 8, 8, 256)    0           conv2d_334[0][0]                 \n",
            "                                                                 conv2d_333[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_301 (BatchN (None, 8, 8, 256)    1024        add_96[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_301 (Activation)     (None, 8, 8, 256)    0           batch_normalization_301[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_335 (Conv2D)             (None, 8, 8, 128)    32896       activation_301[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_302 (BatchN (None, 8, 8, 128)    512         conv2d_335[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_302 (Activation)     (None, 8, 8, 128)    0           batch_normalization_302[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_336 (Conv2D)             (None, 8, 8, 128)    147584      activation_302[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_303 (BatchN (None, 8, 8, 128)    512         conv2d_336[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_303 (Activation)     (None, 8, 8, 128)    0           batch_normalization_303[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_337 (Conv2D)             (None, 8, 8, 256)    33024       activation_303[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_97 (Add)                    (None, 8, 8, 256)    0           add_96[0][0]                     \n",
            "                                                                 conv2d_337[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_304 (BatchN (None, 8, 8, 256)    1024        add_97[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_304 (Activation)     (None, 8, 8, 256)    0           batch_normalization_304[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_338 (Conv2D)             (None, 8, 8, 128)    32896       activation_304[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_305 (BatchN (None, 8, 8, 128)    512         conv2d_338[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_305 (Activation)     (None, 8, 8, 128)    0           batch_normalization_305[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_339 (Conv2D)             (None, 8, 8, 128)    147584      activation_305[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_306 (BatchN (None, 8, 8, 128)    512         conv2d_339[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_306 (Activation)     (None, 8, 8, 128)    0           batch_normalization_306[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_340 (Conv2D)             (None, 8, 8, 256)    33024       activation_306[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_98 (Add)                    (None, 8, 8, 256)    0           add_97[0][0]                     \n",
            "                                                                 conv2d_340[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_307 (BatchN (None, 8, 8, 256)    1024        add_98[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_307 (Activation)     (None, 8, 8, 256)    0           batch_normalization_307[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_10 (AveragePo (None, 1, 1, 256)    0           activation_307[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_10 (Flatten)            (None, 256)          0           average_pooling2d_10[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 10)           2570        flatten_10[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 116s 1s/step - loss: 2.4638 - accuracy: 0.3245\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 109s 1s/step - loss: 2.0466 - accuracy: 0.4692\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 109s 1s/step - loss: 1.7401 - accuracy: 0.5798\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 109s 1s/step - loss: 1.5006 - accuracy: 0.6547\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 109s 1s/step - loss: 1.3579 - accuracy: 0.6938\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 109s 1s/step - loss: 1.2295 - accuracy: 0.7219\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 109s 1s/step - loss: 1.0885 - accuracy: 0.7616\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 109s 1s/step - loss: 0.9813 - accuracy: 0.7890\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "52/98 [==============>...............] - ETA: 50s - loss: 0.9133 - accuracy: 0.8078"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1879fa70a1a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   model.fit(train_ds,epochs=EPOCHS, steps_per_epoch=np.ceil(50000/batch_size), \n\u001b[1;32m     17\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr_sched\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m           verbose=1)\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 535\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 535\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m   \u001b[0mconstant_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    790\u001b[0m   \"\"\"\n\u001b[1;32m    791\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrepresentable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m     \"\"\"\n\u001b[0;32m--> 933\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjXQpXdDyrHt",
        "colab_type": "code",
        "outputId": "f2b7ba2c-81b7-4e15-f361-8ab3fa61a012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for model_params in [12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]:\n",
        "  is_training=True\n",
        "  print(\"Model Param:- \",model_params)\n",
        "  model = resnet_v2(input_shape=(32,32,3), depth=depth, distortion = model_params)\n",
        "  model.summary()\n",
        "  #global_step = tf.train.get_or_create_global_step()\n",
        "  #model=model=build_model(model_params)\n",
        "  opt=SGD(lr=0.025,momentum=0.9,nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,metrics=['accuracy']\n",
        "              )\n",
        "  \n",
        "  \n",
        "  #if model_params in [0,4,5]:  \n",
        "  train_ds=train_ds2  \n",
        "  model.fit(train_ds,epochs=EPOCHS, steps_per_epoch=np.ceil(50000/batch_size), \n",
        "          callbacks=[lr_sched],\n",
        "          verbose=1)\n",
        "  is_training=False\n",
        "  score=model.evaluate(test_ds, steps =np.ceil(10000/batch_size), verbose=1)\n",
        "\n",
        "  del(model)\n",
        "  del(train_ds)\n",
        "  \n",
        "  print('val accuracy score at the end of training model type ',model_params, score)\n",
        "  print(\"=========================================\\n\")\n",
        "\n",
        "#validation_data=test_ds, validation_steps=np.ceil(10000/batch_size),"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Param:-  12\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   272         activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 64)   1088        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 64)   1088        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 64)   0           conv2d_4[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 64)   256         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   1040        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 64)   1088        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 64)   0           add[0][0]                        \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 16)   1040        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 16)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2320        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 16)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 64)   1088        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 64)   0           add_1[0][0]                      \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 64)   4160        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 16, 16, 64)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 64)   36928       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 128)  8320        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 128)  8320        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 128)  0           conv2d_14[0][0]                  \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 128)  512         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 64)   8256        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 16, 16, 64)   0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 64)   256         lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 64)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 64)   36928       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 128)  8320        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 128)  0           add_3[0][0]                      \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 128)  512         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 128)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 64)   8256        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 16, 16, 64)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 64)   256         lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 64)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 64)   36928       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 64)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 128)  8320        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 128)  0           add_4[0][0]                      \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 128)  512         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 128)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 128)    16512       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 128)    512         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 128)    0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 128)    147584      activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 8, 128)    512         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 8, 8, 128)    0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 8, 8, 256)    33024       add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 8, 8, 256)    33024       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 8, 8, 256)    0           conv2d_24[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 8, 256)    1024        add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 8, 8, 256)    0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 8, 8, 128)    32896       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 8, 8, 128)    512         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 8, 8, 128)    0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 8, 8, 128)    147584      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 8, 8, 128)    512         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 8, 8, 128)    0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 8, 8, 256)    33024       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 256)    0           add_6[0][0]                      \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 8, 8, 256)    1024        add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 8, 8, 256)    0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 128)    32896       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 8, 8, 128)    512         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 8, 8, 128)    0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 128)    147584      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 128)    512         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 8, 128)    0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 256)    33024       activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 256)    0           add_7[0][0]                      \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 256)    1024        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 256)    0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 1, 1, 256)    0           activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 256)          0           average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           2570        flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 211s 2s/step - loss: 2.4730 - accuracy: 0.3236\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 199s 2s/step - loss: 2.0787 - accuracy: 0.4533\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 200s 2s/step - loss: 1.7888 - accuracy: 0.5556\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 199s 2s/step - loss: 1.5400 - accuracy: 0.6371\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 202s 2s/step - loss: 1.3630 - accuracy: 0.6892\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 202s 2s/step - loss: 1.2259 - accuracy: 0.7259\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 198s 2s/step - loss: 1.0806 - accuracy: 0.7636\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 203s 2s/step - loss: 0.9704 - accuracy: 0.7925\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 201s 2s/step - loss: 0.8851 - accuracy: 0.8153\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 197s 2s/step - loss: 0.8173 - accuracy: 0.8307\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 198s 2s/step - loss: 0.7563 - accuracy: 0.8451\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 200s 2s/step - loss: 0.7020 - accuracy: 0.8614\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 200s 2s/step - loss: 0.6550 - accuracy: 0.8731\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 202s 2s/step - loss: 0.6141 - accuracy: 0.8835\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 200s 2s/step - loss: 0.5677 - accuracy: 0.8991\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 202s 2s/step - loss: 0.5325 - accuracy: 0.9076\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 202s 2s/step - loss: 0.4906 - accuracy: 0.9211\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 201s 2s/step - loss: 0.4540 - accuracy: 0.9322\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 200s 2s/step - loss: 0.4179 - accuracy: 0.9428\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 195s 2s/step - loss: 0.3716 - accuracy: 0.9598\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 196s 2s/step - loss: 0.3329 - accuracy: 0.9727\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 202s 2s/step - loss: 0.2951 - accuracy: 0.9849\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 201s 2s/step - loss: 0.2698 - accuracy: 0.9921\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 202s 2s/step - loss: 0.2548 - accuracy: 0.9963\n",
            "20/20 [==============================] - 4s 189ms/step - loss: 0.8226 - accuracy: 0.8514\n",
            "val accuracy score at the end of training model type  12 [0.8225763529539108, 0.8513672]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  13\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 32, 32, 16)   448         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 32, 32, 16)   64          conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 32, 32, 16)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 32, 32, 16)   272         activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 32, 32, 16)   64          conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 32, 32, 16)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 32, 32, 16)   2320        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 32, 32, 16)   64          conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 32, 32, 16)   0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 32, 32, 64)   1088        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 32, 32, 64)   1088        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 32, 32, 64)   0           conv2d_35[0][0]                  \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 32, 32, 64)   256         add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 32, 32, 64)   0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 32, 32, 16)   1040        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 32, 32, 16)   64          conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 32, 32, 16)   0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 32, 32, 16)   2320        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 32, 32, 16)   64          conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 32, 32, 16)   0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 32, 32, 64)   1088        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 32, 32, 64)   0           add_9[0][0]                      \n",
            "                                                                 conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 32, 32, 64)   256         add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 32, 32, 64)   0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 32, 32, 16)   1040        activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 32, 32, 16)   64          conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 32, 32, 16)   0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 32, 32, 16)   2320        activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 32, 32, 16)   64          conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 32, 32, 16)   0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 32, 32, 64)   1088        activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 32, 32, 64)   0           add_10[0][0]                     \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 32, 32, 64)   256         add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 32, 32, 64)   0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 16, 16, 64)   4160        activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 16, 16, 64)   256         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 16, 16, 64)   0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 16, 16, 64)   36928       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 16, 16, 64)   256         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 16, 16, 64)   0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 16, 16, 128)  8320        add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 16, 16, 128)  8320        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 16, 16, 128)  0           conv2d_45[0][0]                  \n",
            "                                                                 conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 16, 16, 128)  512         add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 16, 16, 128)  0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 16, 16, 64)   8256        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 16, 16, 64)   256         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 16, 16, 64)   0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 16, 16, 64)   36928       activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 16, 16, 64)   256         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 16, 16, 64)   0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 16, 16, 128)  8320        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 16, 16, 128)  0           add_12[0][0]                     \n",
            "                                                                 conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 16, 16, 128)  512         add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 16, 16, 128)  0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 16, 16, 64)   8256        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 16, 16, 64)   256         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 16, 16, 64)   0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 16, 16, 64)   36928       activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 16, 16, 64)   256         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 16, 16, 64)   0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 16, 16, 128)  8320        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 16, 16, 128)  0           add_13[0][0]                     \n",
            "                                                                 conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 16, 16, 128)  512         add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 16, 16, 128)  0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 8, 8, 128)    16512       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 8, 8, 128)    0           conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 8, 8, 128)    512         lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 8, 8, 128)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 8, 8, 128)    147584      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 8, 8, 128)    512         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 8, 8, 128)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 8, 8, 256)    33024       add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 8, 8, 256)    33024       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 8, 8, 256)    0           conv2d_55[0][0]                  \n",
            "                                                                 conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 8, 8, 256)    1024        add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 8, 8, 256)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 8, 8, 128)    32896       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 8, 8, 128)    0           conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 8, 8, 128)    512         lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 8, 8, 128)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 8, 8, 128)    147584      activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 8, 8, 128)    512         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 8, 8, 128)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 8, 8, 256)    33024       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 8, 8, 256)    0           add_15[0][0]                     \n",
            "                                                                 conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 8, 8, 256)    1024        add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 8, 8, 256)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 8, 8, 128)    32896       activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 8, 8, 128)    0           conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 8, 8, 128)    512         lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 8, 8, 128)    0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 8, 8, 128)    147584      activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 8, 8, 128)    512         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 8, 8, 128)    0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 8, 8, 256)    33024       activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 8, 8, 256)    0           add_16[0][0]                     \n",
            "                                                                 conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 8, 8, 256)    1024        add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 8, 8, 256)    0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 256)    0           activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 256)          0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           2570        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 232s 2s/step - loss: 2.4562 - accuracy: 0.3194\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 228s 2s/step - loss: 2.0513 - accuracy: 0.4655\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 225s 2s/step - loss: 1.7740 - accuracy: 0.5622\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 223s 2s/step - loss: 1.5475 - accuracy: 0.6347\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 223s 2s/step - loss: 1.3641 - accuracy: 0.6870\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 226s 2s/step - loss: 1.2315 - accuracy: 0.7221\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 226s 2s/step - loss: 1.0635 - accuracy: 0.7705\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 224s 2s/step - loss: 0.9554 - accuracy: 0.7984\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 223s 2s/step - loss: 0.8717 - accuracy: 0.8211\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 224s 2s/step - loss: 0.8012 - accuracy: 0.8401\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 226s 2s/step - loss: 0.7426 - accuracy: 0.8543\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 225s 2s/step - loss: 0.6915 - accuracy: 0.8687\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 227s 2s/step - loss: 0.6462 - accuracy: 0.8822\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 226s 2s/step - loss: 0.6002 - accuracy: 0.8939\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 229s 2s/step - loss: 0.5653 - accuracy: 0.9039\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 224s 2s/step - loss: 0.5170 - accuracy: 0.9193\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 224s 2s/step - loss: 0.4770 - accuracy: 0.9314\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 226s 2s/step - loss: 0.4372 - accuracy: 0.9440\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 227s 2s/step - loss: 0.3938 - accuracy: 0.9581\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 226s 2s/step - loss: 0.3625 - accuracy: 0.9681\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 223s 2s/step - loss: 0.3208 - accuracy: 0.9812\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 226s 2s/step - loss: 0.2890 - accuracy: 0.9906\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 226s 2s/step - loss: 0.2681 - accuracy: 0.9963\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 225s 2s/step - loss: 0.2563 - accuracy: 0.9992\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 0.9573 - accuracy: 0.8336\n",
            "val accuracy score at the end of training model type  13 [0.9572968870401383, 0.8335937]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  14\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 32, 32, 16)   448         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 32, 32, 16)   64          conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 32, 32, 16)   0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 32, 32, 16)   272         activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 32, 32, 16)   64          conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 32, 32, 16)   0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 32, 32, 16)   2320        activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 32, 32, 16)   64          conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 32, 32, 16)   0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 32, 32, 64)   1088        activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 32, 32, 64)   1088        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 32, 32, 64)   0           conv2d_66[0][0]                  \n",
            "                                                                 conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 32, 32, 64)   256         add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 32, 32, 64)   0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 32, 32, 16)   1040        activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 32, 32, 16)   64          conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 32, 32, 16)   0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 32, 32, 16)   2320        activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 32, 32, 16)   64          conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 32, 32, 16)   0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 32, 32, 64)   1088        activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 32, 32, 64)   0           add_18[0][0]                     \n",
            "                                                                 conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 32, 32, 64)   256         add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 32, 32, 64)   0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 32, 32, 16)   1040        activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 32, 32, 16)   64          conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 32, 32, 16)   0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 32, 32, 16)   2320        activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 32, 32, 16)   64          conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 32, 32, 16)   0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 32, 32, 64)   1088        activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 32, 32, 64)   0           add_19[0][0]                     \n",
            "                                                                 conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 32, 32, 64)   256         add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 32, 32, 64)   0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 16, 16, 64)   4160        activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 16, 16, 64)   256         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 16, 16, 64)   0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 16, 16, 64)   36928       activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 16, 16, 64)   256         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 16, 16, 64)   0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 16, 16, 128)  8320        add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 16, 16, 128)  8320        activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 16, 16, 128)  0           conv2d_76[0][0]                  \n",
            "                                                                 conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 16, 16, 128)  512         add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 16, 16, 128)  0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 16, 16, 64)   8256        activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 16, 16, 64)   256         conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 16, 16, 64)   0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 16, 16, 64)   36928       activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 16, 16, 64)   256         conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 16, 16, 64)   0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 16, 16, 128)  8320        activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 16, 16, 128)  0           add_21[0][0]                     \n",
            "                                                                 conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 16, 16, 128)  512         add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 16, 16, 128)  0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 16, 16, 64)   8256        activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 16, 16, 64)   256         conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 16, 16, 64)   0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 16, 16, 64)   36928       activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 16, 16, 64)   256         conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 16, 16, 64)   0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 16, 16, 128)  8320        activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 16, 16, 128)  0           add_22[0][0]                     \n",
            "                                                                 conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 16, 16, 128)  512         add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 16, 16, 128)  0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 8, 8, 128)    16512       activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 8, 8, 128)    0           conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 8, 8, 128)    512         lambda_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 8, 8, 128)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 8, 8, 128)    147584      activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 8, 8, 128)    512         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 8, 8, 128)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 8, 8, 256)    33024       add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 8, 8, 256)    33024       activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 8, 8, 256)    0           conv2d_86[0][0]                  \n",
            "                                                                 conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 8, 8, 256)    1024        add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 8, 8, 256)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 8, 8, 128)    32896       activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 8, 8, 128)    0           conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 8, 8, 128)    512         lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 8, 8, 128)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 8, 8, 128)    147584      activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 8, 8, 128)    512         conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 8, 8, 128)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 8, 8, 256)    33024       activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 8, 8, 256)    0           add_24[0][0]                     \n",
            "                                                                 conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 8, 8, 256)    1024        add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 8, 8, 256)    0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 8, 8, 128)    32896       activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 8, 8, 128)    0           conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 8, 8, 128)    512         lambda_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 8, 8, 128)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 8, 8, 128)    147584      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 8, 8, 128)    512         conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 8, 8, 128)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 8, 8, 256)    33024       activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 8, 8, 256)    0           add_25[0][0]                     \n",
            "                                                                 conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 8, 8, 256)    1024        add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 8, 8, 256)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 1, 1, 256)    0           activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 256)          0           average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           2570        flatten_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function angles_to_projective_transforms at 0x7f26afb28a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function rotate at 0x7f26afb28c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function angles_to_projective_transforms at 0x7f26afb28a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function rotate at 0x7f26afb28c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function angles_to_projective_transforms at 0x7f26afb28a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 8 calls to <function rotate at 0x7f26afb28c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 8 calls to <function angles_to_projective_transforms at 0x7f26afb28a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 9 calls to <function rotate at 0x7f26afb28c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "98/98 [==============================] - 286s 3s/step - loss: 2.4614 - accuracy: 0.3213\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 276s 3s/step - loss: 2.0314 - accuracy: 0.4766\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 275s 3s/step - loss: 1.7371 - accuracy: 0.5775\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 275s 3s/step - loss: 1.4992 - accuracy: 0.6545\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 277s 3s/step - loss: 1.3073 - accuracy: 0.7108\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 276s 3s/step - loss: 1.1587 - accuracy: 0.7543\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 278s 3s/step - loss: 0.9973 - accuracy: 0.8006\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 277s 3s/step - loss: 0.8793 - accuracy: 0.8306\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 277s 3s/step - loss: 0.7905 - accuracy: 0.8551\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 275s 3s/step - loss: 0.7227 - accuracy: 0.8722\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 273s 3s/step - loss: 0.6654 - accuracy: 0.8878\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 276s 3s/step - loss: 0.6173 - accuracy: 0.9011\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 271s 3s/step - loss: 0.5678 - accuracy: 0.9165\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 277s 3s/step - loss: 0.5278 - accuracy: 0.9280\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 275s 3s/step - loss: 0.4843 - accuracy: 0.9425\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 280s 3s/step - loss: 0.4475 - accuracy: 0.9536\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 276s 3s/step - loss: 0.4104 - accuracy: 0.9651\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 279s 3s/step - loss: 0.3684 - accuracy: 0.9775\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 278s 3s/step - loss: 0.3432 - accuracy: 0.9833\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 274s 3s/step - loss: 0.3114 - accuracy: 0.9919\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 273s 3s/step - loss: 0.2887 - accuracy: 0.9966\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 268s 3s/step - loss: 0.2725 - accuracy: 0.9995\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 271s 3s/step - loss: 0.2640 - accuracy: 1.0000\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 270s 3s/step - loss: 0.2602 - accuracy: 1.0000\n",
            "20/20 [==============================] - 4s 190ms/step - loss: 0.9697 - accuracy: 0.8562\n",
            "val accuracy score at the end of training model type  14 [0.9696604043245316, 0.85615236]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  15\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 32, 32, 16)   448         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 32, 32, 16)   64          conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 32, 32, 16)   0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 32, 32, 16)   272         activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 32, 32, 16)   64          conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 32, 32, 16)   0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 32, 32, 16)   2320        activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 32, 32, 16)   64          conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 32, 32, 16)   0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 32, 32, 64)   1088        activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 32, 32, 64)   1088        activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 32, 32, 64)   0           conv2d_97[0][0]                  \n",
            "                                                                 conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 32, 32, 64)   256         add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 32, 32, 64)   0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 32, 32, 16)   1040        activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 32, 32, 16)   64          conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 32, 32, 16)   0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 32, 32, 16)   2320        activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 32, 32, 16)   64          conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 32, 32, 16)   0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 32, 32, 64)   1088        activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_28 (Add)                    (None, 32, 32, 64)   0           add_27[0][0]                     \n",
            "                                                                 conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 32, 32, 64)   256         add_28[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 32, 32, 64)   0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 32, 32, 16)   1040        activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 32, 32, 16)   64          conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 32, 32, 16)   0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 32, 32, 16)   2320        activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 32, 32, 16)   64          conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 32, 32, 16)   0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 32, 32, 64)   1088        activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_29 (Add)                    (None, 32, 32, 64)   0           add_28[0][0]                     \n",
            "                                                                 conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 32, 32, 64)   256         add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 32, 32, 64)   0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 16, 16, 64)   4160        activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 16, 16, 64)   256         conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 16, 16, 64)   0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 16, 16, 64)   36928       activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 16, 16, 64)   256         conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 16, 16, 64)   0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 16, 16, 128)  8320        add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 16, 16, 128)  8320        activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_30 (Add)                    (None, 16, 16, 128)  0           conv2d_107[0][0]                 \n",
            "                                                                 conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 16, 16, 128)  512         add_30[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 16, 16, 128)  0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 16, 16, 64)   8256        activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 16, 16, 64)   256         conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 16, 16, 64)   0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 16, 16, 64)   36928       activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 16, 16, 64)   256         conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 16, 16, 64)   0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 16, 16, 128)  8320        activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_31 (Add)                    (None, 16, 16, 128)  0           add_30[0][0]                     \n",
            "                                                                 conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 16, 16, 128)  512         add_31[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 16, 16, 128)  0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 16, 16, 64)   8256        activation_99[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 16, 16, 64)   256         conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 16, 16, 64)   0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 16, 16, 64)   36928       activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 16, 16, 64)   256         conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 16, 16, 64)   0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 16, 16, 128)  8320        activation_101[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_32 (Add)                    (None, 16, 16, 128)  0           add_31[0][0]                     \n",
            "                                                                 conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 16, 16, 128)  512         add_32[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 16, 16, 128)  0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 8, 8, 128)    16512       activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 8, 8, 128)    0           conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 8, 8, 128)    512         lambda_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 8, 8, 128)    0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 8, 8, 128)    147584      activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 8, 8, 128)    512         conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 8, 8, 128)    0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 8, 8, 256)    33024       add_32[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 8, 8, 256)    33024       activation_104[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_33 (Add)                    (None, 8, 8, 256)    0           conv2d_117[0][0]                 \n",
            "                                                                 conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 8, 8, 256)    1024        add_33[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 8, 8, 256)    0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 8, 8, 128)    32896       activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 8, 8, 128)    0           conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 8, 8, 128)    512         lambda_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 8, 8, 128)    0           batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 8, 8, 128)    147584      activation_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 8, 8, 128)    512         conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 8, 8, 128)    0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 8, 8, 256)    33024       activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_34 (Add)                    (None, 8, 8, 256)    0           add_33[0][0]                     \n",
            "                                                                 conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 8, 8, 256)    1024        add_34[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 8, 8, 256)    0           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 8, 8, 128)    32896       activation_108[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 8, 8, 128)    0           conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 8, 8, 128)    512         lambda_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 8, 8, 128)    0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 8, 8, 128)    147584      activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 8, 8, 128)    512         conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 8, 8, 128)    0           batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 8, 8, 256)    33024       activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_35 (Add)                    (None, 8, 8, 256)    0           add_34[0][0]                     \n",
            "                                                                 conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 8, 8, 256)    1024        add_35[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 8, 8, 256)    0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 1, 1, 256)    0           activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 256)          0           average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           2570        flatten_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 269s 3s/step - loss: 2.4702 - accuracy: 0.3234\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 266s 3s/step - loss: 2.1033 - accuracy: 0.4492\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 275s 3s/step - loss: 1.8395 - accuracy: 0.5395\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 278s 3s/step - loss: 1.6101 - accuracy: 0.6142\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 281s 3s/step - loss: 1.4320 - accuracy: 0.6650\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 280s 3s/step - loss: 1.2952 - accuracy: 0.7017\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 281s 3s/step - loss: 1.1365 - accuracy: 0.7464\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 281s 3s/step - loss: 1.0083 - accuracy: 0.7811\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 284s 3s/step - loss: 0.9151 - accuracy: 0.8070\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 280s 3s/step - loss: 0.8402 - accuracy: 0.8268\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 284s 3s/step - loss: 0.7761 - accuracy: 0.8430\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 277s 3s/step - loss: 0.7198 - accuracy: 0.8574\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 281s 3s/step - loss: 0.6701 - accuracy: 0.8717\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 268s 3s/step - loss: 0.6247 - accuracy: 0.8837\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 267s 3s/step - loss: 0.5821 - accuracy: 0.8973\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 263s 3s/step - loss: 0.5310 - accuracy: 0.9116\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.4900 - accuracy: 0.9234\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.4456 - accuracy: 0.9388\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 256s 3s/step - loss: 0.4009 - accuracy: 0.9538\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 271s 3s/step - loss: 0.3565 - accuracy: 0.9685\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 276s 3s/step - loss: 0.3095 - accuracy: 0.9843\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 273s 3s/step - loss: 0.2748 - accuracy: 0.9947\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 278s 3s/step - loss: 0.2558 - accuracy: 0.9991\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 273s 3s/step - loss: 0.2470 - accuracy: 0.9998\n",
            "20/20 [==============================] - 32s 2s/step - loss: 1.0079 - accuracy: 0.8227\n",
            "val accuracy score at the end of training model type  15 [1.0078580319881438, 0.8226563]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  16\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 32, 32, 16)   448         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 32, 32, 16)   64          conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 32, 32, 16)   0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 32, 32, 16)   272         activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 32, 32, 16)   64          conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 32, 32, 16)   0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 32, 32, 16)   2320        activation_113[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 32, 32, 16)   64          conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 32, 32, 16)   0           batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 32, 32, 64)   1088        activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 32, 32, 64)   1088        activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_36 (Add)                    (None, 32, 32, 64)   0           conv2d_128[0][0]                 \n",
            "                                                                 conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 32, 32, 64)   256         add_36[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 32, 32, 64)   0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 32, 32, 16)   1040        activation_115[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 32, 32, 16)   64          conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 32, 32, 16)   0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 32, 32, 16)   2320        activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 32, 32, 16)   64          conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 32, 32, 16)   0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 32, 32, 64)   1088        activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_37 (Add)                    (None, 32, 32, 64)   0           add_36[0][0]                     \n",
            "                                                                 conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 32, 32, 64)   256         add_37[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 32, 32, 64)   0           batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 32, 32, 16)   1040        activation_118[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 32, 32, 16)   64          conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 32, 32, 16)   0           batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 32, 32, 16)   2320        activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 32, 32, 16)   64          conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 32, 32, 16)   0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 32, 32, 64)   1088        activation_120[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_38 (Add)                    (None, 32, 32, 64)   0           add_37[0][0]                     \n",
            "                                                                 conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 32, 32, 64)   256         add_38[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 32, 32, 64)   0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 16, 16, 64)   4160        activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 16, 16, 64)   256         conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 16, 16, 64)   0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 16, 16, 64)   36928       activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 16, 16, 64)   256         conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 16, 16, 64)   0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 16, 16, 128)  8320        add_38[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 16, 16, 128)  8320        activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_39 (Add)                    (None, 16, 16, 128)  0           conv2d_138[0][0]                 \n",
            "                                                                 conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 16, 16, 128)  512         add_39[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 16, 16, 128)  0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 16, 16, 64)   8256        activation_124[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 16, 16, 64)   256         conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 16, 16, 64)   0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 16, 16, 64)   36928       activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 16, 16, 64)   256         conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 16, 16, 64)   0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 16, 16, 128)  8320        activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_40 (Add)                    (None, 16, 16, 128)  0           add_39[0][0]                     \n",
            "                                                                 conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 16, 16, 128)  512         add_40[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 16, 16, 128)  0           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 16, 16, 64)   8256        activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 16, 16, 64)   256         conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 16, 16, 64)   0           batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 16, 16, 64)   36928       activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 16, 16, 64)   256         conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 16, 16, 64)   0           batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 16, 16, 128)  8320        activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_41 (Add)                    (None, 16, 16, 128)  0           add_40[0][0]                     \n",
            "                                                                 conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 16, 16, 128)  512         add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 16, 16, 128)  0           batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, 8, 8, 128)    16512       activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_12 (Lambda)              (None, 8, 8, 128)    0           conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 8, 8, 128)    512         lambda_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 8, 8, 128)    0           batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, 8, 8, 128)    147584      activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 8, 8, 128)    512         conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 8, 8, 128)    0           batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_148 (Conv2D)             (None, 8, 8, 256)    33024       add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, 8, 8, 256)    33024       activation_132[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_42 (Add)                    (None, 8, 8, 256)    0           conv2d_148[0][0]                 \n",
            "                                                                 conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, 8, 8, 256)    1024        add_42[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 8, 8, 256)    0           batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_149 (Conv2D)             (None, 8, 8, 128)    32896       activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_13 (Lambda)              (None, 8, 8, 128)    0           conv2d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 8, 8, 128)    512         lambda_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 8, 8, 128)    0           batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_150 (Conv2D)             (None, 8, 8, 128)    147584      activation_134[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 8, 8, 128)    512         conv2d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 8, 8, 128)    0           batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_151 (Conv2D)             (None, 8, 8, 256)    33024       activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_43 (Add)                    (None, 8, 8, 256)    0           add_42[0][0]                     \n",
            "                                                                 conv2d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 8, 8, 256)    1024        add_43[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 8, 8, 256)    0           batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_152 (Conv2D)             (None, 8, 8, 128)    32896       activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_14 (Lambda)              (None, 8, 8, 128)    0           conv2d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 8, 8, 128)    512         lambda_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 8, 8, 128)    0           batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, 8, 8, 128)    147584      activation_137[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 8, 8, 128)    512         conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 8, 8, 128)    0           batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, 8, 8, 256)    33024       activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_44 (Add)                    (None, 8, 8, 256)    0           add_43[0][0]                     \n",
            "                                                                 conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, 8, 8, 256)    1024        add_44[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 8, 8, 256)    0           batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 1, 1, 256)    0           activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 256)          0           average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 10)           2570        flatten_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 210s 2s/step - loss: 2.4607 - accuracy: 0.3250\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 201s 2s/step - loss: 2.0512 - accuracy: 0.4671\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 202s 2s/step - loss: 1.7356 - accuracy: 0.5780\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 202s 2s/step - loss: 1.5090 - accuracy: 0.6515\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 205s 2s/step - loss: 1.3161 - accuracy: 0.7062\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 203s 2s/step - loss: 1.1799 - accuracy: 0.7438\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 202s 2s/step - loss: 1.0133 - accuracy: 0.7944\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 201s 2s/step - loss: 0.9055 - accuracy: 0.8185\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 201s 2s/step - loss: 0.8286 - accuracy: 0.8401\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 202s 2s/step - loss: 0.7562 - accuracy: 0.8570\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 199s 2s/step - loss: 0.6965 - accuracy: 0.8720\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 203s 2s/step - loss: 0.6537 - accuracy: 0.8832\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 203s 2s/step - loss: 0.6072 - accuracy: 0.8973\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 204s 2s/step - loss: 0.5656 - accuracy: 0.9099\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 201s 2s/step - loss: 0.5194 - accuracy: 0.9247\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 202s 2s/step - loss: 0.4870 - accuracy: 0.9314\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 203s 2s/step - loss: 0.4497 - accuracy: 0.9448\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 201s 2s/step - loss: 0.4143 - accuracy: 0.9562\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 199s 2s/step - loss: 0.3762 - accuracy: 0.9679\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 201s 2s/step - loss: 0.3384 - accuracy: 0.9794\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 203s 2s/step - loss: 0.3035 - accuracy: 0.9894\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 201s 2s/step - loss: 0.2811 - accuracy: 0.9953\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 202s 2s/step - loss: 0.2643 - accuracy: 0.9991\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 205s 2s/step - loss: 0.2578 - accuracy: 0.9997\n",
            "20/20 [==============================] - 4s 189ms/step - loss: 0.9999 - accuracy: 0.8434\n",
            "val accuracy score at the end of training model type  16 [0.9998597502708435, 0.84335935]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  17\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, 32, 32, 16)   448         input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_140 (BatchN (None, 32, 32, 16)   64          conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 32, 32, 16)   0           batch_normalization_140[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, 32, 32, 16)   272         activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_141 (BatchN (None, 32, 32, 16)   64          conv2d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 32, 32, 16)   0           batch_normalization_141[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, 32, 32, 16)   2320        activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_15 (Lambda)              (None, 32, 32, 16)   0           conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_142 (BatchN (None, 32, 32, 16)   64          lambda_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 32, 32, 16)   0           batch_normalization_142[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 32, 32, 64)   1088        activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, 32, 32, 64)   1088        activation_142[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_45 (Add)                    (None, 32, 32, 64)   0           conv2d_159[0][0]                 \n",
            "                                                                 conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_143 (BatchN (None, 32, 32, 64)   256         add_45[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 32, 32, 64)   0           batch_normalization_143[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 32, 32, 16)   1040        activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_144 (BatchN (None, 32, 32, 16)   64          conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 32, 32, 16)   0           batch_normalization_144[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 32, 32, 16)   2320        activation_144[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_16 (Lambda)              (None, 32, 32, 16)   0           conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_145 (BatchN (None, 32, 32, 16)   64          lambda_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 32, 32, 16)   0           batch_normalization_145[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 32, 32, 64)   1088        activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_46 (Add)                    (None, 32, 32, 64)   0           add_45[0][0]                     \n",
            "                                                                 conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_146 (BatchN (None, 32, 32, 64)   256         add_46[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 32, 32, 64)   0           batch_normalization_146[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 32, 32, 16)   1040        activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_147 (BatchN (None, 32, 32, 16)   64          conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 32, 32, 16)   0           batch_normalization_147[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_164 (Conv2D)             (None, 32, 32, 16)   2320        activation_147[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_17 (Lambda)              (None, 32, 32, 16)   0           conv2d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_148 (BatchN (None, 32, 32, 16)   64          lambda_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 32, 32, 16)   0           batch_normalization_148[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_165 (Conv2D)             (None, 32, 32, 64)   1088        activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_47 (Add)                    (None, 32, 32, 64)   0           add_46[0][0]                     \n",
            "                                                                 conv2d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_149 (BatchN (None, 32, 32, 64)   256         add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 32, 32, 64)   0           batch_normalization_149[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_166 (Conv2D)             (None, 16, 16, 64)   4160        activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_150 (BatchN (None, 16, 16, 64)   256         conv2d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 16, 16, 64)   0           batch_normalization_150[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_167 (Conv2D)             (None, 16, 16, 64)   36928       activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_151 (BatchN (None, 16, 16, 64)   256         conv2d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 16, 16, 64)   0           batch_normalization_151[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_169 (Conv2D)             (None, 16, 16, 128)  8320        add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_168 (Conv2D)             (None, 16, 16, 128)  8320        activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_48 (Add)                    (None, 16, 16, 128)  0           conv2d_169[0][0]                 \n",
            "                                                                 conv2d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_152 (BatchN (None, 16, 16, 128)  512         add_48[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 16, 16, 128)  0           batch_normalization_152[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, 16, 16, 64)   8256        activation_152[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_153 (BatchN (None, 16, 16, 64)   256         conv2d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 16, 16, 64)   0           batch_normalization_153[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, 16, 16, 64)   36928       activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, 16, 16, 64)   256         conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 16, 16, 64)   0           batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_172 (Conv2D)             (None, 16, 16, 128)  8320        activation_154[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_49 (Add)                    (None, 16, 16, 128)  0           add_48[0][0]                     \n",
            "                                                                 conv2d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, 16, 16, 128)  512         add_49[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 16, 16, 128)  0           batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_173 (Conv2D)             (None, 16, 16, 64)   8256        activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 16, 16, 64)   256         conv2d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 16, 16, 64)   0           batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, 16, 16, 64)   36928       activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, 16, 16, 64)   256         conv2d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 16, 16, 64)   0           batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_175 (Conv2D)             (None, 16, 16, 128)  8320        activation_157[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_50 (Add)                    (None, 16, 16, 128)  0           add_49[0][0]                     \n",
            "                                                                 conv2d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, 16, 16, 128)  512         add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 16, 16, 128)  0           batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_176 (Conv2D)             (None, 8, 8, 128)    16512       activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, 8, 8, 128)    512         conv2d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 8, 8, 128)    0           batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_177 (Conv2D)             (None, 8, 8, 128)    147584      activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, 8, 8, 128)    512         conv2d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 8, 8, 128)    0           batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, 8, 8, 256)    33024       add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, 8, 8, 256)    33024       activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_51 (Add)                    (None, 8, 8, 256)    0           conv2d_179[0][0]                 \n",
            "                                                                 conv2d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, 8, 8, 256)    1024        add_51[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 8, 8, 256)    0           batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, 8, 8, 128)    32896       activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, 8, 8, 128)    512         conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 8, 8, 128)    0           batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 8, 8, 128)    147584      activation_162[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, 8, 8, 128)    512         conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 8, 8, 128)    0           batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 8, 8, 256)    33024       activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_52 (Add)                    (None, 8, 8, 256)    0           add_51[0][0]                     \n",
            "                                                                 conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_164 (BatchN (None, 8, 8, 256)    1024        add_52[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, 8, 8, 256)    0           batch_normalization_164[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 8, 8, 128)    32896       activation_164[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, 8, 8, 128)    512         conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 8, 8, 128)    0           batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 8, 8, 128)    147584      activation_165[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, 8, 8, 128)    512         conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, 8, 8, 128)    0           batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 8, 8, 256)    33024       activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_53 (Add)                    (None, 8, 8, 256)    0           add_52[0][0]                     \n",
            "                                                                 conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 8, 8, 256)    1024        add_53[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, 8, 8, 256)    0           batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 1, 1, 256)    0           activation_167[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 256)          0           average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 10)           2570        flatten_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 241s 2s/step - loss: 2.4995 - accuracy: 0.2975\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 231s 2s/step - loss: 2.1570 - accuracy: 0.4241\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 228s 2s/step - loss: 1.8666 - accuracy: 0.5246\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 232s 2s/step - loss: 1.6334 - accuracy: 0.5995\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 232s 2s/step - loss: 1.4463 - accuracy: 0.6575\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 232s 2s/step - loss: 1.3143 - accuracy: 0.6882\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 228s 2s/step - loss: 1.1383 - accuracy: 0.7409\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 230s 2s/step - loss: 1.0088 - accuracy: 0.7774\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 227s 2s/step - loss: 0.9002 - accuracy: 0.8084\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 228s 2s/step - loss: 0.8091 - accuracy: 0.8349\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 233s 2s/step - loss: 0.7379 - accuracy: 0.8553\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 234s 2s/step - loss: 0.6718 - accuracy: 0.8748\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 229s 2s/step - loss: 0.6165 - accuracy: 0.8902\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 231s 2s/step - loss: 0.5652 - accuracy: 0.9062\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 230s 2s/step - loss: 0.5098 - accuracy: 0.9229\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 235s 2s/step - loss: 0.4695 - accuracy: 0.9376\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 234s 2s/step - loss: 0.4333 - accuracy: 0.9485\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 230s 2s/step - loss: 0.3811 - accuracy: 0.9664\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 227s 2s/step - loss: 0.3507 - accuracy: 0.9749\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 225s 2s/step - loss: 0.3096 - accuracy: 0.9876\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 226s 2s/step - loss: 0.2802 - accuracy: 0.9955\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 229s 2s/step - loss: 0.2626 - accuracy: 0.9988\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 226s 2s/step - loss: 0.2526 - accuracy: 0.9999\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 224s 2s/step - loss: 0.2483 - accuracy: 1.0000\n",
            "20/20 [==============================] - 4s 186ms/step - loss: 0.9455 - accuracy: 0.8497\n",
            "val accuracy score at the end of training model type  17 [0.945472800731659, 0.849707]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  18\n",
            "WARNING:tensorflow:9 out of the last 9 calls to <function angles_to_projective_transforms at 0x7f26afb28a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 10 calls to <function rotate at 0x7f26afb28c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:10 out of the last 10 calls to <function angles_to_projective_transforms at 0x7f26afb28a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function rotate at 0x7f26afb28c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function rotate at 0x7f26afb28c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 32, 32, 16)   448         input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 32, 32, 16)   64          conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_168 (Activation)     (None, 32, 32, 16)   0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 32, 32, 16)   272         activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 32, 32, 16)   64          conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_169 (Activation)     (None, 32, 32, 16)   0           batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 32, 32, 16)   2320        activation_169[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_18 (Lambda)              (None, 32, 32, 16)   0           conv2d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 32, 32, 16)   64          lambda_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_170 (Activation)     (None, 32, 32, 16)   0           batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 32, 32, 64)   1088        activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 32, 32, 64)   1088        activation_170[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_54 (Add)                    (None, 32, 32, 64)   0           conv2d_190[0][0]                 \n",
            "                                                                 conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 32, 32, 64)   256         add_54[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_171 (Activation)     (None, 32, 32, 64)   0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 32, 32, 16)   1040        activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 32, 32, 16)   64          conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_172 (Activation)     (None, 32, 32, 16)   0           batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 32, 32, 16)   2320        activation_172[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_19 (Lambda)              (None, 32, 32, 16)   0           conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 32, 32, 16)   64          lambda_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_173 (Activation)     (None, 32, 32, 16)   0           batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 32, 32, 64)   1088        activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_55 (Add)                    (None, 32, 32, 64)   0           add_54[0][0]                     \n",
            "                                                                 conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 32, 32, 64)   256         add_55[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_174 (Activation)     (None, 32, 32, 64)   0           batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 32, 32, 16)   1040        activation_174[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 32, 32, 16)   64          conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_175 (Activation)     (None, 32, 32, 16)   0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 32, 32, 16)   2320        activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_20 (Lambda)              (None, 32, 32, 16)   0           conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 32, 32, 16)   64          lambda_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_176 (Activation)     (None, 32, 32, 16)   0           batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 32, 32, 64)   1088        activation_176[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_56 (Add)                    (None, 32, 32, 64)   0           add_55[0][0]                     \n",
            "                                                                 conv2d_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, 32, 32, 64)   256         add_56[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_177 (Activation)     (None, 32, 32, 64)   0           batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 16, 16, 64)   4160        activation_177[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, 16, 16, 64)   256         conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_178 (Activation)     (None, 16, 16, 64)   0           batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 16, 16, 64)   36928       activation_178[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, 16, 16, 64)   256         conv2d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_179 (Activation)     (None, 16, 16, 64)   0           batch_normalization_179[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 16, 16, 128)  8320        add_56[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 16, 16, 128)  8320        activation_179[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_57 (Add)                    (None, 16, 16, 128)  0           conv2d_200[0][0]                 \n",
            "                                                                 conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, 16, 16, 128)  512         add_57[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_180 (Activation)     (None, 16, 16, 128)  0           batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 16, 16, 64)   8256        activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_181 (BatchN (None, 16, 16, 64)   256         conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_181 (Activation)     (None, 16, 16, 64)   0           batch_normalization_181[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 16, 16, 64)   36928       activation_181[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_182 (BatchN (None, 16, 16, 64)   256         conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_182 (Activation)     (None, 16, 16, 64)   0           batch_normalization_182[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 16, 16, 128)  8320        activation_182[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_58 (Add)                    (None, 16, 16, 128)  0           add_57[0][0]                     \n",
            "                                                                 conv2d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_183 (BatchN (None, 16, 16, 128)  512         add_58[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_183 (Activation)     (None, 16, 16, 128)  0           batch_normalization_183[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 16, 16, 64)   8256        activation_183[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_184 (BatchN (None, 16, 16, 64)   256         conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_184 (Activation)     (None, 16, 16, 64)   0           batch_normalization_184[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 16, 16, 64)   36928       activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_185 (BatchN (None, 16, 16, 64)   256         conv2d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_185 (Activation)     (None, 16, 16, 64)   0           batch_normalization_185[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 16, 16, 128)  8320        activation_185[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_59 (Add)                    (None, 16, 16, 128)  0           add_58[0][0]                     \n",
            "                                                                 conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_186 (BatchN (None, 16, 16, 128)  512         add_59[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_186 (Activation)     (None, 16, 16, 128)  0           batch_normalization_186[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 8, 8, 128)    16512       activation_186[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_187 (BatchN (None, 8, 8, 128)    512         conv2d_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_187 (Activation)     (None, 8, 8, 128)    0           batch_normalization_187[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 8, 8, 128)    147584      activation_187[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_188 (BatchN (None, 8, 8, 128)    512         conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_188 (Activation)     (None, 8, 8, 128)    0           batch_normalization_188[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 8, 8, 256)    33024       add_59[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 8, 8, 256)    33024       activation_188[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_60 (Add)                    (None, 8, 8, 256)    0           conv2d_210[0][0]                 \n",
            "                                                                 conv2d_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_189 (BatchN (None, 8, 8, 256)    1024        add_60[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_189 (Activation)     (None, 8, 8, 256)    0           batch_normalization_189[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 8, 8, 128)    32896       activation_189[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_190 (BatchN (None, 8, 8, 128)    512         conv2d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_190 (Activation)     (None, 8, 8, 128)    0           batch_normalization_190[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_212 (Conv2D)             (None, 8, 8, 128)    147584      activation_190[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_191 (BatchN (None, 8, 8, 128)    512         conv2d_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_191 (Activation)     (None, 8, 8, 128)    0           batch_normalization_191[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_213 (Conv2D)             (None, 8, 8, 256)    33024       activation_191[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_61 (Add)                    (None, 8, 8, 256)    0           add_60[0][0]                     \n",
            "                                                                 conv2d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_192 (BatchN (None, 8, 8, 256)    1024        add_61[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_192 (Activation)     (None, 8, 8, 256)    0           batch_normalization_192[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_214 (Conv2D)             (None, 8, 8, 128)    32896       activation_192[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_193 (BatchN (None, 8, 8, 128)    512         conv2d_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_193 (Activation)     (None, 8, 8, 128)    0           batch_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_215 (Conv2D)             (None, 8, 8, 128)    147584      activation_193[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_194 (BatchN (None, 8, 8, 128)    512         conv2d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_194 (Activation)     (None, 8, 8, 128)    0           batch_normalization_194[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_216 (Conv2D)             (None, 8, 8, 256)    33024       activation_194[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_62 (Add)                    (None, 8, 8, 256)    0           add_61[0][0]                     \n",
            "                                                                 conv2d_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_195 (BatchN (None, 8, 8, 256)    1024        add_62[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_195 (Activation)     (None, 8, 8, 256)    0           batch_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 1, 1, 256)    0           activation_195[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 256)          0           average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 10)           2570        flatten_6[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function angles_to_projective_transforms at 0x7f26afb28a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function rotate at 0x7f26afb28c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function rotate at 0x7f26afb28c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 11 calls to <function angles_to_projective_transforms at 0x7f26afb28a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 11 calls to <function rotate at 0x7f26afb28c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "98/98 [==============================] - 285s 3s/step - loss: 2.4904 - accuracy: 0.3181\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 274s 3s/step - loss: 2.0848 - accuracy: 0.4546\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 276s 3s/step - loss: 1.8156 - accuracy: 0.5479\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 278s 3s/step - loss: 1.5318 - accuracy: 0.6413\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 283s 3s/step - loss: 1.3347 - accuracy: 0.7013\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 280s 3s/step - loss: 1.2013 - accuracy: 0.7358\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 280s 3s/step - loss: 1.0534 - accuracy: 0.7782\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 277s 3s/step - loss: 0.9403 - accuracy: 0.8088\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 282s 3s/step - loss: 0.8568 - accuracy: 0.8307\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 276s 3s/step - loss: 0.7804 - accuracy: 0.8518\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 271s 3s/step - loss: 0.7223 - accuracy: 0.8695\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 276s 3s/step - loss: 0.6626 - accuracy: 0.8872\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 283s 3s/step - loss: 0.6176 - accuracy: 0.9014\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 282s 3s/step - loss: 0.5683 - accuracy: 0.9170\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 286s 3s/step - loss: 0.5327 - accuracy: 0.9276\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 287s 3s/step - loss: 0.4870 - accuracy: 0.9432\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 284s 3s/step - loss: 0.4485 - accuracy: 0.9552\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 283s 3s/step - loss: 0.4058 - accuracy: 0.9682\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 283s 3s/step - loss: 0.3758 - accuracy: 0.9763\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 284s 3s/step - loss: 0.3382 - accuracy: 0.9876\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 277s 3s/step - loss: 0.3059 - accuracy: 0.9959\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 270s 3s/step - loss: 0.2879 - accuracy: 0.9992\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 276s 3s/step - loss: 0.2784 - accuracy: 0.9999\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 278s 3s/step - loss: 0.2742 - accuracy: 1.0000\n",
            "20/20 [==============================] - 4s 190ms/step - loss: 1.8422 - accuracy: 0.7360\n",
            "val accuracy score at the end of training model type  18 [1.8421774864196778, 0.73603517]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  19\n",
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_217 (Conv2D)             (None, 32, 32, 16)   448         input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_196 (BatchN (None, 32, 32, 16)   64          conv2d_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_196 (Activation)     (None, 32, 32, 16)   0           batch_normalization_196[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_218 (Conv2D)             (None, 32, 32, 16)   272         activation_196[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_197 (BatchN (None, 32, 32, 16)   64          conv2d_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_197 (Activation)     (None, 32, 32, 16)   0           batch_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_219 (Conv2D)             (None, 32, 32, 16)   2320        activation_197[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_21 (Lambda)              (None, 32, 32, 16)   0           conv2d_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_198 (BatchN (None, 32, 32, 16)   64          lambda_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_198 (Activation)     (None, 32, 32, 16)   0           batch_normalization_198[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_221 (Conv2D)             (None, 32, 32, 64)   1088        activation_196[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_220 (Conv2D)             (None, 32, 32, 64)   1088        activation_198[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_63 (Add)                    (None, 32, 32, 64)   0           conv2d_221[0][0]                 \n",
            "                                                                 conv2d_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_199 (BatchN (None, 32, 32, 64)   256         add_63[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_199 (Activation)     (None, 32, 32, 64)   0           batch_normalization_199[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_222 (Conv2D)             (None, 32, 32, 16)   1040        activation_199[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_200 (BatchN (None, 32, 32, 16)   64          conv2d_222[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_200 (Activation)     (None, 32, 32, 16)   0           batch_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_223 (Conv2D)             (None, 32, 32, 16)   2320        activation_200[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_22 (Lambda)              (None, 32, 32, 16)   0           conv2d_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_201 (BatchN (None, 32, 32, 16)   64          lambda_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_201 (Activation)     (None, 32, 32, 16)   0           batch_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_224 (Conv2D)             (None, 32, 32, 64)   1088        activation_201[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_64 (Add)                    (None, 32, 32, 64)   0           add_63[0][0]                     \n",
            "                                                                 conv2d_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_202 (BatchN (None, 32, 32, 64)   256         add_64[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_202 (Activation)     (None, 32, 32, 64)   0           batch_normalization_202[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_225 (Conv2D)             (None, 32, 32, 16)   1040        activation_202[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_203 (BatchN (None, 32, 32, 16)   64          conv2d_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_203 (Activation)     (None, 32, 32, 16)   0           batch_normalization_203[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_226 (Conv2D)             (None, 32, 32, 16)   2320        activation_203[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_23 (Lambda)              (None, 32, 32, 16)   0           conv2d_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_204 (BatchN (None, 32, 32, 16)   64          lambda_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_204 (Activation)     (None, 32, 32, 16)   0           batch_normalization_204[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_227 (Conv2D)             (None, 32, 32, 64)   1088        activation_204[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_65 (Add)                    (None, 32, 32, 64)   0           add_64[0][0]                     \n",
            "                                                                 conv2d_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_205 (BatchN (None, 32, 32, 64)   256         add_65[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_205 (Activation)     (None, 32, 32, 64)   0           batch_normalization_205[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_228 (Conv2D)             (None, 16, 16, 64)   4160        activation_205[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_206 (BatchN (None, 16, 16, 64)   256         conv2d_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_206 (Activation)     (None, 16, 16, 64)   0           batch_normalization_206[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_229 (Conv2D)             (None, 16, 16, 64)   36928       activation_206[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_207 (BatchN (None, 16, 16, 64)   256         conv2d_229[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_207 (Activation)     (None, 16, 16, 64)   0           batch_normalization_207[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_231 (Conv2D)             (None, 16, 16, 128)  8320        add_65[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_230 (Conv2D)             (None, 16, 16, 128)  8320        activation_207[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_66 (Add)                    (None, 16, 16, 128)  0           conv2d_231[0][0]                 \n",
            "                                                                 conv2d_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_208 (BatchN (None, 16, 16, 128)  512         add_66[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_208 (Activation)     (None, 16, 16, 128)  0           batch_normalization_208[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_232 (Conv2D)             (None, 16, 16, 64)   8256        activation_208[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_209 (BatchN (None, 16, 16, 64)   256         conv2d_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_209 (Activation)     (None, 16, 16, 64)   0           batch_normalization_209[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_233 (Conv2D)             (None, 16, 16, 64)   36928       activation_209[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_210 (BatchN (None, 16, 16, 64)   256         conv2d_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_210 (Activation)     (None, 16, 16, 64)   0           batch_normalization_210[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_234 (Conv2D)             (None, 16, 16, 128)  8320        activation_210[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_67 (Add)                    (None, 16, 16, 128)  0           add_66[0][0]                     \n",
            "                                                                 conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_211 (BatchN (None, 16, 16, 128)  512         add_67[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_211 (Activation)     (None, 16, 16, 128)  0           batch_normalization_211[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_235 (Conv2D)             (None, 16, 16, 64)   8256        activation_211[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_212 (BatchN (None, 16, 16, 64)   256         conv2d_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_212 (Activation)     (None, 16, 16, 64)   0           batch_normalization_212[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_236 (Conv2D)             (None, 16, 16, 64)   36928       activation_212[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_213 (BatchN (None, 16, 16, 64)   256         conv2d_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_213 (Activation)     (None, 16, 16, 64)   0           batch_normalization_213[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_237 (Conv2D)             (None, 16, 16, 128)  8320        activation_213[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_68 (Add)                    (None, 16, 16, 128)  0           add_67[0][0]                     \n",
            "                                                                 conv2d_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_214 (BatchN (None, 16, 16, 128)  512         add_68[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_214 (Activation)     (None, 16, 16, 128)  0           batch_normalization_214[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_238 (Conv2D)             (None, 8, 8, 128)    16512       activation_214[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_215 (BatchN (None, 8, 8, 128)    512         conv2d_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_215 (Activation)     (None, 8, 8, 128)    0           batch_normalization_215[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_239 (Conv2D)             (None, 8, 8, 128)    147584      activation_215[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_216 (BatchN (None, 8, 8, 128)    512         conv2d_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_216 (Activation)     (None, 8, 8, 128)    0           batch_normalization_216[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_241 (Conv2D)             (None, 8, 8, 256)    33024       add_68[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_240 (Conv2D)             (None, 8, 8, 256)    33024       activation_216[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_69 (Add)                    (None, 8, 8, 256)    0           conv2d_241[0][0]                 \n",
            "                                                                 conv2d_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_217 (BatchN (None, 8, 8, 256)    1024        add_69[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_217 (Activation)     (None, 8, 8, 256)    0           batch_normalization_217[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_242 (Conv2D)             (None, 8, 8, 128)    32896       activation_217[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_218 (BatchN (None, 8, 8, 128)    512         conv2d_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_218 (Activation)     (None, 8, 8, 128)    0           batch_normalization_218[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_243 (Conv2D)             (None, 8, 8, 128)    147584      activation_218[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_219 (BatchN (None, 8, 8, 128)    512         conv2d_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_219 (Activation)     (None, 8, 8, 128)    0           batch_normalization_219[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_244 (Conv2D)             (None, 8, 8, 256)    33024       activation_219[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_70 (Add)                    (None, 8, 8, 256)    0           add_69[0][0]                     \n",
            "                                                                 conv2d_244[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_220 (BatchN (None, 8, 8, 256)    1024        add_70[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_220 (Activation)     (None, 8, 8, 256)    0           batch_normalization_220[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_245 (Conv2D)             (None, 8, 8, 128)    32896       activation_220[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_221 (BatchN (None, 8, 8, 128)    512         conv2d_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_221 (Activation)     (None, 8, 8, 128)    0           batch_normalization_221[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_246 (Conv2D)             (None, 8, 8, 128)    147584      activation_221[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_222 (BatchN (None, 8, 8, 128)    512         conv2d_246[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_222 (Activation)     (None, 8, 8, 128)    0           batch_normalization_222[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_247 (Conv2D)             (None, 8, 8, 256)    33024       activation_222[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_71 (Add)                    (None, 8, 8, 256)    0           add_70[0][0]                     \n",
            "                                                                 conv2d_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_223 (BatchN (None, 8, 8, 256)    1024        add_71[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_223 (Activation)     (None, 8, 8, 256)    0           batch_normalization_223[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 1, 1, 256)    0           activation_223[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 256)          0           average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 10)           2570        flatten_7[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 277s 3s/step - loss: 2.4761 - accuracy: 0.3205\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 268s 3s/step - loss: 2.0566 - accuracy: 0.4688\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 274s 3s/step - loss: 1.7235 - accuracy: 0.5860\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 270s 3s/step - loss: 1.4826 - accuracy: 0.6597\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 267s 3s/step - loss: 1.3259 - accuracy: 0.7051\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 270s 3s/step - loss: 1.1741 - accuracy: 0.7473\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 277s 3s/step - loss: 1.0087 - accuracy: 0.7945\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 282s 3s/step - loss: 0.8865 - accuracy: 0.8286\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "97/98 [============================>.] - ETA: 2s - loss: 0.8090 - accuracy: 0.8473"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovVZ-saos4uJ",
        "colab_type": "code",
        "outputId": "4fb41bbd-7059-4d4b-8051-5d307daf7021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for model_params in [19,20,21,22,23,24,25]:\n",
        "  is_training=True\n",
        "  print(\"Model Param:- \",model_params)\n",
        "  model = resnet_v2(input_shape=(32,32,3), depth=depth, distortion = model_params)\n",
        "  model.summary()\n",
        "  #global_step = tf.train.get_or_create_global_step()\n",
        "  #model=model=build_model(model_params)\n",
        "  opt=SGD(lr=0.025,momentum=0.9,nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,metrics=['accuracy']\n",
        "              )\n",
        "  \n",
        "  \n",
        "  #if model_params in [0,4,5]:  \n",
        "  train_ds=train_ds2  \n",
        "  model.fit(train_ds,epochs=EPOCHS, steps_per_epoch=np.ceil(50000/batch_size), \n",
        "          callbacks=[lr_sched],\n",
        "          verbose=1)\n",
        "  is_training=False\n",
        "  score=model.evaluate(test_ds, steps =np.ceil(10000/batch_size), verbose=1)\n",
        "\n",
        "  del(model)\n",
        "  del(train_ds)\n",
        "  \n",
        "  print('val accuracy score at the end of training model type ',model_params, score)\n",
        "  print(\"=========================================\\n\")\n",
        "\n",
        "#validation_data=test_ds, validation_steps=np.ceil(10000/batch_size),"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Param:-  19\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   272         activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 32, 32, 16)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 64)   1088        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 64)   1088        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 64)   0           conv2d_4[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 64)   256         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   1040        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 32, 32, 16)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 64)   1088        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 64)   0           add[0][0]                        \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 16)   1040        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 16)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2320        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 32, 32, 16)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 16)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 64)   1088        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 64)   0           add_1[0][0]                      \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 64)   4160        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 64)   36928       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 128)  8320        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 128)  8320        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 128)  0           conv2d_14[0][0]                  \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 128)  512         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 64)   8256        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 64)   256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 64)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 64)   36928       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 128)  8320        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 128)  0           add_3[0][0]                      \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 128)  512         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 128)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 64)   8256        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 64)   256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 64)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 64)   36928       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 64)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 128)  8320        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 128)  0           add_4[0][0]                      \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 128)  512         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 128)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 128)    16512       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 128)    512         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 128)    0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 128)    147584      activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 8, 128)    512         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 8, 8, 128)    0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 8, 8, 256)    33024       add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 8, 8, 256)    33024       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 8, 8, 256)    0           conv2d_24[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 8, 256)    1024        add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 8, 8, 256)    0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 8, 8, 128)    32896       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 8, 8, 128)    512         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 8, 8, 128)    0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 8, 8, 128)    147584      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 8, 8, 128)    512         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 8, 8, 128)    0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 8, 8, 256)    33024       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 256)    0           add_6[0][0]                      \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 8, 8, 256)    1024        add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 8, 8, 256)    0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 128)    32896       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 8, 8, 128)    512         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 8, 8, 128)    0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 128)    147584      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 128)    512         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 8, 128)    0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 256)    33024       activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 256)    0           add_7[0][0]                      \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 256)    1024        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 256)    0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 1, 1, 256)    0           activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 256)          0           average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           2570        flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 283s 3s/step - loss: 2.4651 - accuracy: 0.3287\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 270s 3s/step - loss: 2.0737 - accuracy: 0.4603\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 270s 3s/step - loss: 1.7567 - accuracy: 0.5699\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 269s 3s/step - loss: 1.4990 - accuracy: 0.6524\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 267s 3s/step - loss: 1.3612 - accuracy: 0.6928\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 266s 3s/step - loss: 1.1878 - accuracy: 0.7415\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 266s 3s/step - loss: 1.0250 - accuracy: 0.7886\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 267s 3s/step - loss: 0.8994 - accuracy: 0.8250\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 266s 3s/step - loss: 0.8178 - accuracy: 0.8432\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.7429 - accuracy: 0.8641\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 258s 3s/step - loss: 0.6804 - accuracy: 0.8820\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.6299 - accuracy: 0.8960\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 263s 3s/step - loss: 0.5831 - accuracy: 0.9093\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 257s 3s/step - loss: 0.5429 - accuracy: 0.9215\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.4978 - accuracy: 0.9371\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 257s 3s/step - loss: 0.4672 - accuracy: 0.9441\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 256s 3s/step - loss: 0.4331 - accuracy: 0.9534\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.3887 - accuracy: 0.9687\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.3580 - accuracy: 0.9766\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 258s 3s/step - loss: 0.3266 - accuracy: 0.9851\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 258s 3s/step - loss: 0.2961 - accuracy: 0.9939\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 256s 3s/step - loss: 0.2767 - accuracy: 0.9980\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 263s 3s/step - loss: 0.2662 - accuracy: 0.9993\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 267s 3s/step - loss: 0.2609 - accuracy: 0.9999\n",
            "20/20 [==============================] - 33s 2s/step - loss: 0.9074 - accuracy: 0.8541\n",
            "val accuracy score at the end of training model type  19 [0.9074338346719741, 0.85410154]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  20\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 32, 32, 16)   448         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 32, 32, 16)   64          conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 32, 32, 16)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 32, 32, 16)   272         activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 32, 32, 16)   64          conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 32, 32, 16)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 32, 32, 16)   2320        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 32, 32, 16)   0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 32, 32, 16)   64          lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 32, 32, 16)   0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 32, 32, 64)   1088        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 32, 32, 64)   1088        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 32, 32, 64)   0           conv2d_35[0][0]                  \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 32, 32, 64)   256         add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 32, 32, 64)   0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 32, 32, 16)   1040        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 32, 32, 16)   64          conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 32, 32, 16)   0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 32, 32, 16)   2320        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 32, 32, 16)   0           conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 32, 32, 16)   64          lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 32, 32, 16)   0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 32, 32, 64)   1088        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 32, 32, 64)   0           add_9[0][0]                      \n",
            "                                                                 conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 32, 32, 64)   256         add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 32, 32, 64)   0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 32, 32, 16)   1040        activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 32, 32, 16)   64          conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 32, 32, 16)   0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 32, 32, 16)   2320        activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 32, 32, 16)   0           conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 32, 32, 16)   64          lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 32, 32, 16)   0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 32, 32, 64)   1088        activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 32, 32, 64)   0           add_10[0][0]                     \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 32, 32, 64)   256         add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 32, 32, 64)   0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 16, 16, 64)   4160        activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 16, 16, 64)   256         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 16, 16, 64)   0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 16, 16, 64)   36928       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 16, 16, 64)   256         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 16, 16, 64)   0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 16, 16, 128)  8320        add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 16, 16, 128)  8320        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 16, 16, 128)  0           conv2d_45[0][0]                  \n",
            "                                                                 conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 16, 16, 128)  512         add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 16, 16, 128)  0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 16, 16, 64)   8256        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 16, 16, 64)   256         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 16, 16, 64)   0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 16, 16, 64)   36928       activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 16, 16, 64)   256         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 16, 16, 64)   0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 16, 16, 128)  8320        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 16, 16, 128)  0           add_12[0][0]                     \n",
            "                                                                 conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 16, 16, 128)  512         add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 16, 16, 128)  0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 16, 16, 64)   8256        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 16, 16, 64)   256         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 16, 16, 64)   0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 16, 16, 64)   36928       activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 16, 16, 64)   256         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 16, 16, 64)   0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 16, 16, 128)  8320        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 16, 16, 128)  0           add_13[0][0]                     \n",
            "                                                                 conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 16, 16, 128)  512         add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 16, 16, 128)  0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 8, 8, 128)    16512       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 8, 8, 128)    512         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 8, 8, 128)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 8, 8, 128)    147584      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 8, 8, 128)    512         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 8, 8, 128)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 8, 8, 256)    33024       add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 8, 8, 256)    33024       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 8, 8, 256)    0           conv2d_55[0][0]                  \n",
            "                                                                 conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 8, 8, 256)    1024        add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 8, 8, 256)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 8, 8, 128)    32896       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 8, 8, 128)    512         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 8, 8, 128)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 8, 8, 128)    147584      activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 8, 8, 128)    512         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 8, 8, 128)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 8, 8, 256)    33024       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 8, 8, 256)    0           add_15[0][0]                     \n",
            "                                                                 conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 8, 8, 256)    1024        add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 8, 8, 256)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 8, 8, 128)    32896       activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 8, 8, 128)    512         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 8, 8, 128)    0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 8, 8, 128)    147584      activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 8, 8, 128)    512         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 8, 8, 128)    0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 8, 8, 256)    33024       activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 8, 8, 256)    0           add_16[0][0]                     \n",
            "                                                                 conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 8, 8, 256)    1024        add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 8, 8, 256)    0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 256)    0           activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 256)          0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           2570        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 202s 2s/step - loss: 2.4882 - accuracy: 0.3117\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 2.1030 - accuracy: 0.4489\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 192s 2s/step - loss: 1.8401 - accuracy: 0.5393\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 193s 2s/step - loss: 1.5996 - accuracy: 0.6171\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 192s 2s/step - loss: 1.4144 - accuracy: 0.6714\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 193s 2s/step - loss: 1.2789 - accuracy: 0.7068\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 193s 2s/step - loss: 1.1254 - accuracy: 0.7456\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 192s 2s/step - loss: 1.0081 - accuracy: 0.7799\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 194s 2s/step - loss: 0.9260 - accuracy: 0.8001\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 194s 2s/step - loss: 0.8465 - accuracy: 0.8221\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 194s 2s/step - loss: 0.7829 - accuracy: 0.8401\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 194s 2s/step - loss: 0.7210 - accuracy: 0.8557\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 192s 2s/step - loss: 0.6665 - accuracy: 0.8717\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 192s 2s/step - loss: 0.6231 - accuracy: 0.8845\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 193s 2s/step - loss: 0.5603 - accuracy: 0.9049\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 193s 2s/step - loss: 0.5284 - accuracy: 0.9145\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 192s 2s/step - loss: 0.4788 - accuracy: 0.9315\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 194s 2s/step - loss: 0.4345 - accuracy: 0.9461\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 193s 2s/step - loss: 0.3858 - accuracy: 0.9621\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 192s 2s/step - loss: 0.3480 - accuracy: 0.9740\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 192s 2s/step - loss: 0.3126 - accuracy: 0.9843\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 0.2805 - accuracy: 0.9942\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 0.2635 - accuracy: 0.9977\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 0.2550 - accuracy: 0.9995\n",
            "20/20 [==============================] - 4s 194ms/step - loss: 0.8706 - accuracy: 0.8545\n",
            "val accuracy score at the end of training model type  20 [0.8706138283014297, 0.8544922]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  21\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 32, 32, 16)   448         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 32, 32, 16)   64          conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 32, 32, 16)   0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 32, 32, 16)   272         activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 32, 32, 16)   64          conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 32, 32, 16)   0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 32, 32, 16)   2320        activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 32, 32, 16)   64          conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 32, 32, 16)   0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 32, 32, 64)   1088        activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 32, 32, 64)   1088        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 32, 32, 64)   0           conv2d_66[0][0]                  \n",
            "                                                                 conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 32, 32, 64)   256         add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 32, 32, 64)   0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 32, 32, 16)   1040        activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 32, 32, 16)   64          conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 32, 32, 16)   0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 32, 32, 16)   2320        activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 32, 32, 16)   64          conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 32, 32, 16)   0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 32, 32, 64)   1088        activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 32, 32, 64)   0           add_18[0][0]                     \n",
            "                                                                 conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 32, 32, 64)   256         add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 32, 32, 64)   0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 32, 32, 16)   1040        activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 32, 32, 16)   64          conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 32, 32, 16)   0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 32, 32, 16)   2320        activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 32, 32, 16)   64          conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 32, 32, 16)   0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 32, 32, 64)   1088        activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 32, 32, 64)   0           add_19[0][0]                     \n",
            "                                                                 conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 32, 32, 64)   256         add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 32, 32, 64)   0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 16, 16, 64)   4160        activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 16, 16, 64)   256         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 16, 16, 64)   0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 16, 16, 64)   36928       activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 16, 16, 64)   0           conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 16, 16, 64)   256         lambda_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 16, 16, 64)   0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 16, 16, 128)  8320        add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 16, 16, 128)  8320        activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 16, 16, 128)  0           conv2d_76[0][0]                  \n",
            "                                                                 conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 16, 16, 128)  512         add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 16, 16, 128)  0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 16, 16, 64)   8256        activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 16, 16, 64)   256         conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 16, 16, 64)   0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 16, 16, 64)   36928       activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 16, 16, 64)   0           conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 16, 16, 64)   256         lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 16, 16, 64)   0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 16, 16, 128)  8320        activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 16, 16, 128)  0           add_21[0][0]                     \n",
            "                                                                 conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 16, 16, 128)  512         add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 16, 16, 128)  0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 16, 16, 64)   8256        activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 16, 16, 64)   256         conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 16, 16, 64)   0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 16, 16, 64)   36928       activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 16, 16, 64)   0           conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 16, 16, 64)   256         lambda_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 16, 16, 64)   0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 16, 16, 128)  8320        activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 16, 16, 128)  0           add_22[0][0]                     \n",
            "                                                                 conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 16, 16, 128)  512         add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 16, 16, 128)  0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 8, 8, 128)    16512       activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 8, 8, 128)    512         conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 8, 8, 128)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 8, 8, 128)    147584      activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 8, 8, 128)    512         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 8, 8, 128)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 8, 8, 256)    33024       add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 8, 8, 256)    33024       activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 8, 8, 256)    0           conv2d_86[0][0]                  \n",
            "                                                                 conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 8, 8, 256)    1024        add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 8, 8, 256)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 8, 8, 128)    32896       activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 8, 8, 128)    512         conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 8, 8, 128)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 8, 8, 128)    147584      activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 8, 8, 128)    512         conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 8, 8, 128)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 8, 8, 256)    33024       activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 8, 8, 256)    0           add_24[0][0]                     \n",
            "                                                                 conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 8, 8, 256)    1024        add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 8, 8, 256)    0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 8, 8, 128)    32896       activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 8, 8, 128)    512         conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 8, 8, 128)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 8, 8, 128)    147584      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 8, 8, 128)    512         conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 8, 8, 128)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 8, 8, 256)    33024       activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 8, 8, 256)    0           add_25[0][0]                     \n",
            "                                                                 conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 8, 8, 256)    1024        add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 8, 8, 256)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 1, 1, 256)    0           activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 256)          0           average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           2570        flatten_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 217s 2s/step - loss: 2.4833 - accuracy: 0.3137\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 211s 2s/step - loss: 2.0845 - accuracy: 0.4537\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 216s 2s/step - loss: 1.8050 - accuracy: 0.5516\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 213s 2s/step - loss: 1.5771 - accuracy: 0.6241\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 209s 2s/step - loss: 1.4334 - accuracy: 0.6620\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 207s 2s/step - loss: 1.3088 - accuracy: 0.6939\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 208s 2s/step - loss: 1.1509 - accuracy: 0.7349\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 209s 2s/step - loss: 1.0302 - accuracy: 0.7684\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 209s 2s/step - loss: 0.9345 - accuracy: 0.7930\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 209s 2s/step - loss: 0.8587 - accuracy: 0.8141\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 208s 2s/step - loss: 0.7912 - accuracy: 0.8304\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 209s 2s/step - loss: 0.7294 - accuracy: 0.8462\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 210s 2s/step - loss: 0.6811 - accuracy: 0.8602\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 210s 2s/step - loss: 0.6248 - accuracy: 0.8770\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 211s 2s/step - loss: 0.5806 - accuracy: 0.8894\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 211s 2s/step - loss: 0.5296 - accuracy: 0.9052\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 210s 2s/step - loss: 0.4830 - accuracy: 0.9207\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 209s 2s/step - loss: 0.4338 - accuracy: 0.9354\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 210s 2s/step - loss: 0.3915 - accuracy: 0.9507\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 210s 2s/step - loss: 0.3449 - accuracy: 0.9654\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 209s 2s/step - loss: 0.3022 - accuracy: 0.9798\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 210s 2s/step - loss: 0.2654 - accuracy: 0.9920\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 209s 2s/step - loss: 0.2425 - accuracy: 0.9980\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 209s 2s/step - loss: 0.2328 - accuracy: 0.9997\n",
            "20/20 [==============================] - 4s 211ms/step - loss: 0.8893 - accuracy: 0.8433\n",
            "val accuracy score at the end of training model type  21 [0.8893059968948365, 0.8432617]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  22\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 32, 32, 16)   448         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 32, 32, 16)   64          conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 32, 32, 16)   0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 32, 32, 16)   272         activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 32, 32, 16)   64          conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 32, 32, 16)   0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 32, 32, 16)   2320        activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 32, 32, 16)   64          conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 32, 32, 16)   0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 32, 32, 64)   1088        activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 32, 32, 64)   1088        activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 32, 32, 64)   0           conv2d_97[0][0]                  \n",
            "                                                                 conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 32, 32, 64)   256         add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 32, 32, 64)   0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 32, 32, 16)   1040        activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 32, 32, 16)   64          conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 32, 32, 16)   0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 32, 32, 16)   2320        activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 32, 32, 16)   64          conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 32, 32, 16)   0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 32, 32, 64)   1088        activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_28 (Add)                    (None, 32, 32, 64)   0           add_27[0][0]                     \n",
            "                                                                 conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 32, 32, 64)   256         add_28[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 32, 32, 64)   0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 32, 32, 16)   1040        activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 32, 32, 16)   64          conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 32, 32, 16)   0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 32, 32, 16)   2320        activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 32, 32, 16)   64          conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 32, 32, 16)   0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 32, 32, 64)   1088        activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_29 (Add)                    (None, 32, 32, 64)   0           add_28[0][0]                     \n",
            "                                                                 conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 32, 32, 64)   256         add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 32, 32, 64)   0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 16, 16, 64)   4160        activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 16, 16, 64)   256         conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 16, 16, 64)   0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 16, 16, 64)   36928       activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 16, 16, 64)   0           conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 16, 16, 64)   256         lambda_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 16, 16, 64)   0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 16, 16, 128)  8320        add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 16, 16, 128)  8320        activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_30 (Add)                    (None, 16, 16, 128)  0           conv2d_107[0][0]                 \n",
            "                                                                 conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 16, 16, 128)  512         add_30[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 16, 16, 128)  0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 16, 16, 64)   8256        activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 16, 16, 64)   256         conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 16, 16, 64)   0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 16, 16, 64)   36928       activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 16, 16, 64)   0           conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 16, 16, 64)   256         lambda_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 16, 16, 64)   0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 16, 16, 128)  8320        activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_31 (Add)                    (None, 16, 16, 128)  0           add_30[0][0]                     \n",
            "                                                                 conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 16, 16, 128)  512         add_31[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 16, 16, 128)  0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 16, 16, 64)   8256        activation_99[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 16, 16, 64)   256         conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 16, 16, 64)   0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 16, 16, 64)   36928       activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 16, 16, 64)   0           conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 16, 16, 64)   256         lambda_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 16, 16, 64)   0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 16, 16, 128)  8320        activation_101[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_32 (Add)                    (None, 16, 16, 128)  0           add_31[0][0]                     \n",
            "                                                                 conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 16, 16, 128)  512         add_32[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 16, 16, 128)  0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 8, 8, 128)    16512       activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 8, 8, 128)    512         conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 8, 8, 128)    0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 8, 8, 128)    147584      activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 8, 8, 128)    512         conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 8, 8, 128)    0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 8, 8, 256)    33024       add_32[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 8, 8, 256)    33024       activation_104[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_33 (Add)                    (None, 8, 8, 256)    0           conv2d_117[0][0]                 \n",
            "                                                                 conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 8, 8, 256)    1024        add_33[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 8, 8, 256)    0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 8, 8, 128)    32896       activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 8, 8, 128)    512         conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 8, 8, 128)    0           batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 8, 8, 128)    147584      activation_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 8, 8, 128)    512         conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 8, 8, 128)    0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 8, 8, 256)    33024       activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_34 (Add)                    (None, 8, 8, 256)    0           add_33[0][0]                     \n",
            "                                                                 conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 8, 8, 256)    1024        add_34[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 8, 8, 256)    0           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 8, 8, 128)    32896       activation_108[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 8, 8, 128)    512         conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 8, 8, 128)    0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 8, 8, 128)    147584      activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 8, 8, 128)    512         conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 8, 8, 128)    0           batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 8, 8, 256)    33024       activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_35 (Add)                    (None, 8, 8, 256)    0           add_34[0][0]                     \n",
            "                                                                 conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 8, 8, 256)    1024        add_35[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 8, 8, 256)    0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 1, 1, 256)    0           activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 256)          0           average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           2570        flatten_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function angles_to_projective_transforms at 0x7f4269428a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function rotate at 0x7f4269428c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function angles_to_projective_transforms at 0x7f4269428a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function rotate at 0x7f4269428c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function angles_to_projective_transforms at 0x7f4269428a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function rotate at 0x7f4269428c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "98/98 [==============================] - 269s 3s/step - loss: 2.4631 - accuracy: 0.3213\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 257s 3s/step - loss: 2.0235 - accuracy: 0.4814\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 254s 3s/step - loss: 1.7240 - accuracy: 0.5803\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 258s 3s/step - loss: 1.5434 - accuracy: 0.6391\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 260s 3s/step - loss: 1.3102 - accuracy: 0.7104\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 258s 3s/step - loss: 1.1579 - accuracy: 0.7514\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.9910 - accuracy: 0.8021\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.8714 - accuracy: 0.8338\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.7886 - accuracy: 0.8539\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.7134 - accuracy: 0.8744\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.6545 - accuracy: 0.8909\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.6039 - accuracy: 0.9066\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.5587 - accuracy: 0.9201\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.5140 - accuracy: 0.9332\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.4840 - accuracy: 0.9433\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.4415 - accuracy: 0.9556\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.4030 - accuracy: 0.9668\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.3650 - accuracy: 0.9771\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.3411 - accuracy: 0.9830\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 264s 3s/step - loss: 0.3079 - accuracy: 0.9929\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.2819 - accuracy: 0.9982\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.2682 - accuracy: 0.9998\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.2612 - accuracy: 1.0000\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 268s 3s/step - loss: 0.2575 - accuracy: 1.0000\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 0.9566 - accuracy: 0.8536\n",
            "val accuracy score at the end of training model type  22 [0.9566449522972107, 0.85361326]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  23\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 32, 32, 16)   448         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 32, 32, 16)   64          conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 32, 32, 16)   0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 32, 32, 16)   272         activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 32, 32, 16)   64          conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 32, 32, 16)   0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 32, 32, 16)   2320        activation_113[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 32, 32, 16)   64          conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 32, 32, 16)   0           batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 32, 32, 64)   1088        activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 32, 32, 64)   1088        activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_36 (Add)                    (None, 32, 32, 64)   0           conv2d_128[0][0]                 \n",
            "                                                                 conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 32, 32, 64)   256         add_36[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 32, 32, 64)   0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 32, 32, 16)   1040        activation_115[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 32, 32, 16)   64          conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 32, 32, 16)   0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 32, 32, 16)   2320        activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 32, 32, 16)   64          conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 32, 32, 16)   0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 32, 32, 64)   1088        activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_37 (Add)                    (None, 32, 32, 64)   0           add_36[0][0]                     \n",
            "                                                                 conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 32, 32, 64)   256         add_37[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 32, 32, 64)   0           batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 32, 32, 16)   1040        activation_118[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 32, 32, 16)   64          conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 32, 32, 16)   0           batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 32, 32, 16)   2320        activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 32, 32, 16)   64          conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 32, 32, 16)   0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 32, 32, 64)   1088        activation_120[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_38 (Add)                    (None, 32, 32, 64)   0           add_37[0][0]                     \n",
            "                                                                 conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 32, 32, 64)   256         add_38[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 32, 32, 64)   0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 16, 16, 64)   4160        activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 16, 16, 64)   256         conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 16, 16, 64)   0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 16, 16, 64)   36928       activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_12 (Lambda)              (None, 16, 16, 64)   0           conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 16, 16, 64)   256         lambda_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 16, 16, 64)   0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 16, 16, 128)  8320        add_38[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 16, 16, 128)  8320        activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_39 (Add)                    (None, 16, 16, 128)  0           conv2d_138[0][0]                 \n",
            "                                                                 conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 16, 16, 128)  512         add_39[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 16, 16, 128)  0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 16, 16, 64)   8256        activation_124[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 16, 16, 64)   256         conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 16, 16, 64)   0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 16, 16, 64)   36928       activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_13 (Lambda)              (None, 16, 16, 64)   0           conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 16, 16, 64)   256         lambda_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 16, 16, 64)   0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 16, 16, 128)  8320        activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_40 (Add)                    (None, 16, 16, 128)  0           add_39[0][0]                     \n",
            "                                                                 conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 16, 16, 128)  512         add_40[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 16, 16, 128)  0           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 16, 16, 64)   8256        activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 16, 16, 64)   256         conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 16, 16, 64)   0           batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 16, 16, 64)   36928       activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_14 (Lambda)              (None, 16, 16, 64)   0           conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 16, 16, 64)   256         lambda_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 16, 16, 64)   0           batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 16, 16, 128)  8320        activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_41 (Add)                    (None, 16, 16, 128)  0           add_40[0][0]                     \n",
            "                                                                 conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 16, 16, 128)  512         add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 16, 16, 128)  0           batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, 8, 8, 128)    16512       activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 8, 8, 128)    512         conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 8, 8, 128)    0           batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, 8, 8, 128)    147584      activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 8, 8, 128)    512         conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 8, 8, 128)    0           batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_148 (Conv2D)             (None, 8, 8, 256)    33024       add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, 8, 8, 256)    33024       activation_132[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_42 (Add)                    (None, 8, 8, 256)    0           conv2d_148[0][0]                 \n",
            "                                                                 conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, 8, 8, 256)    1024        add_42[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 8, 8, 256)    0           batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_149 (Conv2D)             (None, 8, 8, 128)    32896       activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 8, 8, 128)    512         conv2d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 8, 8, 128)    0           batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_150 (Conv2D)             (None, 8, 8, 128)    147584      activation_134[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 8, 8, 128)    512         conv2d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 8, 8, 128)    0           batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_151 (Conv2D)             (None, 8, 8, 256)    33024       activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_43 (Add)                    (None, 8, 8, 256)    0           add_42[0][0]                     \n",
            "                                                                 conv2d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 8, 8, 256)    1024        add_43[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 8, 8, 256)    0           batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_152 (Conv2D)             (None, 8, 8, 128)    32896       activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 8, 8, 128)    512         conv2d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 8, 8, 128)    0           batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, 8, 8, 128)    147584      activation_137[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 8, 8, 128)    512         conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 8, 8, 128)    0           batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, 8, 8, 256)    33024       activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_44 (Add)                    (None, 8, 8, 256)    0           add_43[0][0]                     \n",
            "                                                                 conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, 8, 8, 256)    1024        add_44[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 8, 8, 256)    0           batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 1, 1, 256)    0           activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 256)          0           average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 10)           2570        flatten_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 286s 3s/step - loss: 2.5120 - accuracy: 0.3052\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 278s 3s/step - loss: 2.0649 - accuracy: 0.4623\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 276s 3s/step - loss: 1.7590 - accuracy: 0.5716\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 275s 3s/step - loss: 1.5232 - accuracy: 0.6445\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 276s 3s/step - loss: 1.3710 - accuracy: 0.6889\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 275s 3s/step - loss: 1.2180 - accuracy: 0.7283\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 275s 3s/step - loss: 1.0507 - accuracy: 0.7791\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 276s 3s/step - loss: 0.9278 - accuracy: 0.8110\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 274s 3s/step - loss: 0.8365 - accuracy: 0.8338\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 273s 3s/step - loss: 0.7649 - accuracy: 0.8549\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 273s 3s/step - loss: 0.7020 - accuracy: 0.8704\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 274s 3s/step - loss: 0.6486 - accuracy: 0.8860\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 274s 3s/step - loss: 0.6047 - accuracy: 0.8994\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 273s 3s/step - loss: 0.5627 - accuracy: 0.9103\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 275s 3s/step - loss: 0.5184 - accuracy: 0.9228\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 267s 3s/step - loss: 0.4834 - accuracy: 0.9332\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 268s 3s/step - loss: 0.4454 - accuracy: 0.9464\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 265s 3s/step - loss: 0.4032 - accuracy: 0.9590\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 265s 3s/step - loss: 0.3698 - accuracy: 0.9691\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 267s 3s/step - loss: 0.3341 - accuracy: 0.9799\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 267s 3s/step - loss: 0.3049 - accuracy: 0.9882\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 265s 3s/step - loss: 0.2798 - accuracy: 0.9950\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 267s 3s/step - loss: 0.2651 - accuracy: 0.9983\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 267s 3s/step - loss: 0.2578 - accuracy: 0.9992\n",
            "20/20 [==============================] - 31s 2s/step - loss: 0.8799 - accuracy: 0.8575\n",
            "val accuracy score at the end of training model type  23 [0.8799214720726013, 0.8575195]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  24\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, 32, 32, 16)   448         input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_140 (BatchN (None, 32, 32, 16)   64          conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 32, 32, 16)   0           batch_normalization_140[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, 32, 32, 16)   272         activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_141 (BatchN (None, 32, 32, 16)   64          conv2d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 32, 32, 16)   0           batch_normalization_141[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, 32, 32, 16)   2320        activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_142 (BatchN (None, 32, 32, 16)   64          conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 32, 32, 16)   0           batch_normalization_142[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 32, 32, 64)   1088        activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, 32, 32, 64)   1088        activation_142[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_45 (Add)                    (None, 32, 32, 64)   0           conv2d_159[0][0]                 \n",
            "                                                                 conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_143 (BatchN (None, 32, 32, 64)   256         add_45[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 32, 32, 64)   0           batch_normalization_143[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 32, 32, 16)   1040        activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_144 (BatchN (None, 32, 32, 16)   64          conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 32, 32, 16)   0           batch_normalization_144[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 32, 32, 16)   2320        activation_144[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_145 (BatchN (None, 32, 32, 16)   64          conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 32, 32, 16)   0           batch_normalization_145[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 32, 32, 64)   1088        activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_46 (Add)                    (None, 32, 32, 64)   0           add_45[0][0]                     \n",
            "                                                                 conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_146 (BatchN (None, 32, 32, 64)   256         add_46[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 32, 32, 64)   0           batch_normalization_146[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 32, 32, 16)   1040        activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_147 (BatchN (None, 32, 32, 16)   64          conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 32, 32, 16)   0           batch_normalization_147[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_164 (Conv2D)             (None, 32, 32, 16)   2320        activation_147[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_148 (BatchN (None, 32, 32, 16)   64          conv2d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 32, 32, 16)   0           batch_normalization_148[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_165 (Conv2D)             (None, 32, 32, 64)   1088        activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_47 (Add)                    (None, 32, 32, 64)   0           add_46[0][0]                     \n",
            "                                                                 conv2d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_149 (BatchN (None, 32, 32, 64)   256         add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 32, 32, 64)   0           batch_normalization_149[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_166 (Conv2D)             (None, 16, 16, 64)   4160        activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_150 (BatchN (None, 16, 16, 64)   256         conv2d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 16, 16, 64)   0           batch_normalization_150[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_167 (Conv2D)             (None, 16, 16, 64)   36928       activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_15 (Lambda)              (None, 16, 16, 64)   0           conv2d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_151 (BatchN (None, 16, 16, 64)   256         lambda_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 16, 16, 64)   0           batch_normalization_151[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_169 (Conv2D)             (None, 16, 16, 128)  8320        add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_168 (Conv2D)             (None, 16, 16, 128)  8320        activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_48 (Add)                    (None, 16, 16, 128)  0           conv2d_169[0][0]                 \n",
            "                                                                 conv2d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_152 (BatchN (None, 16, 16, 128)  512         add_48[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 16, 16, 128)  0           batch_normalization_152[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, 16, 16, 64)   8256        activation_152[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_153 (BatchN (None, 16, 16, 64)   256         conv2d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 16, 16, 64)   0           batch_normalization_153[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, 16, 16, 64)   36928       activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_16 (Lambda)              (None, 16, 16, 64)   0           conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, 16, 16, 64)   256         lambda_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 16, 16, 64)   0           batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_172 (Conv2D)             (None, 16, 16, 128)  8320        activation_154[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_49 (Add)                    (None, 16, 16, 128)  0           add_48[0][0]                     \n",
            "                                                                 conv2d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, 16, 16, 128)  512         add_49[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 16, 16, 128)  0           batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_173 (Conv2D)             (None, 16, 16, 64)   8256        activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 16, 16, 64)   256         conv2d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 16, 16, 64)   0           batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, 16, 16, 64)   36928       activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_17 (Lambda)              (None, 16, 16, 64)   0           conv2d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, 16, 16, 64)   256         lambda_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 16, 16, 64)   0           batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_175 (Conv2D)             (None, 16, 16, 128)  8320        activation_157[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_50 (Add)                    (None, 16, 16, 128)  0           add_49[0][0]                     \n",
            "                                                                 conv2d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, 16, 16, 128)  512         add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 16, 16, 128)  0           batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_176 (Conv2D)             (None, 8, 8, 128)    16512       activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, 8, 8, 128)    512         conv2d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 8, 8, 128)    0           batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_177 (Conv2D)             (None, 8, 8, 128)    147584      activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, 8, 8, 128)    512         conv2d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 8, 8, 128)    0           batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, 8, 8, 256)    33024       add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, 8, 8, 256)    33024       activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_51 (Add)                    (None, 8, 8, 256)    0           conv2d_179[0][0]                 \n",
            "                                                                 conv2d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, 8, 8, 256)    1024        add_51[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 8, 8, 256)    0           batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, 8, 8, 128)    32896       activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, 8, 8, 128)    512         conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 8, 8, 128)    0           batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 8, 8, 128)    147584      activation_162[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, 8, 8, 128)    512         conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 8, 8, 128)    0           batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 8, 8, 256)    33024       activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_52 (Add)                    (None, 8, 8, 256)    0           add_51[0][0]                     \n",
            "                                                                 conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_164 (BatchN (None, 8, 8, 256)    1024        add_52[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, 8, 8, 256)    0           batch_normalization_164[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 8, 8, 128)    32896       activation_164[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, 8, 8, 128)    512         conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 8, 8, 128)    0           batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 8, 8, 128)    147584      activation_165[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, 8, 8, 128)    512         conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, 8, 8, 128)    0           batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 8, 8, 256)    33024       activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_53 (Add)                    (None, 8, 8, 256)    0           add_52[0][0]                     \n",
            "                                                                 conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 8, 8, 256)    1024        add_53[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, 8, 8, 256)    0           batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 1, 1, 256)    0           activation_167[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 256)          0           average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 10)           2570        flatten_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 198s 2s/step - loss: 2.4807 - accuracy: 0.3151\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 2.0445 - accuracy: 0.4701\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 190s 2s/step - loss: 1.7387 - accuracy: 0.5777\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 189s 2s/step - loss: 1.5070 - accuracy: 0.6517\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 1.3556 - accuracy: 0.6941\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 189s 2s/step - loss: 1.2225 - accuracy: 0.7256\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 1.0810 - accuracy: 0.7640\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 190s 2s/step - loss: 0.9711 - accuracy: 0.7929\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 190s 2s/step - loss: 0.8850 - accuracy: 0.8150\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 190s 2s/step - loss: 0.8203 - accuracy: 0.8301\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 0.7619 - accuracy: 0.8442\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 0.7139 - accuracy: 0.8582\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 0.6685 - accuracy: 0.8685\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 0.6231 - accuracy: 0.8823\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 192s 2s/step - loss: 0.5927 - accuracy: 0.8909\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 0.5459 - accuracy: 0.9031\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 0.5095 - accuracy: 0.9159\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 0.4773 - accuracy: 0.9235\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 190s 2s/step - loss: 0.4395 - accuracy: 0.9364\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 0.3958 - accuracy: 0.9502\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 190s 2s/step - loss: 0.3587 - accuracy: 0.9623\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 0.3204 - accuracy: 0.9747\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 191s 2s/step - loss: 0.2880 - accuracy: 0.9861\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 190s 2s/step - loss: 0.2677 - accuracy: 0.9927\n",
            "20/20 [==============================] - 4s 197ms/step - loss: 0.7914 - accuracy: 0.8521\n",
            "val accuracy score at the end of training model type  24 [0.7914375275373459, 0.8521484]\n",
            "=========================================\n",
            "\n",
            "Model Param:-  25\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 32, 32, 16)   448         input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 32, 32, 16)   64          conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_168 (Activation)     (None, 32, 32, 16)   0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 32, 32, 16)   272         activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 32, 32, 16)   64          conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_169 (Activation)     (None, 32, 32, 16)   0           batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 32, 32, 16)   2320        activation_169[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 32, 32, 16)   64          conv2d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_170 (Activation)     (None, 32, 32, 16)   0           batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 32, 32, 64)   1088        activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 32, 32, 64)   1088        activation_170[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_54 (Add)                    (None, 32, 32, 64)   0           conv2d_190[0][0]                 \n",
            "                                                                 conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 32, 32, 64)   256         add_54[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_171 (Activation)     (None, 32, 32, 64)   0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 32, 32, 16)   1040        activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 32, 32, 16)   64          conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_172 (Activation)     (None, 32, 32, 16)   0           batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 32, 32, 16)   2320        activation_172[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 32, 32, 16)   64          conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_173 (Activation)     (None, 32, 32, 16)   0           batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 32, 32, 64)   1088        activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_55 (Add)                    (None, 32, 32, 64)   0           add_54[0][0]                     \n",
            "                                                                 conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 32, 32, 64)   256         add_55[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_174 (Activation)     (None, 32, 32, 64)   0           batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 32, 32, 16)   1040        activation_174[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 32, 32, 16)   64          conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_175 (Activation)     (None, 32, 32, 16)   0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 32, 32, 16)   2320        activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 32, 32, 16)   64          conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_176 (Activation)     (None, 32, 32, 16)   0           batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 32, 32, 64)   1088        activation_176[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_56 (Add)                    (None, 32, 32, 64)   0           add_55[0][0]                     \n",
            "                                                                 conv2d_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, 32, 32, 64)   256         add_56[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_177 (Activation)     (None, 32, 32, 64)   0           batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 16, 16, 64)   4160        activation_177[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, 16, 16, 64)   256         conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_178 (Activation)     (None, 16, 16, 64)   0           batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 16, 16, 64)   36928       activation_178[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, 16, 16, 64)   256         conv2d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_179 (Activation)     (None, 16, 16, 64)   0           batch_normalization_179[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 16, 16, 128)  8320        add_56[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 16, 16, 128)  8320        activation_179[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_57 (Add)                    (None, 16, 16, 128)  0           conv2d_200[0][0]                 \n",
            "                                                                 conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, 16, 16, 128)  512         add_57[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_180 (Activation)     (None, 16, 16, 128)  0           batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 16, 16, 64)   8256        activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_181 (BatchN (None, 16, 16, 64)   256         conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_181 (Activation)     (None, 16, 16, 64)   0           batch_normalization_181[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 16, 16, 64)   36928       activation_181[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_182 (BatchN (None, 16, 16, 64)   256         conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_182 (Activation)     (None, 16, 16, 64)   0           batch_normalization_182[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 16, 16, 128)  8320        activation_182[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_58 (Add)                    (None, 16, 16, 128)  0           add_57[0][0]                     \n",
            "                                                                 conv2d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_183 (BatchN (None, 16, 16, 128)  512         add_58[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_183 (Activation)     (None, 16, 16, 128)  0           batch_normalization_183[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 16, 16, 64)   8256        activation_183[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_184 (BatchN (None, 16, 16, 64)   256         conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_184 (Activation)     (None, 16, 16, 64)   0           batch_normalization_184[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 16, 16, 64)   36928       activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_185 (BatchN (None, 16, 16, 64)   256         conv2d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_185 (Activation)     (None, 16, 16, 64)   0           batch_normalization_185[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 16, 16, 128)  8320        activation_185[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_59 (Add)                    (None, 16, 16, 128)  0           add_58[0][0]                     \n",
            "                                                                 conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_186 (BatchN (None, 16, 16, 128)  512         add_59[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_186 (Activation)     (None, 16, 16, 128)  0           batch_normalization_186[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 8, 8, 128)    16512       activation_186[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_187 (BatchN (None, 8, 8, 128)    512         conv2d_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_187 (Activation)     (None, 8, 8, 128)    0           batch_normalization_187[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 8, 8, 128)    147584      activation_187[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_18 (Lambda)              (None, 8, 8, 128)    0           conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_188 (BatchN (None, 8, 8, 128)    512         lambda_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_188 (Activation)     (None, 8, 8, 128)    0           batch_normalization_188[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 8, 8, 256)    33024       add_59[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 8, 8, 256)    33024       activation_188[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_60 (Add)                    (None, 8, 8, 256)    0           conv2d_210[0][0]                 \n",
            "                                                                 conv2d_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_189 (BatchN (None, 8, 8, 256)    1024        add_60[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_189 (Activation)     (None, 8, 8, 256)    0           batch_normalization_189[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 8, 8, 128)    32896       activation_189[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_190 (BatchN (None, 8, 8, 128)    512         conv2d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_190 (Activation)     (None, 8, 8, 128)    0           batch_normalization_190[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_212 (Conv2D)             (None, 8, 8, 128)    147584      activation_190[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_19 (Lambda)              (None, 8, 8, 128)    0           conv2d_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_191 (BatchN (None, 8, 8, 128)    512         lambda_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_191 (Activation)     (None, 8, 8, 128)    0           batch_normalization_191[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_213 (Conv2D)             (None, 8, 8, 256)    33024       activation_191[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_61 (Add)                    (None, 8, 8, 256)    0           add_60[0][0]                     \n",
            "                                                                 conv2d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_192 (BatchN (None, 8, 8, 256)    1024        add_61[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_192 (Activation)     (None, 8, 8, 256)    0           batch_normalization_192[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_214 (Conv2D)             (None, 8, 8, 128)    32896       activation_192[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_193 (BatchN (None, 8, 8, 128)    512         conv2d_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_193 (Activation)     (None, 8, 8, 128)    0           batch_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_215 (Conv2D)             (None, 8, 8, 128)    147584      activation_193[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_20 (Lambda)              (None, 8, 8, 128)    0           conv2d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_194 (BatchN (None, 8, 8, 128)    512         lambda_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_194 (Activation)     (None, 8, 8, 128)    0           batch_normalization_194[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_216 (Conv2D)             (None, 8, 8, 256)    33024       activation_194[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_62 (Add)                    (None, 8, 8, 256)    0           add_61[0][0]                     \n",
            "                                                                 conv2d_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_195 (BatchN (None, 8, 8, 256)    1024        add_62[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_195 (Activation)     (None, 8, 8, 256)    0           batch_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 1, 1, 256)    0           activation_195[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 256)          0           average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 10)           2570        flatten_6[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/24\n",
            "98/98 [==============================] - 217s 2s/step - loss: 2.4886 - accuracy: 0.3179\n",
            "epoch  2 : setting learning rate to  0.1\n",
            "Epoch 2/24\n",
            "98/98 [==============================] - 211s 2s/step - loss: 2.0903 - accuracy: 0.4526\n",
            "epoch  3 : setting learning rate to  0.175\n",
            "Epoch 3/24\n",
            "98/98 [==============================] - 211s 2s/step - loss: 1.8219 - accuracy: 0.5432\n",
            "epoch  4 : setting learning rate to  0.24999999999999997\n",
            "Epoch 4/24\n",
            "98/98 [==============================] - 211s 2s/step - loss: 1.5698 - accuracy: 0.6289\n",
            "epoch  5 : setting learning rate to  0.325\n",
            "Epoch 5/24\n",
            "98/98 [==============================] - 211s 2s/step - loss: 1.3896 - accuracy: 0.6830\n",
            "epoch  6 : setting learning rate to  0.4\n",
            "Epoch 6/24\n",
            "98/98 [==============================] - 210s 2s/step - loss: 1.2485 - accuracy: 0.7190\n",
            "epoch  7 : setting learning rate to  0.37894736842105264\n",
            "Epoch 7/24\n",
            "98/98 [==============================] - 212s 2s/step - loss: 1.0923 - accuracy: 0.7650\n",
            "epoch  8 : setting learning rate to  0.35789473684210527\n",
            "Epoch 8/24\n",
            "98/98 [==============================] - 212s 2s/step - loss: 0.9808 - accuracy: 0.7933\n",
            "epoch  9 : setting learning rate to  0.33684210526315794\n",
            "Epoch 9/24\n",
            "98/98 [==============================] - 211s 2s/step - loss: 0.8972 - accuracy: 0.8153\n",
            "epoch  10 : setting learning rate to  0.31578947368421056\n",
            "Epoch 10/24\n",
            "98/98 [==============================] - 217s 2s/step - loss: 0.8263 - accuracy: 0.8330\n",
            "epoch  11 : setting learning rate to  0.2947368421052632\n",
            "Epoch 11/24\n",
            "98/98 [==============================] - 214s 2s/step - loss: 0.7637 - accuracy: 0.8460\n",
            "epoch  12 : setting learning rate to  0.2736842105263158\n",
            "Epoch 12/24\n",
            "98/98 [==============================] - 212s 2s/step - loss: 0.7146 - accuracy: 0.8611\n",
            "epoch  13 : setting learning rate to  0.25263157894736843\n",
            "Epoch 13/24\n",
            "98/98 [==============================] - 211s 2s/step - loss: 0.6686 - accuracy: 0.8736\n",
            "epoch  14 : setting learning rate to  0.23157894736842108\n",
            "Epoch 14/24\n",
            "98/98 [==============================] - 210s 2s/step - loss: 0.6210 - accuracy: 0.8872\n",
            "epoch  15 : setting learning rate to  0.2105263157894737\n",
            "Epoch 15/24\n",
            "98/98 [==============================] - 214s 2s/step - loss: 0.5777 - accuracy: 0.8998\n",
            "epoch  16 : setting learning rate to  0.18947368421052635\n",
            "Epoch 16/24\n",
            "98/98 [==============================] - 211s 2s/step - loss: 0.5379 - accuracy: 0.9125\n",
            "epoch  17 : setting learning rate to  0.16842105263157897\n",
            "Epoch 17/24\n",
            "98/98 [==============================] - 210s 2s/step - loss: 0.5027 - accuracy: 0.9214\n",
            "epoch  18 : setting learning rate to  0.1473684210526316\n",
            "Epoch 18/24\n",
            "98/98 [==============================] - 210s 2s/step - loss: 0.4592 - accuracy: 0.9357\n",
            "epoch  19 : setting learning rate to  0.12631578947368421\n",
            "Epoch 19/24\n",
            "98/98 [==============================] - 211s 2s/step - loss: 0.4195 - accuracy: 0.9471\n",
            "epoch  20 : setting learning rate to  0.10526315789473689\n",
            "Epoch 20/24\n",
            "98/98 [==============================] - 212s 2s/step - loss: 0.3777 - accuracy: 0.9620\n",
            "epoch  21 : setting learning rate to  0.08421052631578951\n",
            "Epoch 21/24\n",
            "98/98 [==============================] - 211s 2s/step - loss: 0.3370 - accuracy: 0.9748\n",
            "epoch  22 : setting learning rate to  0.06315789473684214\n",
            "Epoch 22/24\n",
            "98/98 [==============================] - 209s 2s/step - loss: 0.3067 - accuracy: 0.9840\n",
            "epoch  23 : setting learning rate to  0.04210526315789476\n",
            "Epoch 23/24\n",
            "98/98 [==============================] - 210s 2s/step - loss: 0.2789 - accuracy: 0.9931\n",
            "epoch  24 : setting learning rate to  0.02105263157894738\n",
            "Epoch 24/24\n",
            "98/98 [==============================] - 208s 2s/step - loss: 0.2636 - accuracy: 0.9971\n",
            "20/20 [==============================] - 4s 195ms/step - loss: 0.9024 - accuracy: 0.8266\n",
            "val accuracy score at the end of training model type  25 [0.9023929476737976, 0.8265625]\n",
            "=========================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqkyspQ4R1C6",
        "colab_type": "text"
      },
      "source": [
        "### Summary of Test Results \n",
        "Hyperparameters : Epochs:24, max_lr:0.4, momentum:0.9, L2-wt_decay on Conv Layers :1.25e-4 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzOpzOMqq-gQ",
        "colab_type": "text"
      },
      "source": [
        "| Trial | Augmentation strategy | Train accuracy |Test Accuracy | Hyperparameters |Comments |\n",
        "| :--- | :---: | :---: | :---: | :---: | :--- |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7EJpOdo09FV",
        "colab_type": "text"
      },
      "source": [
        "**Validation accuracy went down when only Distortion was used in the mid/lower layers (after Res Blk1 and Res Blk2 ) But when used in combination with usual Image augmentation , there seems to be better Regularization and perhaps we could explore such an option. If we were to pursue only Distortion of middle or lower layers (with a good enough size of channels ) , we may need to try more stringent augmentation strategies to overcome the problem of overfitting**"
      ]
    }
  ]
}